<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <title id="head-title">gpu_ops_test_log.html</title>
      <link href="assets/style.css" rel="stylesheet" type="text/css"/>
  </head>
  <body>
    <h1 id="title">gpu_ops_test_log.html</h1>
    <p>Report generated on 12-Dec-2025 at 20:24:23 by <a href="https://pypi.python.org/pypi/pytest-html">pytest-html</a>
        v4.1.1</p>
    <div id="environment-header">
      <h2>Environment</h2>
    </div>
    <table id="environment"></table>
    <!-- TEMPLATES -->
      <template id="template_environment_row">
      <tr>
        <td></td>
        <td></td>
      </tr>
    </template>
    <template id="template_results-table__body--empty">
      <tbody class="results-table-row">
        <tr id="not-found-message">
          <td colspan="4">No results found. Check the filters.</th>
        </tr>
    </template>
    <template id="template_results-table__tbody">
      <tbody class="results-table-row">
        <tr class="collapsible">
        </tr>
        <tr class="extras-row">
          <td class="extra" colspan="4">
            <div class="extraHTML"></div>
            <div class="media">
              <div class="media-container">
                  <div class="media-container__nav--left"><</div>
                  <div class="media-container__viewport">
                    <img src="" />
                    <video controls>
                      <source src="" type="video/mp4">
                    </video>
                  </div>
                  <div class="media-container__nav--right">></div>
                </div>
                <div class="media__name"></div>
                <div class="media__counter"></div>
            </div>
            <div class="logwrapper">
              <div class="logexpander"></div>
              <div class="log"></div>
            </div>
          </td>
        </tr>
      </tbody>
    </template>
    <!-- END TEMPLATES -->
    <div class="summary">
      <div class="summary__data">
        <h2>Summary</h2>
        <div class="additional-summary prefix">
        </div>
        <p class="run-count">70 tests took 00:02:60.</p>
        <p class="filter">(Un)check the boxes to filter the results.</p>
        <div class="summary__reload">
          <div class="summary__reload__button hidden" onclick="location.reload()">
            <div>There are still tests running. <br />Reload this page to get the latest results!</div>
          </div>
        </div>
        <div class="summary__spacer"></div>
        <div class="controls">
          <div class="filters">
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="failed" />
            <span class="failed">9 Failed,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="passed" />
            <span class="passed">61 Passed,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="skipped" disabled/>
            <span class="skipped">0 Skipped,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="xfailed" disabled/>
            <span class="xfailed">0 Expected failures,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="xpassed" disabled/>
            <span class="xpassed">0 Unexpected passes,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="error" disabled/>
            <span class="error">0 Errors,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="rerun" />
            <span class="rerun">27 Reruns</span>
          </div>
          <div class="collapse">
            <button id="show_all_details">Show all details</button>&nbsp;/&nbsp;<button id="hide_all_details">Hide all details</button>
          </div>
        </div>
      </div>
      <div class="additional-summary summary">
      </div>
      <div class="additional-summary postfix">
      </div>
    </div>
    <table id="results-table">
      <thead id="results-table-head">
        <tr>
          <th class="sortable" data-column-type="result">Result</th>
          <th class="sortable" data-column-type="testId">Test</th>
          <th class="sortable" data-column-type="duration">Duration</th>
          <th>Links</th>
        </tr>
      </thead>
    </table>
  </body>
  <footer>
    <div id="data-container" data-jsonblob="{&#34;environment&#34;: {&#34;Python&#34;: &#34;3.12.3&#34;, &#34;Platform&#34;: &#34;Linux-6.2.0-25-generic-x86_64-with-glibc2.39&#34;, &#34;Packages&#34;: {&#34;pytest&#34;: &#34;9.0.2&#34;, &#34;pluggy&#34;: &#34;1.6.0&#34;}, &#34;Plugins&#34;: {&#34;hypothesis&#34;: &#34;6.148.7&#34;, &#34;html&#34;: &#34;4.1.1&#34;, &#34;json-report&#34;: &#34;1.5.0&#34;, &#34;metadata&#34;: &#34;3.1.1&#34;, &#34;reportlog&#34;: &#34;1.0.0&#34;, &#34;rerunfailures&#34;: &#34;16.1&#34;}}, &#34;tests&#34;: {&#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd0&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd0&#34;, &#34;duration&#34;: &#34;00:00:14&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd0&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:14&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd1&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd1&#34;, &#34;duration&#34;: &#34;496 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd1&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;496 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;&amp;gt;   dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n&amp;gt;   return attention.mha(\n        q, k, v,\n        block_sizes=BlockSizes(**dict(block_sizes)),\n        causal=causal,\n        segment_ids=segment_ids,\n        interpret=self.INTERPRET).sum()\nE   jax._src.source_info_util.JaxStackTraceBeforeTransformation: jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\nE   \nE   The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\nE   \nE   --------------------\n\njax/tests/pallas/gpu_ops_test.py:292: JaxStackTraceBeforeTransformation\n\nThe above exception was the direct cause of the following exception:\n\nself = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd1&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (&amp;quot;block_q&amp;quot;, 128),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 32),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 128),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 64),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 64),\n                (&amp;quot;block_q_dq&amp;quot;, 64),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(&amp;quot;8.0&amp;quot;):\n        # TODO(b/416306534)\n        self.skipTest(&amp;quot;Precision issues after CUDA 12.8.1 upgrade&amp;quot;)\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n&amp;gt;     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:505: in value_and_grad_f\n    g = vjp_py(lax_internal._one(ans))\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2105: in _vjp_pullback_wrapper\n    ans = fun(*args)\n          ^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:320: in unbound_vjp\n    arg_cts = backward_pass(jaxpr, True, consts, dummy_args, cts)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:438: in backward_pass\n    cts_out = get_primitive_transpose(eqn.primitive)(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:1559: in _custom_lin_transpose\n    cts_in = bwd.call_wrapped(*res, *cts_out)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:921: in _flatten_bwd\n    py_cts_in = f(py_res, py_cts_out)\n                ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:234: in _prepend_static_args\n    return f(*all_args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:602: in _mha_backward\n    dq, dk, dv = pl.pallas_call(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7fc5bfa72ac0&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x77c61b40&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd1&#34;, &#34;duration&#34;: &#34;496 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd1&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;496 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;&amp;gt;   dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n&amp;gt;   return attention.mha(\n        q, k, v,\n        block_sizes=BlockSizes(**dict(block_sizes)),\n        causal=causal,\n        segment_ids=segment_ids,\n        interpret=self.INTERPRET).sum()\nE   jax._src.source_info_util.JaxStackTraceBeforeTransformation: jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\nE   \nE   The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\nE   \nE   --------------------\n\njax/tests/pallas/gpu_ops_test.py:292: JaxStackTraceBeforeTransformation\n\nThe above exception was the direct cause of the following exception:\n\nself = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd1&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (&amp;quot;block_q&amp;quot;, 128),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 32),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 128),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 64),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 64),\n                (&amp;quot;block_q_dq&amp;quot;, 64),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(&amp;quot;8.0&amp;quot;):\n        # TODO(b/416306534)\n        self.skipTest(&amp;quot;Precision issues after CUDA 12.8.1 upgrade&amp;quot;)\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n&amp;gt;     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:505: in value_and_grad_f\n    g = vjp_py(lax_internal._one(ans))\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2105: in _vjp_pullback_wrapper\n    ans = fun(*args)\n          ^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:320: in unbound_vjp\n    arg_cts = backward_pass(jaxpr, True, consts, dummy_args, cts)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:438: in backward_pass\n    cts_out = get_primitive_transpose(eqn.primitive)(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:1559: in _custom_lin_transpose\n    cts_in = bwd.call_wrapped(*res, *cts_out)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:921: in _flatten_bwd\n    py_cts_in = f(py_res, py_cts_out)\n                ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:234: in _prepend_static_args\n    return f(*all_args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:602: in _mha_backward\n    dq, dk, dv = pl.pallas_call(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7fadbf4a1c10&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x7839b6f0&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd1&#34;, &#34;duration&#34;: &#34;496 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd1&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;496 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;&amp;gt;   dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n&amp;gt;   return attention.mha(\n        q, k, v,\n        block_sizes=BlockSizes(**dict(block_sizes)),\n        causal=causal,\n        segment_ids=segment_ids,\n        interpret=self.INTERPRET).sum()\nE   jax._src.source_info_util.JaxStackTraceBeforeTransformation: jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\nE   \nE   The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\nE   \nE   --------------------\n\njax/tests/pallas/gpu_ops_test.py:292: JaxStackTraceBeforeTransformation\n\nThe above exception was the direct cause of the following exception:\n\nself = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd1&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (&amp;quot;block_q&amp;quot;, 128),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 32),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 128),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 64),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 64),\n                (&amp;quot;block_q_dq&amp;quot;, 64),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(&amp;quot;8.0&amp;quot;):\n        # TODO(b/416306534)\n        self.skipTest(&amp;quot;Precision issues after CUDA 12.8.1 upgrade&amp;quot;)\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n&amp;gt;     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:505: in value_and_grad_f\n    g = vjp_py(lax_internal._one(ans))\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2105: in _vjp_pullback_wrapper\n    ans = fun(*args)\n          ^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:320: in unbound_vjp\n    arg_cts = backward_pass(jaxpr, True, consts, dummy_args, cts)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:438: in backward_pass\n    cts_out = get_primitive_transpose(eqn.primitive)(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:1559: in _custom_lin_transpose\n    cts_in = bwd.call_wrapped(*res, *cts_out)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:921: in _flatten_bwd\n    py_cts_in = f(py_res, py_cts_out)\n                ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:234: in _prepend_static_args\n    return f(*all_args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:602: in _mha_backward\n    dq, dk, dv = pl.pallas_call(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7fb5bf69fec0&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x77f331b0&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd1&#34;, &#34;duration&#34;: &#34;496 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd1&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;496 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;&amp;gt;   dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n&amp;gt;   return attention.mha(\n        q, k, v,\n        block_sizes=BlockSizes(**dict(block_sizes)),\n        causal=causal,\n        segment_ids=segment_ids,\n        interpret=self.INTERPRET).sum()\nE   jax._src.source_info_util.JaxStackTraceBeforeTransformation: jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\nE   \nE   The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\nE   \nE   --------------------\n\njax/tests/pallas/gpu_ops_test.py:292: JaxStackTraceBeforeTransformation\n\nThe above exception was the direct cause of the following exception:\n\nself = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd1&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (&amp;quot;block_q&amp;quot;, 128),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 32),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 128),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 64),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 64),\n                (&amp;quot;block_q_dq&amp;quot;, 64),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(&amp;quot;8.0&amp;quot;):\n        # TODO(b/416306534)\n        self.skipTest(&amp;quot;Precision issues after CUDA 12.8.1 upgrade&amp;quot;)\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n&amp;gt;     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:505: in value_and_grad_f\n    g = vjp_py(lax_internal._one(ans))\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2105: in _vjp_pullback_wrapper\n    ans = fun(*args)\n          ^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:320: in unbound_vjp\n    arg_cts = backward_pass(jaxpr, True, consts, dummy_args, cts)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:438: in backward_pass\n    cts_out = get_primitive_transpose(eqn.primitive)(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:1559: in _custom_lin_transpose\n    cts_in = bwd.call_wrapped(*res, *cts_out)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:921: in _flatten_bwd\n    py_cts_in = f(py_res, py_cts_out)\n                ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:234: in _prepend_static_args\n    return f(*all_args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:602: in _mha_backward\n    dq, dk, dv = pl.pallas_call(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7fd5bffcf1f0&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x796cf480&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd2&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd2&#34;, &#34;duration&#34;: &#34;00:00:09&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd2&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:09&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd3&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd3&#34;, &#34;duration&#34;: &#34;00:00:07&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd3&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:07&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd4&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd4&#34;, &#34;duration&#34;: &#34;514 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd4&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;514 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;&amp;gt;   dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n&amp;gt;   return attention.mha(\n        q, k, v,\n        block_sizes=BlockSizes(**dict(block_sizes)),\n        causal=causal,\n        segment_ids=segment_ids,\n        interpret=self.INTERPRET).sum()\nE   jax._src.source_info_util.JaxStackTraceBeforeTransformation: jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\nE   \nE   The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\nE   \nE   --------------------\n\njax/tests/pallas/gpu_ops_test.py:292: JaxStackTraceBeforeTransformation\n\nThe above exception was the direct cause of the following exception:\n\nself = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd4&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (&amp;quot;block_q&amp;quot;, 128),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 32),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 128),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 64),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 64),\n                (&amp;quot;block_q_dq&amp;quot;, 64),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(&amp;quot;8.0&amp;quot;):\n        # TODO(b/416306534)\n        self.skipTest(&amp;quot;Precision issues after CUDA 12.8.1 upgrade&amp;quot;)\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n&amp;gt;     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:505: in value_and_grad_f\n    g = vjp_py(lax_internal._one(ans))\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2105: in _vjp_pullback_wrapper\n    ans = fun(*args)\n          ^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:320: in unbound_vjp\n    arg_cts = backward_pass(jaxpr, True, consts, dummy_args, cts)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:438: in backward_pass\n    cts_out = get_primitive_transpose(eqn.primitive)(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:1559: in _custom_lin_transpose\n    cts_in = bwd.call_wrapped(*res, *cts_out)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:921: in _flatten_bwd\n    py_cts_in = f(py_res, py_cts_out)\n                ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:234: in _prepend_static_args\n    return f(*all_args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:602: in _mha_backward\n    dq, dk, dv = pl.pallas_call(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f95000e98a0&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x7d68cf80&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd4&#34;, &#34;duration&#34;: &#34;514 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd4&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;514 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;&amp;gt;   dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n&amp;gt;   return attention.mha(\n        q, k, v,\n        block_sizes=BlockSizes(**dict(block_sizes)),\n        causal=causal,\n        segment_ids=segment_ids,\n        interpret=self.INTERPRET).sum()\nE   jax._src.source_info_util.JaxStackTraceBeforeTransformation: jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\nE   \nE   The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\nE   \nE   --------------------\n\njax/tests/pallas/gpu_ops_test.py:292: JaxStackTraceBeforeTransformation\n\nThe above exception was the direct cause of the following exception:\n\nself = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd4&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (&amp;quot;block_q&amp;quot;, 128),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 32),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 128),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 64),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 64),\n                (&amp;quot;block_q_dq&amp;quot;, 64),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(&amp;quot;8.0&amp;quot;):\n        # TODO(b/416306534)\n        self.skipTest(&amp;quot;Precision issues after CUDA 12.8.1 upgrade&amp;quot;)\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n&amp;gt;     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:505: in value_and_grad_f\n    g = vjp_py(lax_internal._one(ans))\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2105: in _vjp_pullback_wrapper\n    ans = fun(*args)\n          ^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:320: in unbound_vjp\n    arg_cts = backward_pass(jaxpr, True, consts, dummy_args, cts)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:438: in backward_pass\n    cts_out = get_primitive_transpose(eqn.primitive)(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:1559: in _custom_lin_transpose\n    cts_in = bwd.call_wrapped(*res, *cts_out)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:921: in _flatten_bwd\n    py_cts_in = f(py_res, py_cts_out)\n                ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:234: in _prepend_static_args\n    return f(*all_args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:602: in _mha_backward\n    dq, dk, dv = pl.pallas_call(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f950006ee30&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x7d49b640&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd4&#34;, &#34;duration&#34;: &#34;514 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd4&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;514 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;&amp;gt;   dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n&amp;gt;   return attention.mha(\n        q, k, v,\n        block_sizes=BlockSizes(**dict(block_sizes)),\n        causal=causal,\n        segment_ids=segment_ids,\n        interpret=self.INTERPRET).sum()\nE   jax._src.source_info_util.JaxStackTraceBeforeTransformation: jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\nE   \nE   The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\nE   \nE   --------------------\n\njax/tests/pallas/gpu_ops_test.py:292: JaxStackTraceBeforeTransformation\n\nThe above exception was the direct cause of the following exception:\n\nself = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd4&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (&amp;quot;block_q&amp;quot;, 128),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 32),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 128),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 64),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 64),\n                (&amp;quot;block_q_dq&amp;quot;, 64),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(&amp;quot;8.0&amp;quot;):\n        # TODO(b/416306534)\n        self.skipTest(&amp;quot;Precision issues after CUDA 12.8.1 upgrade&amp;quot;)\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n&amp;gt;     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:505: in value_and_grad_f\n    g = vjp_py(lax_internal._one(ans))\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2105: in _vjp_pullback_wrapper\n    ans = fun(*args)\n          ^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:320: in unbound_vjp\n    arg_cts = backward_pass(jaxpr, True, consts, dummy_args, cts)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:438: in backward_pass\n    cts_out = get_primitive_transpose(eqn.primitive)(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:1559: in _custom_lin_transpose\n    cts_in = bwd.call_wrapped(*res, *cts_out)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:921: in _flatten_bwd\n    py_cts_in = f(py_res, py_cts_out)\n                ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:234: in _prepend_static_args\n    return f(*all_args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:602: in _mha_backward\n    dq, dk, dv = pl.pallas_call(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f95007e33d0&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x7e6defb0&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd4&#34;, &#34;duration&#34;: &#34;514 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd4&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;514 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;&amp;gt;   dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n&amp;gt;   return attention.mha(\n        q, k, v,\n        block_sizes=BlockSizes(**dict(block_sizes)),\n        causal=causal,\n        segment_ids=segment_ids,\n        interpret=self.INTERPRET).sum()\nE   jax._src.source_info_util.JaxStackTraceBeforeTransformation: jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\nE   \nE   The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\nE   \nE   --------------------\n\njax/tests/pallas/gpu_ops_test.py:292: JaxStackTraceBeforeTransformation\n\nThe above exception was the direct cause of the following exception:\n\nself = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd4&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (&amp;quot;block_q&amp;quot;, 128),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 32),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 128),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 64),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 64),\n                (&amp;quot;block_q_dq&amp;quot;, 64),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(&amp;quot;8.0&amp;quot;):\n        # TODO(b/416306534)\n        self.skipTest(&amp;quot;Precision issues after CUDA 12.8.1 upgrade&amp;quot;)\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n&amp;gt;     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:505: in value_and_grad_f\n    g = vjp_py(lax_internal._one(ans))\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2105: in _vjp_pullback_wrapper\n    ans = fun(*args)\n          ^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:320: in unbound_vjp\n    arg_cts = backward_pass(jaxpr, True, consts, dummy_args, cts)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:438: in backward_pass\n    cts_out = get_primitive_transpose(eqn.primitive)(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:1559: in _custom_lin_transpose\n    cts_in = bwd.call_wrapped(*res, *cts_out)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:921: in _flatten_bwd\n    py_cts_in = f(py_res, py_cts_out)\n                ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:234: in _prepend_static_args\n    return f(*all_args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:602: in _mha_backward\n    dq, dk, dv = pl.pallas_call(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f9500765350&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x7f05f680&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd5&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd5&#34;, &#34;duration&#34;: &#34;337 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd5&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;337 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;&amp;gt;   dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n&amp;gt;   return attention.mha(\n        q, k, v,\n        block_sizes=BlockSizes(**dict(block_sizes)),\n        causal=causal,\n        segment_ids=segment_ids,\n        interpret=self.INTERPRET).sum()\nE   jax._src.source_info_util.JaxStackTraceBeforeTransformation: jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 81920, available: 65536\nE   \nE   The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\nE   \nE   --------------------\n\njax/tests/pallas/gpu_ops_test.py:292: JaxStackTraceBeforeTransformation\n\nThe above exception was the direct cause of the following exception:\n\nself = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd5&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (&amp;quot;block_q&amp;quot;, 128),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 32),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 128),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 64),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 64),\n                (&amp;quot;block_q_dq&amp;quot;, 64),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(&amp;quot;8.0&amp;quot;):\n        # TODO(b/416306534)\n        self.skipTest(&amp;quot;Precision issues after CUDA 12.8.1 upgrade&amp;quot;)\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n&amp;gt;     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:505: in value_and_grad_f\n    g = vjp_py(lax_internal._one(ans))\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2105: in _vjp_pullback_wrapper\n    ans = fun(*args)\n          ^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:320: in unbound_vjp\n    arg_cts = backward_pass(jaxpr, True, consts, dummy_args, cts)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:438: in backward_pass\n    cts_out = get_primitive_transpose(eqn.primitive)(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:1559: in _custom_lin_transpose\n    cts_in = bwd.call_wrapped(*res, *cts_out)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:921: in _flatten_bwd\n    py_cts_in = f(py_res, py_cts_out)\n                ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:234: in _prepend_static_args\n    return f(*all_args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:602: in _mha_backward\n    dq, dk, dv = pl.pallas_call(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f950074fb00&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x7e715030&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 81920, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd5&#34;, &#34;duration&#34;: &#34;337 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd5&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;337 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;&amp;gt;   dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n&amp;gt;   return attention.mha(\n        q, k, v,\n        block_sizes=BlockSizes(**dict(block_sizes)),\n        causal=causal,\n        segment_ids=segment_ids,\n        interpret=self.INTERPRET).sum()\nE   jax._src.source_info_util.JaxStackTraceBeforeTransformation: jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 81920, available: 65536\nE   \nE   The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\nE   \nE   --------------------\n\njax/tests/pallas/gpu_ops_test.py:292: JaxStackTraceBeforeTransformation\n\nThe above exception was the direct cause of the following exception:\n\nself = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd5&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (&amp;quot;block_q&amp;quot;, 128),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 32),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 128),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 64),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 64),\n                (&amp;quot;block_q_dq&amp;quot;, 64),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(&amp;quot;8.0&amp;quot;):\n        # TODO(b/416306534)\n        self.skipTest(&amp;quot;Precision issues after CUDA 12.8.1 upgrade&amp;quot;)\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n&amp;gt;     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:505: in value_and_grad_f\n    g = vjp_py(lax_internal._one(ans))\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2105: in _vjp_pullback_wrapper\n    ans = fun(*args)\n          ^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:320: in unbound_vjp\n    arg_cts = backward_pass(jaxpr, True, consts, dummy_args, cts)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:438: in backward_pass\n    cts_out = get_primitive_transpose(eqn.primitive)(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:1559: in _custom_lin_transpose\n    cts_in = bwd.call_wrapped(*res, *cts_out)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:921: in _flatten_bwd\n    py_cts_in = f(py_res, py_cts_out)\n                ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:234: in _prepend_static_args\n    return f(*all_args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:602: in _mha_backward\n    dq, dk, dv = pl.pallas_call(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7fd5bff77c40&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x7f05f680&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 81920, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd5&#34;, &#34;duration&#34;: &#34;337 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd5&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;337 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;&amp;gt;   dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n&amp;gt;   return attention.mha(\n        q, k, v,\n        block_sizes=BlockSizes(**dict(block_sizes)),\n        causal=causal,\n        segment_ids=segment_ids,\n        interpret=self.INTERPRET).sum()\nE   jax._src.source_info_util.JaxStackTraceBeforeTransformation: jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 81920, available: 65536\nE   \nE   The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\nE   \nE   --------------------\n\njax/tests/pallas/gpu_ops_test.py:292: JaxStackTraceBeforeTransformation\n\nThe above exception was the direct cause of the following exception:\n\nself = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd5&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (&amp;quot;block_q&amp;quot;, 128),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 32),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 128),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 64),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 64),\n                (&amp;quot;block_q_dq&amp;quot;, 64),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(&amp;quot;8.0&amp;quot;):\n        # TODO(b/416306534)\n        self.skipTest(&amp;quot;Precision issues after CUDA 12.8.1 upgrade&amp;quot;)\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n&amp;gt;     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:505: in value_and_grad_f\n    g = vjp_py(lax_internal._one(ans))\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2105: in _vjp_pullback_wrapper\n    ans = fun(*args)\n          ^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:320: in unbound_vjp\n    arg_cts = backward_pass(jaxpr, True, consts, dummy_args, cts)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:438: in backward_pass\n    cts_out = get_primitive_transpose(eqn.primitive)(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:1559: in _custom_lin_transpose\n    cts_in = bwd.call_wrapped(*res, *cts_out)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:921: in _flatten_bwd\n    py_cts_in = f(py_res, py_cts_out)\n                ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:234: in _prepend_static_args\n    return f(*all_args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:602: in _mha_backward\n    dq, dk, dv = pl.pallas_call(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7fcdbfd19210&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x7df97140&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 81920, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd5&#34;, &#34;duration&#34;: &#34;337 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd5&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;337 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;&amp;gt;   dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n&amp;gt;   return attention.mha(\n        q, k, v,\n        block_sizes=BlockSizes(**dict(block_sizes)),\n        causal=causal,\n        segment_ids=segment_ids,\n        interpret=self.INTERPRET).sum()\nE   jax._src.source_info_util.JaxStackTraceBeforeTransformation: jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 81920, available: 65536\nE   \nE   The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\nE   \nE   --------------------\n\njax/tests/pallas/gpu_ops_test.py:292: JaxStackTraceBeforeTransformation\n\nThe above exception was the direct cause of the following exception:\n\nself = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd5&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (&amp;quot;block_q&amp;quot;, 128),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 32),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 128),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 64),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 64),\n                (&amp;quot;block_q_dq&amp;quot;, 64),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(&amp;quot;8.0&amp;quot;):\n        # TODO(b/416306534)\n        self.skipTest(&amp;quot;Precision issues after CUDA 12.8.1 upgrade&amp;quot;)\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n&amp;gt;     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:505: in value_and_grad_f\n    g = vjp_py(lax_internal._one(ans))\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2105: in _vjp_pullback_wrapper\n    ans = fun(*args)\n          ^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:320: in unbound_vjp\n    arg_cts = backward_pass(jaxpr, True, consts, dummy_args, cts)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:438: in backward_pass\n    cts_out = get_primitive_transpose(eqn.primitive)(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:1559: in _custom_lin_transpose\n    cts_in = bwd.call_wrapped(*res, *cts_out)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:921: in _flatten_bwd\n    py_cts_in = f(py_res, py_cts_out)\n                ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:234: in _prepend_static_args\n    return f(*all_args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:602: in _mha_backward\n    dq, dk, dv = pl.pallas_call(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f95007c9ee0&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x7f96ff70&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 81920, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd6&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd6&#34;, &#34;duration&#34;: &#34;00:00:06&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd6&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:06&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd7&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd7&#34;, &#34;duration&#34;: &#34;227 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd7&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;227 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd7&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (&amp;quot;block_q&amp;quot;, 128),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 32),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 128),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 64),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 64),\n                (&amp;quot;block_q_dq&amp;quot;, 64),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(&amp;quot;8.0&amp;quot;):\n        # TODO(b/416306534)\n        self.skipTest(&amp;quot;Precision issues after CUDA 12.8.1 upgrade&amp;quot;)\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n&amp;gt;     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:500: in value_and_grad_f\n    ans, vjp_py = _vjp(f_partial, *dyn_args)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2190: in _vjp\n    out_primals, vjp = ad.vjp(flat_fun, primals_flat)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:313: in vjp\n    out_primals, pvals, jaxpr, consts = linearize(traceable, *primals)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:287: in linearize\n    return direct_linearize(traceable, primals, kwargs, has_aux=has_aux)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:261: in direct_linearize\n    ans = traceable.call_wrapped(*tracers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:90: in flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\njax/tests/pallas/gpu_ops_test.py:292: in f\n    return attention.mha(\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:979: in process_custom_vjp_call\n    res_and_primals_out = fwd.call_wrapped(*fwd_in_flat)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:850: in _flatten_fwd\n    pair_out = f(*py_args)\n               ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:314: in _mha_forward\n    out, lse = mha(q, k, v, segment_ids=segment_ids, sm_scale=sm_scale,\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:1210: in process_custom_vjp_call\n    return fun.call_wrapped(*tracers)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:87: in _flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:759: in _check_primal_refs\n    out = f(*args)\n          ^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f9500142fc0&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x7fccdec0&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd7&#34;, &#34;duration&#34;: &#34;227 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd7&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;227 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd7&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (&amp;quot;block_q&amp;quot;, 128),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 32),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 128),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 64),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 64),\n                (&amp;quot;block_q_dq&amp;quot;, 64),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(&amp;quot;8.0&amp;quot;):\n        # TODO(b/416306534)\n        self.skipTest(&amp;quot;Precision issues after CUDA 12.8.1 upgrade&amp;quot;)\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n&amp;gt;     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:500: in value_and_grad_f\n    ans, vjp_py = _vjp(f_partial, *dyn_args)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2190: in _vjp\n    out_primals, vjp = ad.vjp(flat_fun, primals_flat)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:313: in vjp\n    out_primals, pvals, jaxpr, consts = linearize(traceable, *primals)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:287: in linearize\n    return direct_linearize(traceable, primals, kwargs, has_aux=has_aux)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:261: in direct_linearize\n    ans = traceable.call_wrapped(*tracers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:90: in flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\njax/tests/pallas/gpu_ops_test.py:292: in f\n    return attention.mha(\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:979: in process_custom_vjp_call\n    res_and_primals_out = fwd.call_wrapped(*fwd_in_flat)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:850: in _flatten_fwd\n    pair_out = f(*py_args)\n               ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:314: in _mha_forward\n    out, lse = mha(q, k, v, segment_ids=segment_ids, sm_scale=sm_scale,\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:1210: in process_custom_vjp_call\n    return fun.call_wrapped(*tracers)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:87: in _flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:759: in _check_primal_refs\n    out = f(*args)\n          ^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f9500142fc0&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x7fa84140&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd7&#34;, &#34;duration&#34;: &#34;227 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd7&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;227 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd7&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (&amp;quot;block_q&amp;quot;, 128),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 32),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 128),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 64),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 64),\n                (&amp;quot;block_q_dq&amp;quot;, 64),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(&amp;quot;8.0&amp;quot;):\n        # TODO(b/416306534)\n        self.skipTest(&amp;quot;Precision issues after CUDA 12.8.1 upgrade&amp;quot;)\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n&amp;gt;     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:500: in value_and_grad_f\n    ans, vjp_py = _vjp(f_partial, *dyn_args)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2190: in _vjp\n    out_primals, vjp = ad.vjp(flat_fun, primals_flat)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:313: in vjp\n    out_primals, pvals, jaxpr, consts = linearize(traceable, *primals)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:287: in linearize\n    return direct_linearize(traceable, primals, kwargs, has_aux=has_aux)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:261: in direct_linearize\n    ans = traceable.call_wrapped(*tracers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:90: in flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\njax/tests/pallas/gpu_ops_test.py:292: in f\n    return attention.mha(\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:979: in process_custom_vjp_call\n    res_and_primals_out = fwd.call_wrapped(*fwd_in_flat)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:850: in _flatten_fwd\n    pair_out = f(*py_args)\n               ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:314: in _mha_forward\n    out, lse = mha(q, k, v, segment_ids=segment_ids, sm_scale=sm_scale,\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:1210: in process_custom_vjp_call\n    return fun.call_wrapped(*tracers)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:87: in _flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:759: in _check_primal_refs\n    out = f(*args)\n          ^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f9500142fc0&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x8112e510&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd7&#34;, &#34;duration&#34;: &#34;227 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd7&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;227 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd7&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (&amp;quot;block_q&amp;quot;, 128),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 32),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 128),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 64),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 64),\n                (&amp;quot;block_q_dq&amp;quot;, 64),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(&amp;quot;8.0&amp;quot;):\n        # TODO(b/416306534)\n        self.skipTest(&amp;quot;Precision issues after CUDA 12.8.1 upgrade&amp;quot;)\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n&amp;gt;     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:500: in value_and_grad_f\n    ans, vjp_py = _vjp(f_partial, *dyn_args)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2190: in _vjp\n    out_primals, vjp = ad.vjp(flat_fun, primals_flat)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:313: in vjp\n    out_primals, pvals, jaxpr, consts = linearize(traceable, *primals)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:287: in linearize\n    return direct_linearize(traceable, primals, kwargs, has_aux=has_aux)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:261: in direct_linearize\n    ans = traceable.call_wrapped(*tracers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:90: in flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\njax/tests/pallas/gpu_ops_test.py:292: in f\n    return attention.mha(\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:979: in process_custom_vjp_call\n    res_and_primals_out = fwd.call_wrapped(*fwd_in_flat)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:850: in _flatten_fwd\n    pair_out = f(*py_args)\n               ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:314: in _mha_forward\n    out, lse = mha(q, k, v, segment_ids=segment_ids, sm_scale=sm_scale,\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:1210: in process_custom_vjp_call\n    return fun.call_wrapped(*tracers)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:87: in _flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:759: in _check_primal_refs\n    out = f(*args)\n          ^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f9500142fc0&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x7ecadff0&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd8&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd8&#34;, &#34;duration&#34;: &#34;149 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd8&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;149 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd8&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (&amp;quot;block_q&amp;quot;, 128),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 32),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 128),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 64),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 64),\n                (&amp;quot;block_q_dq&amp;quot;, 64),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(&amp;quot;8.0&amp;quot;):\n        # TODO(b/416306534)\n        self.skipTest(&amp;quot;Precision issues after CUDA 12.8.1 upgrade&amp;quot;)\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n&amp;gt;     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:500: in value_and_grad_f\n    ans, vjp_py = _vjp(f_partial, *dyn_args)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2190: in _vjp\n    out_primals, vjp = ad.vjp(flat_fun, primals_flat)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:313: in vjp\n    out_primals, pvals, jaxpr, consts = linearize(traceable, *primals)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:287: in linearize\n    return direct_linearize(traceable, primals, kwargs, has_aux=has_aux)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:261: in direct_linearize\n    ans = traceable.call_wrapped(*tracers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:90: in flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\njax/tests/pallas/gpu_ops_test.py:292: in f\n    return attention.mha(\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:979: in process_custom_vjp_call\n    res_and_primals_out = fwd.call_wrapped(*fwd_in_flat)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:850: in _flatten_fwd\n    pair_out = f(*py_args)\n               ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:314: in _mha_forward\n    out, lse = mha(q, k, v, segment_ids=segment_ids, sm_scale=sm_scale,\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:1210: in process_custom_vjp_call\n    return fun.call_wrapped(*tracers)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:87: in _flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:759: in _check_primal_refs\n    out = f(*args)\n          ^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f9500140130&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x7839b6f0&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 98304, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd8&#34;, &#34;duration&#34;: &#34;149 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd8&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;149 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd8&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (&amp;quot;block_q&amp;quot;, 128),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 32),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 128),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 64),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 64),\n                (&amp;quot;block_q_dq&amp;quot;, 64),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(&amp;quot;8.0&amp;quot;):\n        # TODO(b/416306534)\n        self.skipTest(&amp;quot;Precision issues after CUDA 12.8.1 upgrade&amp;quot;)\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n&amp;gt;     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:500: in value_and_grad_f\n    ans, vjp_py = _vjp(f_partial, *dyn_args)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2190: in _vjp\n    out_primals, vjp = ad.vjp(flat_fun, primals_flat)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:313: in vjp\n    out_primals, pvals, jaxpr, consts = linearize(traceable, *primals)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:287: in linearize\n    return direct_linearize(traceable, primals, kwargs, has_aux=has_aux)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:261: in direct_linearize\n    ans = traceable.call_wrapped(*tracers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:90: in flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\njax/tests/pallas/gpu_ops_test.py:292: in f\n    return attention.mha(\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:979: in process_custom_vjp_call\n    res_and_primals_out = fwd.call_wrapped(*fwd_in_flat)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:850: in _flatten_fwd\n    pair_out = f(*py_args)\n               ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:314: in _mha_forward\n    out, lse = mha(q, k, v, segment_ids=segment_ids, sm_scale=sm_scale,\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:1210: in process_custom_vjp_call\n    return fun.call_wrapped(*tracers)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:87: in _flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:759: in _check_primal_refs\n    out = f(*args)\n          ^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f9500140130&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x7ede77f0&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 98304, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd8&#34;, &#34;duration&#34;: &#34;149 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd8&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;149 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd8&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (&amp;quot;block_q&amp;quot;, 128),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 32),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 128),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 64),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 64),\n                (&amp;quot;block_q_dq&amp;quot;, 64),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(&amp;quot;8.0&amp;quot;):\n        # TODO(b/416306534)\n        self.skipTest(&amp;quot;Precision issues after CUDA 12.8.1 upgrade&amp;quot;)\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n&amp;gt;     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:500: in value_and_grad_f\n    ans, vjp_py = _vjp(f_partial, *dyn_args)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2190: in _vjp\n    out_primals, vjp = ad.vjp(flat_fun, primals_flat)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:313: in vjp\n    out_primals, pvals, jaxpr, consts = linearize(traceable, *primals)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:287: in linearize\n    return direct_linearize(traceable, primals, kwargs, has_aux=has_aux)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:261: in direct_linearize\n    ans = traceable.call_wrapped(*tracers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:90: in flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\njax/tests/pallas/gpu_ops_test.py:292: in f\n    return attention.mha(\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:979: in process_custom_vjp_call\n    res_and_primals_out = fwd.call_wrapped(*fwd_in_flat)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:850: in _flatten_fwd\n    pair_out = f(*py_args)\n               ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:314: in _mha_forward\n    out, lse = mha(q, k, v, segment_ids=segment_ids, sm_scale=sm_scale,\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:1210: in process_custom_vjp_call\n    return fun.call_wrapped(*tracers)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:87: in _flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:759: in _check_primal_refs\n    out = f(*args)\n          ^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f9500140130&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x79caac00&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 98304, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd8&#34;, &#34;duration&#34;: &#34;149 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd8&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;149 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd8&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (&amp;quot;block_q&amp;quot;, 128),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 32),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 128),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 64),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 64),\n                (&amp;quot;block_q_dq&amp;quot;, 64),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n            (\n                (&amp;quot;block_q&amp;quot;, 64),\n                (&amp;quot;block_k&amp;quot;, 128),\n                (&amp;quot;block_q_dkv&amp;quot;, 64),\n                (&amp;quot;block_kv_dkv&amp;quot;, 32),\n                (&amp;quot;block_q_dq&amp;quot;, 32),\n                (&amp;quot;block_kv_dq&amp;quot;, 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(&amp;quot;8.0&amp;quot;):\n        # TODO(b/416306534)\n        self.skipTest(&amp;quot;Precision issues after CUDA 12.8.1 upgrade&amp;quot;)\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n&amp;gt;     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:500: in value_and_grad_f\n    ans, vjp_py = _vjp(f_partial, *dyn_args)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2190: in _vjp\n    out_primals, vjp = ad.vjp(flat_fun, primals_flat)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:313: in vjp\n    out_primals, pvals, jaxpr, consts = linearize(traceable, *primals)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:287: in linearize\n    return direct_linearize(traceable, primals, kwargs, has_aux=has_aux)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:261: in direct_linearize\n    ans = traceable.call_wrapped(*tracers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:90: in flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\njax/tests/pallas/gpu_ops_test.py:292: in f\n    return attention.mha(\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:979: in process_custom_vjp_call\n    res_and_primals_out = fwd.call_wrapped(*fwd_in_flat)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:850: in _flatten_fwd\n    pair_out = f(*py_args)\n               ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:314: in _mha_forward\n    out, lse = mha(q, k, v, segment_ids=segment_ids, sm_scale=sm_scale,\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:1210: in process_custom_vjp_call\n    return fun.call_wrapped(*tracers)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:87: in _flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:759: in _check_primal_refs\n    out = f(*args)\n          ^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f9500140130&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x7dbc9370&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 98304, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd9&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd9&#34;, &#34;duration&#34;: &#34;00:00:14&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd9&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:14&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd0&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd0&#34;, &#34;duration&#34;: &#34;252 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd0&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;252 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_fwd0&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2, 8),\n        head_dim=(32, 64, 72, 128),\n        block_sizes=(\n          ((&amp;quot;block_q&amp;quot;, 128), (&amp;quot;block_k&amp;quot;, 128)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 64)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 128)),\n        ),\n        causal=(True, False),\n        use_fwd=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_fwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_fwd,\n        use_segment_ids,\n    ):\n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      if use_fwd:\n    \n        @jax.jit\n        def impl(q, k, v):\n          v, _ = jax.vjp(\n              functools.partial(\n                  attention.mha,\n                  block_sizes=BlockSizes(**dict(block_sizes)),\n                  causal=causal,\n                  segment_ids=segment_ids,\n                  interpret=self.INTERPRET,\n              ),\n              q,\n              k,\n              v,\n          )\n          return v\n    \n      else:\n        impl = functools.partial(\n            attention.mha,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET,\n        )\n&amp;gt;     o = impl(q, k, v)\n          ^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:221: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f950802d760&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x841e5c50&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd0&#34;, &#34;duration&#34;: &#34;252 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd0&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;252 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_fwd0&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2, 8),\n        head_dim=(32, 64, 72, 128),\n        block_sizes=(\n          ((&amp;quot;block_q&amp;quot;, 128), (&amp;quot;block_k&amp;quot;, 128)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 64)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 128)),\n        ),\n        causal=(True, False),\n        use_fwd=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_fwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_fwd,\n        use_segment_ids,\n    ):\n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      if use_fwd:\n    \n        @jax.jit\n        def impl(q, k, v):\n          v, _ = jax.vjp(\n              functools.partial(\n                  attention.mha,\n                  block_sizes=BlockSizes(**dict(block_sizes)),\n                  causal=causal,\n                  segment_ids=segment_ids,\n                  interpret=self.INTERPRET,\n              ),\n              q,\n              k,\n              v,\n          )\n          return v\n    \n      else:\n        impl = functools.partial(\n            attention.mha,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET,\n        )\n&amp;gt;     o = impl(q, k, v)\n          ^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:221: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f95080ab8d0&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x83e18100&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd0&#34;, &#34;duration&#34;: &#34;252 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd0&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;252 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_fwd0&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2, 8),\n        head_dim=(32, 64, 72, 128),\n        block_sizes=(\n          ((&amp;quot;block_q&amp;quot;, 128), (&amp;quot;block_k&amp;quot;, 128)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 64)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 128)),\n        ),\n        causal=(True, False),\n        use_fwd=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_fwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_fwd,\n        use_segment_ids,\n    ):\n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      if use_fwd:\n    \n        @jax.jit\n        def impl(q, k, v):\n          v, _ = jax.vjp(\n              functools.partial(\n                  attention.mha,\n                  block_sizes=BlockSizes(**dict(block_sizes)),\n                  causal=causal,\n                  segment_ids=segment_ids,\n                  interpret=self.INTERPRET,\n              ),\n              q,\n              k,\n              v,\n          )\n          return v\n    \n      else:\n        impl = functools.partial(\n            attention.mha,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET,\n        )\n&amp;gt;     o = impl(q, k, v)\n          ^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:221: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f9508031530&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x840d4350&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd0&#34;, &#34;duration&#34;: &#34;252 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd0&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;252 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_fwd0&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2, 8),\n        head_dim=(32, 64, 72, 128),\n        block_sizes=(\n          ((&amp;quot;block_q&amp;quot;, 128), (&amp;quot;block_k&amp;quot;, 128)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 64)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 128)),\n        ),\n        causal=(True, False),\n        use_fwd=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_fwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_fwd,\n        use_segment_ids,\n    ):\n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      if use_fwd:\n    \n        @jax.jit\n        def impl(q, k, v):\n          v, _ = jax.vjp(\n              functools.partial(\n                  attention.mha,\n                  block_sizes=BlockSizes(**dict(block_sizes)),\n                  causal=causal,\n                  segment_ids=segment_ids,\n                  interpret=self.INTERPRET,\n              ),\n              q,\n              k,\n              v,\n          )\n          return v\n    \n      else:\n        impl = functools.partial(\n            attention.mha,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET,\n        )\n&amp;gt;     o = impl(q, k, v)\n          ^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:221: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f9460115ee0&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x816de140&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd1&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd1&#34;, &#34;duration&#34;: &#34;240 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd1&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;240 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_fwd1&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2, 8),\n        head_dim=(32, 64, 72, 128),\n        block_sizes=(\n          ((&amp;quot;block_q&amp;quot;, 128), (&amp;quot;block_k&amp;quot;, 128)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 64)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 128)),\n        ),\n        causal=(True, False),\n        use_fwd=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_fwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_fwd,\n        use_segment_ids,\n    ):\n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      if use_fwd:\n    \n        @jax.jit\n        def impl(q, k, v):\n          v, _ = jax.vjp(\n              functools.partial(\n                  attention.mha,\n                  block_sizes=BlockSizes(**dict(block_sizes)),\n                  causal=causal,\n                  segment_ids=segment_ids,\n                  interpret=self.INTERPRET,\n              ),\n              q,\n              k,\n              v,\n          )\n          return v\n    \n      else:\n        impl = functools.partial(\n            attention.mha,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET,\n        )\n&amp;gt;     o = impl(q, k, v)\n          ^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:221: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f9508033bf0&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x8158fd60&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd1&#34;, &#34;duration&#34;: &#34;240 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd1&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;240 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_fwd1&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2, 8),\n        head_dim=(32, 64, 72, 128),\n        block_sizes=(\n          ((&amp;quot;block_q&amp;quot;, 128), (&amp;quot;block_k&amp;quot;, 128)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 64)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 128)),\n        ),\n        causal=(True, False),\n        use_fwd=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_fwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_fwd,\n        use_segment_ids,\n    ):\n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      if use_fwd:\n    \n        @jax.jit\n        def impl(q, k, v):\n          v, _ = jax.vjp(\n              functools.partial(\n                  attention.mha,\n                  block_sizes=BlockSizes(**dict(block_sizes)),\n                  causal=causal,\n                  segment_ids=segment_ids,\n                  interpret=self.INTERPRET,\n              ),\n              q,\n              k,\n              v,\n          )\n          return v\n    \n      else:\n        impl = functools.partial(\n            attention.mha,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET,\n        )\n&amp;gt;     o = impl(q, k, v)\n          ^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:221: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f9460106570&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x82c8ef10&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd1&#34;, &#34;duration&#34;: &#34;240 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd1&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;240 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_fwd1&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2, 8),\n        head_dim=(32, 64, 72, 128),\n        block_sizes=(\n          ((&amp;quot;block_q&amp;quot;, 128), (&amp;quot;block_k&amp;quot;, 128)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 64)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 128)),\n        ),\n        causal=(True, False),\n        use_fwd=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_fwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_fwd,\n        use_segment_ids,\n    ):\n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      if use_fwd:\n    \n        @jax.jit\n        def impl(q, k, v):\n          v, _ = jax.vjp(\n              functools.partial(\n                  attention.mha,\n                  block_sizes=BlockSizes(**dict(block_sizes)),\n                  causal=causal,\n                  segment_ids=segment_ids,\n                  interpret=self.INTERPRET,\n              ),\n              q,\n              k,\n              v,\n          )\n          return v\n    \n      else:\n        impl = functools.partial(\n            attention.mha,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET,\n        )\n&amp;gt;     o = impl(q, k, v)\n          ^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:221: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7fb5bf6cf8d0&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x840d4350&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd1&#34;, &#34;duration&#34;: &#34;240 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd1&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;240 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_fwd1&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2, 8),\n        head_dim=(32, 64, 72, 128),\n        block_sizes=(\n          ((&amp;quot;block_q&amp;quot;, 128), (&amp;quot;block_k&amp;quot;, 128)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 64)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 128)),\n        ),\n        causal=(True, False),\n        use_fwd=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_fwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_fwd,\n        use_segment_ids,\n    ):\n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      if use_fwd:\n    \n        @jax.jit\n        def impl(q, k, v):\n          v, _ = jax.vjp(\n              functools.partial(\n                  attention.mha,\n                  block_sizes=BlockSizes(**dict(block_sizes)),\n                  causal=causal,\n                  segment_ids=segment_ids,\n                  interpret=self.INTERPRET,\n              ),\n              q,\n              k,\n              v,\n          )\n          return v\n    \n      else:\n        impl = functools.partial(\n            attention.mha,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET,\n        )\n&amp;gt;     o = impl(q, k, v)\n          ^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:221: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7fcdbfda4ef0&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x7dbc9370&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd2&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd2&#34;, &#34;duration&#34;: &#34;00:00:04&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd2&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:04&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd3&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd3&#34;, &#34;duration&#34;: &#34;00:00:02&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd3&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:02&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd4&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd4&#34;, &#34;duration&#34;: &#34;169 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd4&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;169 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_fwd4&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2, 8),\n        head_dim=(32, 64, 72, 128),\n        block_sizes=(\n          ((&amp;quot;block_q&amp;quot;, 128), (&amp;quot;block_k&amp;quot;, 128)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 64)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 128)),\n        ),\n        causal=(True, False),\n        use_fwd=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_fwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_fwd,\n        use_segment_ids,\n    ):\n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      if use_fwd:\n    \n        @jax.jit\n        def impl(q, k, v):\n          v, _ = jax.vjp(\n              functools.partial(\n                  attention.mha,\n                  block_sizes=BlockSizes(**dict(block_sizes)),\n                  causal=causal,\n                  segment_ids=segment_ids,\n                  interpret=self.INTERPRET,\n              ),\n              q,\n              k,\n              v,\n          )\n          return v\n    \n      else:\n        impl = functools.partial(\n            attention.mha,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET,\n        )\n&amp;gt;     o = impl(q, k, v)\n          ^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:221: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f94601dd440&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x849bcfc0&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 98304, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd4&#34;, &#34;duration&#34;: &#34;169 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd4&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;169 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_fwd4&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2, 8),\n        head_dim=(32, 64, 72, 128),\n        block_sizes=(\n          ((&amp;quot;block_q&amp;quot;, 128), (&amp;quot;block_k&amp;quot;, 128)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 64)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 128)),\n        ),\n        causal=(True, False),\n        use_fwd=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_fwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_fwd,\n        use_segment_ids,\n    ):\n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      if use_fwd:\n    \n        @jax.jit\n        def impl(q, k, v):\n          v, _ = jax.vjp(\n              functools.partial(\n                  attention.mha,\n                  block_sizes=BlockSizes(**dict(block_sizes)),\n                  causal=causal,\n                  segment_ids=segment_ids,\n                  interpret=self.INTERPRET,\n              ),\n              q,\n              k,\n              v,\n          )\n          return v\n    \n      else:\n        impl = functools.partial(\n            attention.mha,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET,\n        )\n&amp;gt;     o = impl(q, k, v)\n          ^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:221: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f94601f6520&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x8505ef00&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 98304, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd4&#34;, &#34;duration&#34;: &#34;169 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd4&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;169 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_fwd4&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2, 8),\n        head_dim=(32, 64, 72, 128),\n        block_sizes=(\n          ((&amp;quot;block_q&amp;quot;, 128), (&amp;quot;block_k&amp;quot;, 128)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 64)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 128)),\n        ),\n        causal=(True, False),\n        use_fwd=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_fwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_fwd,\n        use_segment_ids,\n    ):\n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      if use_fwd:\n    \n        @jax.jit\n        def impl(q, k, v):\n          v, _ = jax.vjp(\n              functools.partial(\n                  attention.mha,\n                  block_sizes=BlockSizes(**dict(block_sizes)),\n                  causal=causal,\n                  segment_ids=segment_ids,\n                  interpret=self.INTERPRET,\n              ),\n              q,\n              k,\n              v,\n          )\n          return v\n    \n      else:\n        impl = functools.partial(\n            attention.mha,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET,\n        )\n&amp;gt;     o = impl(q, k, v)\n          ^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:221: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f94601f5da0&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x849bcfc0&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 98304, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd4&#34;, &#34;duration&#34;: &#34;169 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd4&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;169 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_fwd4&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2, 8),\n        head_dim=(32, 64, 72, 128),\n        block_sizes=(\n          ((&amp;quot;block_q&amp;quot;, 128), (&amp;quot;block_k&amp;quot;, 128)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 64)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 128)),\n        ),\n        causal=(True, False),\n        use_fwd=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_fwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_fwd,\n        use_segment_ids,\n    ):\n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      if use_fwd:\n    \n        @jax.jit\n        def impl(q, k, v):\n          v, _ = jax.vjp(\n              functools.partial(\n                  attention.mha,\n                  block_sizes=BlockSizes(**dict(block_sizes)),\n                  causal=causal,\n                  segment_ids=segment_ids,\n                  interpret=self.INTERPRET,\n              ),\n              q,\n              k,\n              v,\n          )\n          return v\n    \n      else:\n        impl = functools.partial(\n            attention.mha,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET,\n        )\n&amp;gt;     o = impl(q, k, v)\n          ^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:221: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f94601d7830&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x853c26e0&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 98304, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd5&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd5&#34;, &#34;duration&#34;: &#34;00:00:04&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd5&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:04&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd6&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd6&#34;, &#34;duration&#34;: &#34;00:00:02&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd6&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:02&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd7&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd7&#34;, &#34;duration&#34;: &#34;141 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd7&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;141 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_fwd7&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2, 8),\n        head_dim=(32, 64, 72, 128),\n        block_sizes=(\n          ((&amp;quot;block_q&amp;quot;, 128), (&amp;quot;block_k&amp;quot;, 128)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 64)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 128)),\n        ),\n        causal=(True, False),\n        use_fwd=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_fwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_fwd,\n        use_segment_ids,\n    ):\n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      if use_fwd:\n    \n        @jax.jit\n        def impl(q, k, v):\n          v, _ = jax.vjp(\n              functools.partial(\n                  attention.mha,\n                  block_sizes=BlockSizes(**dict(block_sizes)),\n                  causal=causal,\n                  segment_ids=segment_ids,\n                  interpret=self.INTERPRET,\n              ),\n              q,\n              k,\n              v,\n          )\n          return v\n    \n      else:\n        impl = functools.partial(\n            attention.mha,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET,\n        )\n&amp;gt;     o = impl(q, k, v)\n          ^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:221: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:1210: in process_custom_vjp_call\n    return fun.call_wrapped(*tracers)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:87: in _flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:759: in _check_primal_refs\n    out = f(*args)\n          ^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f950870f560&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x82bea4b0&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 98304, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd7&#34;, &#34;duration&#34;: &#34;141 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd7&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;141 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_fwd7&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2, 8),\n        head_dim=(32, 64, 72, 128),\n        block_sizes=(\n          ((&amp;quot;block_q&amp;quot;, 128), (&amp;quot;block_k&amp;quot;, 128)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 64)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 128)),\n        ),\n        causal=(True, False),\n        use_fwd=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_fwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_fwd,\n        use_segment_ids,\n    ):\n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      if use_fwd:\n    \n        @jax.jit\n        def impl(q, k, v):\n          v, _ = jax.vjp(\n              functools.partial(\n                  attention.mha,\n                  block_sizes=BlockSizes(**dict(block_sizes)),\n                  causal=causal,\n                  segment_ids=segment_ids,\n                  interpret=self.INTERPRET,\n              ),\n              q,\n              k,\n              v,\n          )\n          return v\n    \n      else:\n        impl = functools.partial(\n            attention.mha,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET,\n        )\n&amp;gt;     o = impl(q, k, v)\n          ^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:221: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:1210: in process_custom_vjp_call\n    return fun.call_wrapped(*tracers)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:87: in _flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:759: in _check_primal_refs\n    out = f(*args)\n          ^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f950870f560&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x8301fcd0&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 98304, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Rerun&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd7&#34;, &#34;duration&#34;: &#34;141 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Rerun&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd7&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;141 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_fwd7&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2, 8),\n        head_dim=(32, 64, 72, 128),\n        block_sizes=(\n          ((&amp;quot;block_q&amp;quot;, 128), (&amp;quot;block_k&amp;quot;, 128)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 64)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 128)),\n        ),\n        causal=(True, False),\n        use_fwd=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_fwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_fwd,\n        use_segment_ids,\n    ):\n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      if use_fwd:\n    \n        @jax.jit\n        def impl(q, k, v):\n          v, _ = jax.vjp(\n              functools.partial(\n                  attention.mha,\n                  block_sizes=BlockSizes(**dict(block_sizes)),\n                  causal=causal,\n                  segment_ids=segment_ids,\n                  interpret=self.INTERPRET,\n              ),\n              q,\n              k,\n              v,\n          )\n          return v\n    \n      else:\n        impl = functools.partial(\n            attention.mha,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET,\n        )\n&amp;gt;     o = impl(q, k, v)\n          ^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:221: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:1210: in process_custom_vjp_call\n    return fun.call_wrapped(*tracers)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:87: in _flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:759: in _check_primal_refs\n    out = f(*args)\n          ^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f950870f560&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x85384280&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 98304, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}, {&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd7&#34;, &#34;duration&#34;: &#34;141 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd7&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;141 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_fwd7&amp;gt;\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2, 8),\n        head_dim=(32, 64, 72, 128),\n        block_sizes=(\n          ((&amp;quot;block_q&amp;quot;, 128), (&amp;quot;block_k&amp;quot;, 128)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 64)),\n          ((&amp;quot;block_q&amp;quot;, 64), (&amp;quot;block_k&amp;quot;, 128)),\n        ),\n        causal=(True, False),\n        use_fwd=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_fwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_fwd,\n        use_segment_ids,\n    ):\n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      if use_fwd:\n    \n        @jax.jit\n        def impl(q, k, v):\n          v, _ = jax.vjp(\n              functools.partial(\n                  attention.mha,\n                  block_sizes=BlockSizes(**dict(block_sizes)),\n                  causal=causal,\n                  segment_ids=segment_ids,\n                  interpret=self.INTERPRET,\n              ),\n              q,\n              k,\n              v,\n          )\n          return v\n    \n      else:\n        impl = functools.partial(\n            attention.mha,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET,\n        )\n&amp;gt;     o = impl(q, k, v)\n          ^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:221: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:1210: in process_custom_vjp_call\n    return fun.call_wrapped(*tracers)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:87: in _flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:759: in _check_primal_refs\n    out = f(*args)\n          ^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = &amp;lt;jaxlib._jax.Client object at 0x7fd5c5b18a00&amp;gt;\nmodule = &amp;lt;jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f950870f560&amp;gt;\nexecutable_devices = (RocmDevice(id=0),)\noptions = &amp;lt;jaxlib._jax.CompileOptions object at 0x853936a0&amp;gt;, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -&amp;gt; xc.LoadedExecutable:\n      sym_name = module.operation.attributes[&amp;#x27;sym_name&amp;#x27;]\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version &amp;lt; 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            &amp;quot;Compiling module %s with FDO profile of length %d&amp;quot;,\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don&amp;#x27;t have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n&amp;gt;         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 98304, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError\n&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd8&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd8&#34;, &#34;duration&#34;: &#34;00:00:04&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd8&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:04&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd9&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd9&#34;, &#34;duration&#34;: &#34;00:00:06&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd9&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:06&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_return_residuals_not_differentiable&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_return_residuals_not_differentiable&#34;, &#34;duration&#34;: &#34;00:00:05&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_return_residuals_not_differentiable&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:05&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd0&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd0&#34;, &#34;duration&#34;: &#34;00:00:03&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd0&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:03&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd1&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd1&#34;, &#34;duration&#34;: &#34;00:00:11&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd1&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:11&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd2&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd2&#34;, &#34;duration&#34;: &#34;00:00:03&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd2&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:03&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd3&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd3&#34;, &#34;duration&#34;: &#34;00:00:03&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd3&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:03&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd4&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd4&#34;, &#34;duration&#34;: &#34;00:00:04&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd4&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:04&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd5&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd5&#34;, &#34;duration&#34;: &#34;00:00:06&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd5&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:06&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd6&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd6&#34;, &#34;duration&#34;: &#34;00:00:02&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd6&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:02&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd7&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd7&#34;, &#34;duration&#34;: &#34;00:00:09&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd7&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:09&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd8&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd8&#34;, &#34;duration&#34;: &#34;00:00:10&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd8&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:10&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd9&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd9&#34;, &#34;duration&#34;: &#34;00:00:02&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd9&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:02&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd0&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd0&#34;, &#34;duration&#34;: &#34;00:00:03&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd0&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:03&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd1&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd1&#34;, &#34;duration&#34;: &#34;00:00:02&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd1&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:02&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd2&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd2&#34;, &#34;duration&#34;: &#34;00:00:01&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd2&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:01&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd3&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd3&#34;, &#34;duration&#34;: &#34;00:00:01&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd3&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:01&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd4&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd4&#34;, &#34;duration&#34;: &#34;00:00:01&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd4&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:01&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd5&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd5&#34;, &#34;duration&#34;: &#34;578 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd5&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;578 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd6&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd6&#34;, &#34;duration&#34;: &#34;568 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd6&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;568 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd7&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd7&#34;, &#34;duration&#34;: &#34;569 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd7&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;569 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd8&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd8&#34;, &#34;duration&#34;: &#34;596 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd8&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;596 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd9&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd9&#34;, &#34;duration&#34;: &#34;568 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd9&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;568 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_return_residuals_not_differentiable&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_return_residuals_not_differentiable&#34;, &#34;duration&#34;: &#34;00:00:01&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_return_residuals_not_differentiable&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:01&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_bwd0&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_bwd0&#34;, &#34;duration&#34;: &#34;00:00:02&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_bwd0&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:02&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_bwd1&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_bwd1&#34;, &#34;duration&#34;: &#34;00:00:02&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_bwd1&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:02&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_fwd0&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_fwd0&#34;, &#34;duration&#34;: &#34;347 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_fwd0&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;347 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_fwd1&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_fwd1&#34;, &#34;duration&#34;: &#34;339 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_fwd1&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;339 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_bwd0&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_bwd0&#34;, &#34;duration&#34;: &#34;200 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_bwd0&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;200 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_bwd1&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_bwd1&#34;, &#34;duration&#34;: &#34;217 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_bwd1&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;217 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_fwd0&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_fwd0&#34;, &#34;duration&#34;: &#34;4 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_fwd0&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;4 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_fwd1&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_fwd1&#34;, &#34;duration&#34;: &#34;12 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_fwd1&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;12 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_fwd0&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_fwd0&#34;, &#34;duration&#34;: &#34;351 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_fwd0&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;351 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_fwd1&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_fwd1&#34;, &#34;duration&#34;: &#34;379 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_fwd1&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;379 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_norm_bwd0&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_norm_bwd0&#34;, &#34;duration&#34;: &#34;931 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_norm_bwd0&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;931 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_norm_bwd1&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_norm_bwd1&#34;, &#34;duration&#34;: &#34;943 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_norm_bwd1&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;943 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_fwd0&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_fwd0&#34;, &#34;duration&#34;: &#34;4 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_fwd0&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;4 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_fwd1&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_fwd1&#34;, &#34;duration&#34;: &#34;15 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_fwd1&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;15 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_norm_bwd0&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_norm_bwd0&#34;, &#34;duration&#34;: &#34;196 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_norm_bwd0&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;196 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_norm_bwd1&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_norm_bwd1&#34;, &#34;duration&#34;: &#34;182 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_norm_bwd1&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;182 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax0&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax0&#34;, &#34;duration&#34;: &#34;00:00:02&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax0&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:02&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax1&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax1&#34;, &#34;duration&#34;: &#34;00:00:02&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax1&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:02&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax2&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax2&#34;, &#34;duration&#34;: &#34;00:00:01&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax2&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:01&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax3&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax3&#34;, &#34;duration&#34;: &#34;00:00:01&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax3&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:01&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax4&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax4&#34;, &#34;duration&#34;: &#34;00:00:01&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax4&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:01&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax5&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax5&#34;, &#34;duration&#34;: &#34;00:00:01&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax5&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:01&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax0&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax0&#34;, &#34;duration&#34;: &#34;8 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax0&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;8 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax1&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax1&#34;, &#34;duration&#34;: &#34;5 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax1&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;5 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax2&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax2&#34;, &#34;duration&#34;: &#34;5 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax2&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;5 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax3&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax3&#34;, &#34;duration&#34;: &#34;8 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax3&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;8 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax4&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax4&#34;, &#34;duration&#34;: &#34;7 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax4&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;7 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax5&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax5&#34;, &#34;duration&#34;: &#34;7 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax5&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;7 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}]}, &#34;renderCollapsed&#34;: [&#34;passed&#34;], &#34;initialSort&#34;: &#34;result&#34;, &#34;title&#34;: &#34;gpu_ops_test_log.html&#34;}"></div>
    <script>
      (function(){function r(e,n,t){function o(i,f){if(!n[i]){if(!e[i]){var c="function"==typeof require&&require;if(!f&&c)return c(i,!0);if(u)return u(i,!0);var a=new Error("Cannot find module '"+i+"'");throw a.code="MODULE_NOT_FOUND",a}var p=n[i]={exports:{}};e[i][0].call(p.exports,function(r){var n=e[i][1][r];return o(n||r)},p,p.exports,r,e,n,t)}return n[i].exports}for(var u="function"==typeof require&&require,i=0;i<t.length;i++)o(t[i]);return o}return r})()({1:[function(require,module,exports){
const { getCollapsedCategory, setCollapsedIds } = require('./storage.js')

class DataManager {
    setManager(data) {
        const collapsedCategories = [...getCollapsedCategory(data.renderCollapsed)]
        const collapsedIds = []
        const tests = Object.values(data.tests).flat().map((test, index) => {
            const collapsed = collapsedCategories.includes(test.result.toLowerCase())
            const id = `test_${index}`
            if (collapsed) {
                collapsedIds.push(id)
            }
            return {
                ...test,
                id,
                collapsed,
            }
        })
        const dataBlob = { ...data, tests }
        this.data = { ...dataBlob }
        this.renderData = { ...dataBlob }
        setCollapsedIds(collapsedIds)
    }

    get allData() {
        return { ...this.data }
    }

    resetRender() {
        this.renderData = { ...this.data }
    }

    setRender(data) {
        this.renderData.tests = [...data]
    }

    toggleCollapsedItem(id) {
        this.renderData.tests = this.renderData.tests.map((test) =>
            test.id === id ? { ...test, collapsed: !test.collapsed } : test,
        )
    }

    set allCollapsed(collapsed) {
        this.renderData = { ...this.renderData, tests: [...this.renderData.tests.map((test) => (
            { ...test, collapsed }
        ))] }
    }

    get testSubset() {
        return [...this.renderData.tests]
    }

    get environment() {
        return this.renderData.environment
    }

    get initialSort() {
        return this.data.initialSort
    }
}

module.exports = {
    manager: new DataManager(),
}

},{"./storage.js":8}],2:[function(require,module,exports){
const mediaViewer = require('./mediaviewer.js')
const templateEnvRow = document.getElementById('template_environment_row')
const templateResult = document.getElementById('template_results-table__tbody')

function htmlToElements(html) {
    const temp = document.createElement('template')
    temp.innerHTML = html
    return temp.content.childNodes
}

const find = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return elem.querySelector(selector)
}

const findAll = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return [...elem.querySelectorAll(selector)]
}

const dom = {
    getStaticRow: (key, value) => {
        const envRow = templateEnvRow.content.cloneNode(true)
        const isObj = typeof value === 'object' && value !== null
        const values = isObj ? Object.keys(value).map((k) => `${k}: ${value[k]}`) : null

        const valuesElement = htmlToElements(
            values ? `<ul>${values.map((val) => `<li>${val}</li>`).join('')}<ul>` : `<div>${value}</div>`)[0]
        const td = findAll('td', envRow)
        td[0].textContent = key
        td[1].appendChild(valuesElement)

        return envRow
    },
    getResultTBody: ({ testId, id, log, extras, resultsTableRow, tableHtml, result, collapsed }) => {
        const resultBody = templateResult.content.cloneNode(true)
        resultBody.querySelector('tbody').classList.add(result.toLowerCase())
        resultBody.querySelector('tbody').id = testId
        resultBody.querySelector('.collapsible').dataset.id = id

        resultsTableRow.forEach((html) => {
            const t = document.createElement('template')
            t.innerHTML = html
            resultBody.querySelector('.collapsible').appendChild(t.content)
        })

        if (log) {
            // Wrap lines starting with "E" with span.error to color those lines red
            const wrappedLog = log.replace(/^E.*$/gm, (match) => `<span class="error">${match}</span>`)
            resultBody.querySelector('.log').innerHTML = wrappedLog
        } else {
            resultBody.querySelector('.log').remove()
        }

        if (collapsed) {
            resultBody.querySelector('.collapsible > td')?.classList.add('collapsed')
            resultBody.querySelector('.extras-row').classList.add('hidden')
        } else {
            resultBody.querySelector('.collapsible > td')?.classList.remove('collapsed')
        }

        const media = []
        extras?.forEach(({ name, format_type, content }) => {
            if (['image', 'video'].includes(format_type)) {
                media.push({ path: content, name, format_type })
            }

            if (format_type === 'html') {
                resultBody.querySelector('.extraHTML').insertAdjacentHTML('beforeend', `<div>${content}</div>`)
            }
        })
        mediaViewer.setup(resultBody, media)

        // Add custom html from the pytest_html_results_table_html hook
        tableHtml?.forEach((item) => {
            resultBody.querySelector('td[class="extra"]').insertAdjacentHTML('beforeend', item)
        })

        return resultBody
    },
}

module.exports = {
    dom,
    htmlToElements,
    find,
    findAll,
}

},{"./mediaviewer.js":6}],3:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const storageModule = require('./storage.js')

const getFilteredSubSet = (filter) =>
    manager.allData.tests.filter(({ result }) => filter.includes(result.toLowerCase()))

const doInitFilter = () => {
    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)
}

const doFilter = (type, show) => {
    if (show) {
        storageModule.showCategory(type)
    } else {
        storageModule.hideCategory(type)
    }

    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)

    const sortColumn = storageModule.getSort()
    doSort(sortColumn, true)
}

module.exports = {
    doFilter,
    doInitFilter,
}

},{"./datamanager.js":1,"./sort.js":7,"./storage.js":8}],4:[function(require,module,exports){
const { redraw, bindEvents, renderStatic } = require('./main.js')
const { doInitFilter } = require('./filter.js')
const { doInitSort } = require('./sort.js')
const { manager } = require('./datamanager.js')
const data = JSON.parse(document.getElementById('data-container').dataset.jsonblob)

function init() {
    manager.setManager(data)
    doInitFilter()
    doInitSort()
    renderStatic()
    redraw()
    bindEvents()
}

init()

},{"./datamanager.js":1,"./filter.js":3,"./main.js":5,"./sort.js":7}],5:[function(require,module,exports){
const { dom, find, findAll } = require('./dom.js')
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const { doFilter } = require('./filter.js')
const {
    getVisible,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    getSortDirection,
    possibleFilters,
} = require('./storage.js')

const removeChildren = (node) => {
    while (node.firstChild) {
        node.removeChild(node.firstChild)
    }
}

const renderStatic = () => {
    const renderEnvironmentTable = () => {
        const environment = manager.environment
        const rows = Object.keys(environment).map((key) => dom.getStaticRow(key, environment[key]))
        const table = document.getElementById('environment')
        removeChildren(table)
        rows.forEach((row) => table.appendChild(row))
    }
    renderEnvironmentTable()
}

const addItemToggleListener = (elem) => {
    elem.addEventListener('click', ({ target }) => {
        const id = target.parentElement.dataset.id
        manager.toggleCollapsedItem(id)

        const collapsedIds = getCollapsedIds()
        if (collapsedIds.includes(id)) {
            const updated = collapsedIds.filter((item) => item !== id)
            setCollapsedIds(updated)
        } else {
            collapsedIds.push(id)
            setCollapsedIds(collapsedIds)
        }
        redraw()
    })
}

const renderContent = (tests) => {
    const sortAttr = getSort(manager.initialSort)
    const sortAsc = JSON.parse(getSortDirection())
    const rows = tests.map(dom.getResultTBody)
    const table = document.getElementById('results-table')
    const tableHeader = document.getElementById('results-table-head')

    const newTable = document.createElement('table')
    newTable.id = 'results-table'

    // remove all sorting classes and set the relevant
    findAll('.sortable', tableHeader).forEach((elem) => elem.classList.remove('asc', 'desc'))
    tableHeader.querySelector(`.sortable[data-column-type="${sortAttr}"]`)?.classList.add(sortAsc ? 'desc' : 'asc')
    newTable.appendChild(tableHeader)

    if (!rows.length) {
        const emptyTable = document.getElementById('template_results-table__body--empty').content.cloneNode(true)
        newTable.appendChild(emptyTable)
    } else {
        rows.forEach((row) => {
            if (!!row) {
                findAll('.collapsible td:not(.col-links', row).forEach(addItemToggleListener)
                find('.logexpander', row).addEventListener('click',
                    (evt) => evt.target.parentNode.classList.toggle('expanded'),
                )
                newTable.appendChild(row)
            }
        })
    }

    table.replaceWith(newTable)
}

const renderDerived = () => {
    const currentFilter = getVisible()
    possibleFilters.forEach((result) => {
        const input = document.querySelector(`input[data-test-result="${result}"]`)
        input.checked = currentFilter.includes(result)
    })
}

const bindEvents = () => {
    const filterColumn = (evt) => {
        const { target: element } = evt
        const { testResult } = element.dataset

        doFilter(testResult, element.checked)
        const collapsedIds = getCollapsedIds()
        const updated = manager.renderData.tests.map((test) => {
            return {
                ...test,
                collapsed: collapsedIds.includes(test.id),
            }
        })
        manager.setRender(updated)
        redraw()
    }

    const header = document.getElementById('environment-header')
    header.addEventListener('click', () => {
        const table = document.getElementById('environment')
        table.classList.toggle('hidden')
        header.classList.toggle('collapsed')
    })

    findAll('input[name="filter_checkbox"]').forEach((elem) => {
        elem.addEventListener('click', filterColumn)
    })

    findAll('.sortable').forEach((elem) => {
        elem.addEventListener('click', (evt) => {
            const { target: element } = evt
            const { columnType } = element.dataset
            doSort(columnType)
            redraw()
        })
    })

    document.getElementById('show_all_details').addEventListener('click', () => {
        manager.allCollapsed = false
        setCollapsedIds([])
        redraw()
    })
    document.getElementById('hide_all_details').addEventListener('click', () => {
        manager.allCollapsed = true
        const allIds = manager.renderData.tests.map((test) => test.id)
        setCollapsedIds(allIds)
        redraw()
    })
}

const redraw = () => {
    const { testSubset } = manager

    renderContent(testSubset)
    renderDerived()
}

module.exports = {
    redraw,
    bindEvents,
    renderStatic,
}

},{"./datamanager.js":1,"./dom.js":2,"./filter.js":3,"./sort.js":7,"./storage.js":8}],6:[function(require,module,exports){
class MediaViewer {
    constructor(assets) {
        this.assets = assets
        this.index = 0
    }

    nextActive() {
        this.index = this.index === this.assets.length - 1 ? 0 : this.index + 1
        return [this.activeFile, this.index]
    }

    prevActive() {
        this.index = this.index === 0 ? this.assets.length - 1 : this.index -1
        return [this.activeFile, this.index]
    }

    get currentIndex() {
        return this.index
    }

    get activeFile() {
        return this.assets[this.index]
    }
}


const setup = (resultBody, assets) => {
    if (!assets.length) {
        resultBody.querySelector('.media').classList.add('hidden')
        return
    }

    const mediaViewer = new MediaViewer(assets)
    const container = resultBody.querySelector('.media-container')
    const leftArrow = resultBody.querySelector('.media-container__nav--left')
    const rightArrow = resultBody.querySelector('.media-container__nav--right')
    const mediaName = resultBody.querySelector('.media__name')
    const counter = resultBody.querySelector('.media__counter')
    const imageEl = resultBody.querySelector('img')
    const sourceEl = resultBody.querySelector('source')
    const videoEl = resultBody.querySelector('video')

    const setImg = (media, index) => {
        if (media?.format_type === 'image') {
            imageEl.src = media.path

            imageEl.classList.remove('hidden')
            videoEl.classList.add('hidden')
        } else if (media?.format_type === 'video') {
            sourceEl.src = media.path

            videoEl.classList.remove('hidden')
            imageEl.classList.add('hidden')
        }

        mediaName.innerText = media?.name
        counter.innerText = `${index + 1} / ${assets.length}`
    }
    setImg(mediaViewer.activeFile, mediaViewer.currentIndex)

    const moveLeft = () => {
        const [media, index] = mediaViewer.prevActive()
        setImg(media, index)
    }
    const doRight = () => {
        const [media, index] = mediaViewer.nextActive()
        setImg(media, index)
    }
    const openImg = () => {
        window.open(mediaViewer.activeFile.path, '_blank')
    }
    if (assets.length === 1) {
        container.classList.add('media-container--fullscreen')
    } else {
        leftArrow.addEventListener('click', moveLeft)
        rightArrow.addEventListener('click', doRight)
    }
    imageEl.addEventListener('click', openImg)
}

module.exports = {
    setup,
}

},{}],7:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const storageModule = require('./storage.js')

const genericSort = (list, key, ascending, customOrder) => {
    let sorted
    if (customOrder) {
        sorted = list.sort((a, b) => {
            const aValue = a.result.toLowerCase()
            const bValue = b.result.toLowerCase()

            const aIndex = customOrder.findIndex((item) => item.toLowerCase() === aValue)
            const bIndex = customOrder.findIndex((item) => item.toLowerCase() === bValue)

            // Compare the indices to determine the sort order
            return aIndex - bIndex
        })
    } else {
        sorted = list.sort((a, b) => a[key] === b[key] ? 0 : a[key] > b[key] ? 1 : -1)
    }

    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const durationSort = (list, ascending) => {
    const parseDuration = (duration) => {
        if (duration.includes(':')) {
            // If it's in the format "HH:mm:ss"
            const [hours, minutes, seconds] = duration.split(':').map(Number)
            return (hours * 3600 + minutes * 60 + seconds) * 1000
        } else {
            // If it's in the format "nnn ms"
            return parseInt(duration)
        }
    }
    const sorted = list.sort((a, b) => parseDuration(a['duration']) - parseDuration(b['duration']))
    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const doInitSort = () => {
    const type = storageModule.getSort(manager.initialSort)
    const ascending = storageModule.getSortDirection()
    const list = manager.testSubset
    const initialOrder = ['Error', 'Failed', 'Rerun', 'XFailed', 'XPassed', 'Skipped', 'Passed']

    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    if (type?.toLowerCase() === 'original') {
        manager.setRender(list)
    } else {
        let sortedList
        switch (type) {
        case 'duration':
            sortedList = durationSort(list, ascending)
            break
        case 'result':
            sortedList = genericSort(list, type, ascending, initialOrder)
            break
        default:
            sortedList = genericSort(list, type, ascending)
            break
        }
        manager.setRender(sortedList)
    }
}

const doSort = (type, skipDirection) => {
    const newSortType = storageModule.getSort(manager.initialSort) !== type
    const currentAsc = storageModule.getSortDirection()
    let ascending
    if (skipDirection) {
        ascending = currentAsc
    } else {
        ascending = newSortType ? false : !currentAsc
    }
    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    const list = manager.testSubset
    const sortedList = type === 'duration' ? durationSort(list, ascending) : genericSort(list, type, ascending)
    manager.setRender(sortedList)
}

module.exports = {
    doInitSort,
    doSort,
}

},{"./datamanager.js":1,"./storage.js":8}],8:[function(require,module,exports){
const possibleFilters = [
    'passed',
    'skipped',
    'failed',
    'error',
    'xfailed',
    'xpassed',
    'rerun',
]

const getVisible = () => {
    const url = new URL(window.location.href)
    const settings = new URLSearchParams(url.search).get('visible')
    const lower = (item) => {
        const lowerItem = item.toLowerCase()
        if (possibleFilters.includes(lowerItem)) {
            return lowerItem
        }
        return null
    }
    return settings === null ?
        possibleFilters :
        [...new Set(settings?.split(',').map(lower).filter((item) => item))]
}

const hideCategory = (categoryToHide) => {
    const url = new URL(window.location.href)
    const visibleParams = new URLSearchParams(url.search).get('visible')
    const currentVisible = visibleParams ? visibleParams.split(',') : [...possibleFilters]
    const settings = [...new Set(currentVisible)].filter((f) => f !== categoryToHide).join(',')

    url.searchParams.set('visible', settings)
    window.history.pushState({}, null, unescape(url.href))
}

const showCategory = (categoryToShow) => {
    if (typeof window === 'undefined') {
        return
    }
    const url = new URL(window.location.href)
    const currentVisible = new URLSearchParams(url.search).get('visible')?.split(',').filter(Boolean) ||
        [...possibleFilters]
    const settings = [...new Set([categoryToShow, ...currentVisible])]
    const noFilter = possibleFilters.length === settings.length || !settings.length

    noFilter ? url.searchParams.delete('visible') : url.searchParams.set('visible', settings.join(','))
    window.history.pushState({}, null, unescape(url.href))
}

const getSort = (initialSort) => {
    const url = new URL(window.location.href)
    let sort = new URLSearchParams(url.search).get('sort')
    if (!sort) {
        sort = initialSort || 'result'
    }
    return sort
}

const setSort = (type) => {
    const url = new URL(window.location.href)
    url.searchParams.set('sort', type)
    window.history.pushState({}, null, unescape(url.href))
}

const getCollapsedCategory = (renderCollapsed) => {
    let categories
    if (typeof window !== 'undefined') {
        const url = new URL(window.location.href)
        const collapsedItems = new URLSearchParams(url.search).get('collapsed')
        switch (true) {
        case !renderCollapsed && collapsedItems === null:
            categories = ['passed']
            break
        case collapsedItems?.length === 0 || /^["']{2}$/.test(collapsedItems):
            categories = []
            break
        case /^all$/.test(collapsedItems) || collapsedItems === null && /^all$/.test(renderCollapsed):
            categories = [...possibleFilters]
            break
        default:
            categories = collapsedItems?.split(',').map((item) => item.toLowerCase()) || renderCollapsed
            break
        }
    } else {
        categories = []
    }
    return categories
}

const getSortDirection = () => JSON.parse(sessionStorage.getItem('sortAsc')) || false
const setSortDirection = (ascending) => sessionStorage.setItem('sortAsc', ascending)

const getCollapsedIds = () => JSON.parse(sessionStorage.getItem('collapsedIds')) || []
const setCollapsedIds = (list) => sessionStorage.setItem('collapsedIds', JSON.stringify(list))

module.exports = {
    getVisible,
    hideCategory,
    showCategory,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    setSort,
    getSortDirection,
    setSortDirection,
    getCollapsedCategory,
    possibleFilters,
}

},{}]},{},[4]);
    </script>
  </footer>
</html>