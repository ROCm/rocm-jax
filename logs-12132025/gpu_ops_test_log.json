{"created": 1765571063.158592, "duration": 191.42697095870972, "exitcode": 1, "root": "/dockerx/rocm/rocm-jax/jax", "environment": {}, "summary": {"passed": 61, "failed": 9, "total": 70, "collected": 70}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd0", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd1", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd2", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd3", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd4", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd5", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd6", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd7", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd8", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd9", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd0", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd1", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd2", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd3", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd4", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd5", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd6", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd7", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd8", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd9", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_return_residuals_not_differentiable", "type": "TestCaseFunction", "lineno": 308}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd0", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd1", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd2", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd3", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd4", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd5", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd6", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd7", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd8", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd9", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd0", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd1", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd2", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd3", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd4", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd5", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd6", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd7", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd8", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd9", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_return_residuals_not_differentiable", "type": "TestCaseFunction", "lineno": 308}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_bwd0", "type": "TestCaseFunction", "lineno": 358}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_bwd1", "type": "TestCaseFunction", "lineno": 358}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_fwd0", "type": "TestCaseFunction", "lineno": 344}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_fwd1", "type": "TestCaseFunction", "lineno": 344}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_bwd0", "type": "TestCaseFunction", "lineno": 358}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_bwd1", "type": "TestCaseFunction", "lineno": 358}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_fwd0", "type": "TestCaseFunction", "lineno": 344}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_fwd1", "type": "TestCaseFunction", "lineno": 344}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_fwd0", "type": "TestCaseFunction", "lineno": 392}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_fwd1", "type": "TestCaseFunction", "lineno": 392}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_norm_bwd0", "type": "TestCaseFunction", "lineno": 406}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_norm_bwd1", "type": "TestCaseFunction", "lineno": 406}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_fwd0", "type": "TestCaseFunction", "lineno": 392}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_fwd1", "type": "TestCaseFunction", "lineno": 392}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_norm_bwd0", "type": "TestCaseFunction", "lineno": 406}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_norm_bwd1", "type": "TestCaseFunction", "lineno": 406}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax0", "type": "TestCaseFunction", "lineno": 440}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax1", "type": "TestCaseFunction", "lineno": 440}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax2", "type": "TestCaseFunction", "lineno": 440}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax3", "type": "TestCaseFunction", "lineno": 440}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax4", "type": "TestCaseFunction", "lineno": 440}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax5", "type": "TestCaseFunction", "lineno": 440}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax0", "type": "TestCaseFunction", "lineno": 440}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax1", "type": "TestCaseFunction", "lineno": 440}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax2", "type": "TestCaseFunction", "lineno": 440}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax3", "type": "TestCaseFunction", "lineno": 440}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax4", "type": "TestCaseFunction", "lineno": 440}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax5", "type": "TestCaseFunction", "lineno": 440}]}], "tests": [{"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd0", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd0", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00031818822026252747, "outcome": "passed"}, "call": {"duration": 13.697676870040596, "outcome": "passed"}, "teardown": {"duration": 0.00015909411013126373, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd1", "lineno": 224, "outcome": "failed", "keywords": ["test_fused_attention_bwd1", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002611987292766571, "outcome": "passed"}, "call": {"duration": 0.4955293005332351, "outcome": "failed", "crash": {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536"}, "traceback": [{"path": "jax/tests/pallas/gpu_ops_test.py", "lineno": 302, "message": ""}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 427, "message": "in grad_f"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 505, "message": "in value_and_grad_f"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py", "lineno": 488, "message": "in __call__"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 2105, "message": "in _vjp_pullback_wrapper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py", "lineno": 488, "message": "in __call__"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 320, "message": "in unbound_vjp"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 438, "message": "in backward_pass"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 1559, "message": "in _custom_lin_transpose"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 212, "message": "in call_wrapped"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 921, "message": "in _flatten_bwd"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py", "lineno": 234, "message": "in _prepend_static_args"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 421, "message": "in _get_result_paths_thunk"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py", "lineno": 602, "message": "in _mha_backward"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 263, "message": "in cache_miss"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 146, "message": "in _python_pjit_helper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 1600, "message": "in _pjit_call_impl_python"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2527, "message": "in compile"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 3073, "message": "in from_hlo"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2854, "message": "in _cached_compilation"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 491, "message": "in compile_or_get_cached"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 759, "message": "in _compile_and_write_cache"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py", "lineno": 359, "message": "in wrapper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 385, "message": "in backend_compile_and_load"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "JaxRuntimeError"}], "longrepr": ">   dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n>   return attention.mha(\n        q, k, v,\n        block_sizes=BlockSizes(**dict(block_sizes)),\n        causal=causal,\n        segment_ids=segment_ids,\n        interpret=self.INTERPRET).sum()\nE   jax._src.source_info_util.JaxStackTraceBeforeTransformation: jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\nE   \nE   The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\nE   \nE   --------------------\n\njax/tests/pallas/gpu_ops_test.py:292: JaxStackTraceBeforeTransformation\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd1>\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (\"block_q\", 128),\n                (\"block_k\", 128),\n                (\"block_q_dkv\", 32),\n                (\"block_kv_dkv\", 32),\n                (\"block_q_dq\", 32),\n                (\"block_kv_dq\", 128),\n            ),\n            (\n                (\"block_q\", 64),\n                (\"block_k\", 64),\n                (\"block_q_dkv\", 64),\n                (\"block_kv_dkv\", 64),\n                (\"block_q_dq\", 64),\n                (\"block_kv_dq\", 64),\n            ),\n            (\n                (\"block_q\", 64),\n                (\"block_k\", 128),\n                (\"block_q_dkv\", 64),\n                (\"block_kv_dkv\", 32),\n                (\"block_q_dq\", 32),\n                (\"block_kv_dq\", 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(\"8.0\"):\n        # TODO(b/416306534)\n        self.skipTest(\"Precision issues after CUDA 12.8.1 upgrade\")\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n>     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:505: in value_and_grad_f\n    g = vjp_py(lax_internal._one(ans))\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2105: in _vjp_pullback_wrapper\n    ans = fun(*args)\n          ^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:320: in unbound_vjp\n    arg_cts = backward_pass(jaxpr, True, consts, dummy_args, cts)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:438: in backward_pass\n    cts_out = get_primitive_transpose(eqn.primitive)(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:1559: in _custom_lin_transpose\n    cts_in = bwd.call_wrapped(*res, *cts_out)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:921: in _flatten_bwd\n    py_cts_in = f(py_res, py_cts_out)\n                ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:234: in _prepend_static_args\n    return f(*all_args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:602: in _mha_backward\n    dq, dk, dv = pl.pallas_call(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = <jaxlib._jax.Client object at 0x7fd5c5b18a00>\nmodule = <jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7fd5bffcf1f0>\nexecutable_devices = (RocmDevice(id=0),)\noptions = <jaxlib._jax.CompileOptions object at 0x796cf480>, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -> xc.LoadedExecutable:\n      sym_name = module.operation.attributes['sym_name']\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version < 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            \"Compiling module %s with FDO profile of length %d\",\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n>         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError"}, "teardown": {"duration": 0.00023270491510629654, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd2", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd2", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.000224393792450428, "outcome": "passed"}, "call": {"duration": 8.510408885776997, "outcome": "passed"}, "teardown": {"duration": 0.00014484673738479614, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd3", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd3", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00021133385598659515, "outcome": "passed"}, "call": {"duration": 7.484941058792174, "outcome": "passed"}, "teardown": {"duration": 0.00014603417366743088, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd4", "lineno": 224, "outcome": "failed", "keywords": ["test_fused_attention_bwd4", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.000224393792450428, "outcome": "passed"}, "call": {"duration": 0.5139355724677444, "outcome": "failed", "crash": {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536"}, "traceback": [{"path": "jax/tests/pallas/gpu_ops_test.py", "lineno": 302, "message": ""}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 427, "message": "in grad_f"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 505, "message": "in value_and_grad_f"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py", "lineno": 488, "message": "in __call__"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 2105, "message": "in _vjp_pullback_wrapper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py", "lineno": 488, "message": "in __call__"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 320, "message": "in unbound_vjp"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 438, "message": "in backward_pass"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 1559, "message": "in _custom_lin_transpose"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 212, "message": "in call_wrapped"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 921, "message": "in _flatten_bwd"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py", "lineno": 234, "message": "in _prepend_static_args"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 421, "message": "in _get_result_paths_thunk"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py", "lineno": 602, "message": "in _mha_backward"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 263, "message": "in cache_miss"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 146, "message": "in _python_pjit_helper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 1600, "message": "in _pjit_call_impl_python"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2527, "message": "in compile"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 3073, "message": "in from_hlo"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2854, "message": "in _cached_compilation"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 491, "message": "in compile_or_get_cached"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 759, "message": "in _compile_and_write_cache"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py", "lineno": 359, "message": "in wrapper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 385, "message": "in backend_compile_and_load"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "JaxRuntimeError"}], "longrepr": ">   dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n>   return attention.mha(\n        q, k, v,\n        block_sizes=BlockSizes(**dict(block_sizes)),\n        causal=causal,\n        segment_ids=segment_ids,\n        interpret=self.INTERPRET).sum()\nE   jax._src.source_info_util.JaxStackTraceBeforeTransformation: jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\nE   \nE   The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\nE   \nE   --------------------\n\njax/tests/pallas/gpu_ops_test.py:292: JaxStackTraceBeforeTransformation\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd4>\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (\"block_q\", 128),\n                (\"block_k\", 128),\n                (\"block_q_dkv\", 32),\n                (\"block_kv_dkv\", 32),\n                (\"block_q_dq\", 32),\n                (\"block_kv_dq\", 128),\n            ),\n            (\n                (\"block_q\", 64),\n                (\"block_k\", 64),\n                (\"block_q_dkv\", 64),\n                (\"block_kv_dkv\", 64),\n                (\"block_q_dq\", 64),\n                (\"block_kv_dq\", 64),\n            ),\n            (\n                (\"block_q\", 64),\n                (\"block_k\", 128),\n                (\"block_q_dkv\", 64),\n                (\"block_kv_dkv\", 32),\n                (\"block_q_dq\", 32),\n                (\"block_kv_dq\", 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(\"8.0\"):\n        # TODO(b/416306534)\n        self.skipTest(\"Precision issues after CUDA 12.8.1 upgrade\")\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n>     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:505: in value_and_grad_f\n    g = vjp_py(lax_internal._one(ans))\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2105: in _vjp_pullback_wrapper\n    ans = fun(*args)\n          ^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:320: in unbound_vjp\n    arg_cts = backward_pass(jaxpr, True, consts, dummy_args, cts)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:438: in backward_pass\n    cts_out = get_primitive_transpose(eqn.primitive)(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:1559: in _custom_lin_transpose\n    cts_in = bwd.call_wrapped(*res, *cts_out)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:921: in _flatten_bwd\n    py_cts_in = f(py_res, py_cts_out)\n                ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:234: in _prepend_static_args\n    return f(*all_args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:602: in _mha_backward\n    dq, dk, dv = pl.pallas_call(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = <jaxlib._jax.Client object at 0x7fd5c5b18a00>\nmodule = <jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f9500765350>\nexecutable_devices = (RocmDevice(id=0),)\noptions = <jaxlib._jax.CompileOptions object at 0x7f05f680>, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -> xc.LoadedExecutable:\n      sym_name = module.operation.attributes['sym_name']\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version < 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            \"Compiling module %s with FDO profile of length %d\",\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n>         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError"}, "teardown": {"duration": 0.0002267695963382721, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd5", "lineno": 224, "outcome": "failed", "keywords": ["test_fused_attention_bwd5", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00025763828307390213, "outcome": "passed"}, "call": {"duration": 0.33635217044502497, "outcome": "failed", "crash": {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 81920, available: 65536"}, "traceback": [{"path": "jax/tests/pallas/gpu_ops_test.py", "lineno": 302, "message": ""}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 427, "message": "in grad_f"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 505, "message": "in value_and_grad_f"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py", "lineno": 488, "message": "in __call__"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 2105, "message": "in _vjp_pullback_wrapper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py", "lineno": 488, "message": "in __call__"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 320, "message": "in unbound_vjp"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 438, "message": "in backward_pass"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 1559, "message": "in _custom_lin_transpose"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 212, "message": "in call_wrapped"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 921, "message": "in _flatten_bwd"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py", "lineno": 234, "message": "in _prepend_static_args"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 421, "message": "in _get_result_paths_thunk"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py", "lineno": 602, "message": "in _mha_backward"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 263, "message": "in cache_miss"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 146, "message": "in _python_pjit_helper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 1600, "message": "in _pjit_call_impl_python"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2527, "message": "in compile"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 3073, "message": "in from_hlo"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2854, "message": "in _cached_compilation"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 491, "message": "in compile_or_get_cached"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 759, "message": "in _compile_and_write_cache"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py", "lineno": 359, "message": "in wrapper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 385, "message": "in backend_compile_and_load"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "JaxRuntimeError"}], "longrepr": ">   dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n>   return attention.mha(\n        q, k, v,\n        block_sizes=BlockSizes(**dict(block_sizes)),\n        causal=causal,\n        segment_ids=segment_ids,\n        interpret=self.INTERPRET).sum()\nE   jax._src.source_info_util.JaxStackTraceBeforeTransformation: jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 81920, available: 65536\nE   \nE   The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\nE   \nE   --------------------\n\njax/tests/pallas/gpu_ops_test.py:292: JaxStackTraceBeforeTransformation\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd5>\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (\"block_q\", 128),\n                (\"block_k\", 128),\n                (\"block_q_dkv\", 32),\n                (\"block_kv_dkv\", 32),\n                (\"block_q_dq\", 32),\n                (\"block_kv_dq\", 128),\n            ),\n            (\n                (\"block_q\", 64),\n                (\"block_k\", 64),\n                (\"block_q_dkv\", 64),\n                (\"block_kv_dkv\", 64),\n                (\"block_q_dq\", 64),\n                (\"block_kv_dq\", 64),\n            ),\n            (\n                (\"block_q\", 64),\n                (\"block_k\", 128),\n                (\"block_q_dkv\", 64),\n                (\"block_kv_dkv\", 32),\n                (\"block_q_dq\", 32),\n                (\"block_kv_dq\", 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(\"8.0\"):\n        # TODO(b/416306534)\n        self.skipTest(\"Precision issues after CUDA 12.8.1 upgrade\")\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n>     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:505: in value_and_grad_f\n    g = vjp_py(lax_internal._one(ans))\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2105: in _vjp_pullback_wrapper\n    ans = fun(*args)\n          ^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:320: in unbound_vjp\n    arg_cts = backward_pass(jaxpr, True, consts, dummy_args, cts)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:438: in backward_pass\n    cts_out = get_primitive_transpose(eqn.primitive)(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:1559: in _custom_lin_transpose\n    cts_in = bwd.call_wrapped(*res, *cts_out)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:921: in _flatten_bwd\n    py_cts_in = f(py_res, py_cts_out)\n                ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:234: in _prepend_static_args\n    return f(*all_args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:602: in _mha_backward\n    dq, dk, dv = pl.pallas_call(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = <jaxlib._jax.Client object at 0x7fd5c5b18a00>\nmodule = <jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f95007c9ee0>\nexecutable_devices = (RocmDevice(id=0),)\noptions = <jaxlib._jax.CompileOptions object at 0x7f96ff70>, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -> xc.LoadedExecutable:\n      sym_name = module.operation.attributes['sym_name']\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version < 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            \"Compiling module %s with FDO profile of length %d\",\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n>         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 81920, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError"}, "teardown": {"duration": 0.00023270398378372192, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd6", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd6", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002386411651968956, "outcome": "passed"}, "call": {"duration": 6.186631795018911, "outcome": "passed"}, "teardown": {"duration": 0.00018758978694677353, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd7", "lineno": 224, "outcome": "failed", "keywords": ["test_fused_attention_bwd7", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002944422885775566, "outcome": "passed"}, "call": {"duration": 0.22677198145538568, "outcome": "failed", "crash": {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536"}, "traceback": [{"path": "jax/tests/pallas/gpu_ops_test.py", "lineno": 302, "message": ""}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 427, "message": "in grad_f"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 500, "message": "in value_and_grad_f"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 2190, "message": "in _vjp"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 313, "message": "in vjp"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 287, "message": "in linearize"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 261, "message": "in direct_linearize"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 212, "message": "in call_wrapped"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py", "lineno": 90, "message": "in flatten_fun_nokwargs"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py", "lineno": 292, "message": "in _argnums_partial"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 421, "message": "in _get_result_paths_thunk"}, {"path": "jax/tests/pallas/gpu_ops_test.py", "lineno": 292, "message": "in f"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 749, "message": "in __call__"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/core.py", "lineno": 632, "message": "in bind"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/core.py", "lineno": 648, "message": "in _true_bind"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 1007, "message": "in bind_with_trace"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 979, "message": "in process_custom_vjp_call"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 212, "message": "in call_wrapped"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 850, "message": "in _flatten_fwd"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py", "lineno": 292, "message": "in _argnums_partial"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 421, "message": "in _get_result_paths_thunk"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py", "lineno": 314, "message": "in _mha_forward"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 749, "message": "in __call__"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/core.py", "lineno": 632, "message": "in bind"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/core.py", "lineno": 648, "message": "in _true_bind"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 1007, "message": "in bind_with_trace"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/core.py", "lineno": 1210, "message": "in process_custom_vjp_call"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 212, "message": "in call_wrapped"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 87, "message": "in _flatten_fun_nokwargs"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 759, "message": "in _check_primal_refs"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py", "lineno": 292, "message": "in _argnums_partial"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 421, "message": "in _get_result_paths_thunk"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 263, "message": "in cache_miss"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 146, "message": "in _python_pjit_helper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 1600, "message": "in _pjit_call_impl_python"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2527, "message": "in compile"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 3073, "message": "in from_hlo"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2854, "message": "in _cached_compilation"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 491, "message": "in compile_or_get_cached"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 759, "message": "in _compile_and_write_cache"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py", "lineno": 359, "message": "in wrapper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 385, "message": "in backend_compile_and_load"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "JaxRuntimeError"}], "longrepr": "self = <tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd7>\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (\"block_q\", 128),\n                (\"block_k\", 128),\n                (\"block_q_dkv\", 32),\n                (\"block_kv_dkv\", 32),\n                (\"block_q_dq\", 32),\n                (\"block_kv_dq\", 128),\n            ),\n            (\n                (\"block_q\", 64),\n                (\"block_k\", 64),\n                (\"block_q_dkv\", 64),\n                (\"block_kv_dkv\", 64),\n                (\"block_q_dq\", 64),\n                (\"block_kv_dq\", 64),\n            ),\n            (\n                (\"block_q\", 64),\n                (\"block_k\", 128),\n                (\"block_q_dkv\", 64),\n                (\"block_kv_dkv\", 32),\n                (\"block_q_dq\", 32),\n                (\"block_kv_dq\", 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(\"8.0\"):\n        # TODO(b/416306534)\n        self.skipTest(\"Precision issues after CUDA 12.8.1 upgrade\")\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n>     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:500: in value_and_grad_f\n    ans, vjp_py = _vjp(f_partial, *dyn_args)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2190: in _vjp\n    out_primals, vjp = ad.vjp(flat_fun, primals_flat)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:313: in vjp\n    out_primals, pvals, jaxpr, consts = linearize(traceable, *primals)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:287: in linearize\n    return direct_linearize(traceable, primals, kwargs, has_aux=has_aux)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:261: in direct_linearize\n    ans = traceable.call_wrapped(*tracers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:90: in flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\njax/tests/pallas/gpu_ops_test.py:292: in f\n    return attention.mha(\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:979: in process_custom_vjp_call\n    res_and_primals_out = fwd.call_wrapped(*fwd_in_flat)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:850: in _flatten_fwd\n    pair_out = f(*py_args)\n               ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:314: in _mha_forward\n    out, lse = mha(q, k, v, segment_ids=segment_ids, sm_scale=sm_scale,\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:1210: in process_custom_vjp_call\n    return fun.call_wrapped(*tracers)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:87: in _flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:759: in _check_primal_refs\n    out = f(*args)\n          ^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = <jaxlib._jax.Client object at 0x7fd5c5b18a00>\nmodule = <jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f9500142fc0>\nexecutable_devices = (RocmDevice(id=0),)\noptions = <jaxlib._jax.CompileOptions object at 0x7ecadff0>, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -> xc.LoadedExecutable:\n      sym_name = module.operation.attributes['sym_name']\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version < 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            \"Compiling module %s with FDO profile of length %d\",\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n>         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError"}, "teardown": {"duration": 0.0003158142790198326, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd8", "lineno": 224, "outcome": "failed", "keywords": ["test_fused_attention_bwd8", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002980036661028862, "outcome": "passed"}, "call": {"duration": 0.14847276639193296, "outcome": "failed", "crash": {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 98304, available: 65536"}, "traceback": [{"path": "jax/tests/pallas/gpu_ops_test.py", "lineno": 302, "message": ""}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 427, "message": "in grad_f"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 500, "message": "in value_and_grad_f"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 2190, "message": "in _vjp"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 313, "message": "in vjp"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 287, "message": "in linearize"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 261, "message": "in direct_linearize"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 212, "message": "in call_wrapped"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py", "lineno": 90, "message": "in flatten_fun_nokwargs"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py", "lineno": 292, "message": "in _argnums_partial"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 421, "message": "in _get_result_paths_thunk"}, {"path": "jax/tests/pallas/gpu_ops_test.py", "lineno": 292, "message": "in f"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 749, "message": "in __call__"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/core.py", "lineno": 632, "message": "in bind"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/core.py", "lineno": 648, "message": "in _true_bind"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 1007, "message": "in bind_with_trace"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 979, "message": "in process_custom_vjp_call"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 212, "message": "in call_wrapped"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 850, "message": "in _flatten_fwd"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py", "lineno": 292, "message": "in _argnums_partial"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 421, "message": "in _get_result_paths_thunk"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py", "lineno": 314, "message": "in _mha_forward"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 749, "message": "in __call__"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/core.py", "lineno": 632, "message": "in bind"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/core.py", "lineno": 648, "message": "in _true_bind"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 1007, "message": "in bind_with_trace"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/core.py", "lineno": 1210, "message": "in process_custom_vjp_call"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 212, "message": "in call_wrapped"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 87, "message": "in _flatten_fun_nokwargs"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 759, "message": "in _check_primal_refs"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py", "lineno": 292, "message": "in _argnums_partial"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 421, "message": "in _get_result_paths_thunk"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 263, "message": "in cache_miss"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 146, "message": "in _python_pjit_helper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 1600, "message": "in _pjit_call_impl_python"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2527, "message": "in compile"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 3073, "message": "in from_hlo"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2854, "message": "in _cached_compilation"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 491, "message": "in compile_or_get_cached"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 759, "message": "in _compile_and_write_cache"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py", "lineno": 359, "message": "in wrapper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 385, "message": "in backend_compile_and_load"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "JaxRuntimeError"}], "longrepr": "self = <tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd8>\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (\"block_q\", 128),\n                (\"block_k\", 128),\n                (\"block_q_dkv\", 32),\n                (\"block_kv_dkv\", 32),\n                (\"block_q_dq\", 32),\n                (\"block_kv_dq\", 128),\n            ),\n            (\n                (\"block_q\", 64),\n                (\"block_k\", 64),\n                (\"block_q_dkv\", 64),\n                (\"block_kv_dkv\", 64),\n                (\"block_q_dq\", 64),\n                (\"block_kv_dq\", 64),\n            ),\n            (\n                (\"block_q\", 64),\n                (\"block_k\", 128),\n                (\"block_q_dkv\", 64),\n                (\"block_kv_dkv\", 32),\n                (\"block_q_dq\", 32),\n                (\"block_kv_dq\", 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(\"8.0\"):\n        # TODO(b/416306534)\n        self.skipTest(\"Precision issues after CUDA 12.8.1 upgrade\")\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n>     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:500: in value_and_grad_f\n    ans, vjp_py = _vjp(f_partial, *dyn_args)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2190: in _vjp\n    out_primals, vjp = ad.vjp(flat_fun, primals_flat)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:313: in vjp\n    out_primals, pvals, jaxpr, consts = linearize(traceable, *primals)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:287: in linearize\n    return direct_linearize(traceable, primals, kwargs, has_aux=has_aux)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:261: in direct_linearize\n    ans = traceable.call_wrapped(*tracers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:90: in flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\njax/tests/pallas/gpu_ops_test.py:292: in f\n    return attention.mha(\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:979: in process_custom_vjp_call\n    res_and_primals_out = fwd.call_wrapped(*fwd_in_flat)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:850: in _flatten_fwd\n    pair_out = f(*py_args)\n               ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:314: in _mha_forward\n    out, lse = mha(q, k, v, segment_ids=segment_ids, sm_scale=sm_scale,\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:1210: in process_custom_vjp_call\n    return fun.call_wrapped(*tracers)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:87: in _flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:759: in _check_primal_refs\n    out = f(*args)\n          ^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = <jaxlib._jax.Client object at 0x7fd5c5b18a00>\nmodule = <jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f9500140130>\nexecutable_devices = (RocmDevice(id=0),)\noptions = <jaxlib._jax.CompileOptions object at 0x7dbc9370>, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -> xc.LoadedExecutable:\n      sym_name = module.operation.attributes['sym_name']\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version < 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            \"Compiling module %s with FDO profile of length %d\",\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n>         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 98304, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError"}, "teardown": {"duration": 0.000315813347697258, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd9", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd9", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00029563065618276596, "outcome": "passed"}, "call": {"duration": 14.204910915344954, "outcome": "passed"}, "teardown": {"duration": 0.00018877536058425903, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd0", "lineno": 151, "outcome": "failed", "keywords": ["test_fused_attention_fwd0", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00022201891988515854, "outcome": "passed"}, "call": {"duration": 0.25147788412868977, "outcome": "failed", "crash": {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536"}, "traceback": [{"path": "jax/tests/pallas/gpu_ops_test.py", "lineno": 221, "message": ""}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 263, "message": "in cache_miss"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 146, "message": "in _python_pjit_helper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 1600, "message": "in _pjit_call_impl_python"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2527, "message": "in compile"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 3073, "message": "in from_hlo"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2854, "message": "in _cached_compilation"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 491, "message": "in compile_or_get_cached"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 759, "message": "in _compile_and_write_cache"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py", "lineno": 359, "message": "in wrapper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 385, "message": "in backend_compile_and_load"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "JaxRuntimeError"}], "longrepr": "self = <tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_fwd0>\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2, 8),\n        head_dim=(32, 64, 72, 128),\n        block_sizes=(\n          ((\"block_q\", 128), (\"block_k\", 128)),\n          ((\"block_q\", 64), (\"block_k\", 64)),\n          ((\"block_q\", 64), (\"block_k\", 128)),\n        ),\n        causal=(True, False),\n        use_fwd=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_fwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_fwd,\n        use_segment_ids,\n    ):\n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      if use_fwd:\n    \n        @jax.jit\n        def impl(q, k, v):\n          v, _ = jax.vjp(\n              functools.partial(\n                  attention.mha,\n                  block_sizes=BlockSizes(**dict(block_sizes)),\n                  causal=causal,\n                  segment_ids=segment_ids,\n                  interpret=self.INTERPRET,\n              ),\n              q,\n              k,\n              v,\n          )\n          return v\n    \n      else:\n        impl = functools.partial(\n            attention.mha,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET,\n        )\n>     o = impl(q, k, v)\n          ^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:221: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = <jaxlib._jax.Client object at 0x7fd5c5b18a00>\nmodule = <jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f9460115ee0>\nexecutable_devices = (RocmDevice(id=0),)\noptions = <jaxlib._jax.CompileOptions object at 0x816de140>, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -> xc.LoadedExecutable:\n      sym_name = module.operation.attributes['sym_name']\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version < 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            \"Compiling module %s with FDO profile of length %d\",\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n>         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError"}, "teardown": {"duration": 0.00021845661103725433, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd1", "lineno": 151, "outcome": "failed", "keywords": ["test_fused_attention_fwd1", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002742605283856392, "outcome": "passed"}, "call": {"duration": 0.23970136232674122, "outcome": "failed", "crash": {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536"}, "traceback": [{"path": "jax/tests/pallas/gpu_ops_test.py", "lineno": 221, "message": ""}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 263, "message": "in cache_miss"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 146, "message": "in _python_pjit_helper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 1600, "message": "in _pjit_call_impl_python"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2527, "message": "in compile"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 3073, "message": "in from_hlo"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2854, "message": "in _cached_compilation"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 491, "message": "in compile_or_get_cached"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 759, "message": "in _compile_and_write_cache"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py", "lineno": 359, "message": "in wrapper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 385, "message": "in backend_compile_and_load"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "JaxRuntimeError"}], "longrepr": "self = <tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_fwd1>\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2, 8),\n        head_dim=(32, 64, 72, 128),\n        block_sizes=(\n          ((\"block_q\", 128), (\"block_k\", 128)),\n          ((\"block_q\", 64), (\"block_k\", 64)),\n          ((\"block_q\", 64), (\"block_k\", 128)),\n        ),\n        causal=(True, False),\n        use_fwd=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_fwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_fwd,\n        use_segment_ids,\n    ):\n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      if use_fwd:\n    \n        @jax.jit\n        def impl(q, k, v):\n          v, _ = jax.vjp(\n              functools.partial(\n                  attention.mha,\n                  block_sizes=BlockSizes(**dict(block_sizes)),\n                  causal=causal,\n                  segment_ids=segment_ids,\n                  interpret=self.INTERPRET,\n              ),\n              q,\n              k,\n              v,\n          )\n          return v\n    \n      else:\n        impl = functools.partial(\n            attention.mha,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET,\n        )\n>     o = impl(q, k, v)\n          ^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:221: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = <jaxlib._jax.Client object at 0x7fd5c5b18a00>\nmodule = <jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7fcdbfda4ef0>\nexecutable_devices = (RocmDevice(id=0),)\noptions = <jaxlib._jax.CompileOptions object at 0x7dbc9370>, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -> xc.LoadedExecutable:\n      sym_name = module.operation.attributes['sym_name']\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version < 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            \"Compiling module %s with FDO profile of length %d\",\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n>         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError"}, "teardown": {"duration": 0.00028494372963905334, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd2", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd2", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002683233469724655, "outcome": "passed"}, "call": {"duration": 3.5947487261146307, "outcome": "passed"}, "teardown": {"duration": 0.0001507829874753952, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd3", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd3", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002089599147439003, "outcome": "passed"}, "call": {"duration": 2.0486355926841497, "outcome": "passed"}, "teardown": {"duration": 0.00014840904623270035, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd4", "lineno": 151, "outcome": "failed", "keywords": ["test_fused_attention_fwd4", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00022676773369312286, "outcome": "passed"}, "call": {"duration": 0.16825387254357338, "outcome": "failed", "crash": {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 98304, available: 65536"}, "traceback": [{"path": "jax/tests/pallas/gpu_ops_test.py", "lineno": 221, "message": ""}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 263, "message": "in cache_miss"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 146, "message": "in _python_pjit_helper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 1600, "message": "in _pjit_call_impl_python"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2527, "message": "in compile"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 3073, "message": "in from_hlo"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2854, "message": "in _cached_compilation"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 491, "message": "in compile_or_get_cached"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 759, "message": "in _compile_and_write_cache"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py", "lineno": 359, "message": "in wrapper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 385, "message": "in backend_compile_and_load"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "JaxRuntimeError"}], "longrepr": "self = <tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_fwd4>\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2, 8),\n        head_dim=(32, 64, 72, 128),\n        block_sizes=(\n          ((\"block_q\", 128), (\"block_k\", 128)),\n          ((\"block_q\", 64), (\"block_k\", 64)),\n          ((\"block_q\", 64), (\"block_k\", 128)),\n        ),\n        causal=(True, False),\n        use_fwd=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_fwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_fwd,\n        use_segment_ids,\n    ):\n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      if use_fwd:\n    \n        @jax.jit\n        def impl(q, k, v):\n          v, _ = jax.vjp(\n              functools.partial(\n                  attention.mha,\n                  block_sizes=BlockSizes(**dict(block_sizes)),\n                  causal=causal,\n                  segment_ids=segment_ids,\n                  interpret=self.INTERPRET,\n              ),\n              q,\n              k,\n              v,\n          )\n          return v\n    \n      else:\n        impl = functools.partial(\n            attention.mha,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET,\n        )\n>     o = impl(q, k, v)\n          ^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:221: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = <jaxlib._jax.Client object at 0x7fd5c5b18a00>\nmodule = <jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f94601d7830>\nexecutable_devices = (RocmDevice(id=0),)\noptions = <jaxlib._jax.CompileOptions object at 0x853c26e0>, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -> xc.LoadedExecutable:\n      sym_name = module.operation.attributes['sym_name']\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version < 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            \"Compiling module %s with FDO profile of length %d\",\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n>         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 98304, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError"}, "teardown": {"duration": 0.0002232072874903679, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd5", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd5", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00023982767015695572, "outcome": "passed"}, "call": {"duration": 4.187469291500747, "outcome": "passed"}, "teardown": {"duration": 0.000147220678627491, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd6", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd6", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00020777247846126556, "outcome": "passed"}, "call": {"duration": 2.214040077291429, "outcome": "passed"}, "teardown": {"duration": 0.0001424727961421013, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd7", "lineno": 151, "outcome": "failed", "keywords": ["test_fused_attention_fwd7", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002160836011171341, "outcome": "passed"}, "call": {"duration": 0.1409478709101677, "outcome": "failed", "crash": {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 98304, available: 65536"}, "traceback": [{"path": "jax/tests/pallas/gpu_ops_test.py", "lineno": 221, "message": ""}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 749, "message": "in __call__"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/core.py", "lineno": 632, "message": "in bind"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/core.py", "lineno": 648, "message": "in _true_bind"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 1007, "message": "in bind_with_trace"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/core.py", "lineno": 1210, "message": "in process_custom_vjp_call"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 212, "message": "in call_wrapped"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 87, "message": "in _flatten_fun_nokwargs"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 759, "message": "in _check_primal_refs"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py", "lineno": 292, "message": "in _argnums_partial"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 421, "message": "in _get_result_paths_thunk"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 263, "message": "in cache_miss"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 146, "message": "in _python_pjit_helper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 1600, "message": "in _pjit_call_impl_python"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2527, "message": "in compile"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 3073, "message": "in from_hlo"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2854, "message": "in _cached_compilation"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 491, "message": "in compile_or_get_cached"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 759, "message": "in _compile_and_write_cache"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py", "lineno": 359, "message": "in wrapper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 385, "message": "in backend_compile_and_load"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "JaxRuntimeError"}], "longrepr": "self = <tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_fwd7>\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2, 8),\n        head_dim=(32, 64, 72, 128),\n        block_sizes=(\n          ((\"block_q\", 128), (\"block_k\", 128)),\n          ((\"block_q\", 64), (\"block_k\", 64)),\n          ((\"block_q\", 64), (\"block_k\", 128)),\n        ),\n        causal=(True, False),\n        use_fwd=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_fwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_fwd,\n        use_segment_ids,\n    ):\n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      if use_fwd:\n    \n        @jax.jit\n        def impl(q, k, v):\n          v, _ = jax.vjp(\n              functools.partial(\n                  attention.mha,\n                  block_sizes=BlockSizes(**dict(block_sizes)),\n                  causal=causal,\n                  segment_ids=segment_ids,\n                  interpret=self.INTERPRET,\n              ),\n              q,\n              k,\n              v,\n          )\n          return v\n    \n      else:\n        impl = functools.partial(\n            attention.mha,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET,\n        )\n>     o = impl(q, k, v)\n          ^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:221: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:1210: in process_custom_vjp_call\n    return fun.call_wrapped(*tracers)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:87: in _flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:759: in _check_primal_refs\n    out = f(*args)\n          ^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = <jaxlib._jax.Client object at 0x7fd5c5b18a00>\nmodule = <jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f950870f560>\nexecutable_devices = (RocmDevice(id=0),)\noptions = <jaxlib._jax.CompileOptions object at 0x853936a0>, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -> xc.LoadedExecutable:\n      sym_name = module.operation.attributes['sym_name']\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version < 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            \"Compiling module %s with FDO profile of length %d\",\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n>         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 98304, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError"}, "teardown": {"duration": 0.00023270491510629654, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd8", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd8", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00021370872855186462, "outcome": "passed"}, "call": {"duration": 4.2258799420669675, "outcome": "passed"}, "teardown": {"duration": 0.00014840904623270035, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd9", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd9", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00020895898342132568, "outcome": "passed"}, "call": {"duration": 6.325111215002835, "outcome": "passed"}, "teardown": {"duration": 0.00019115116447210312, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_return_residuals_not_differentiable", "lineno": 308, "outcome": "passed", "keywords": ["test_return_residuals_not_differentiable", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00026594754308462143, "outcome": "passed"}, "call": {"duration": 4.912139622494578, "outcome": "passed"}, "teardown": {"duration": 0.00017571542412042618, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd0", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd0", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00024695228785276413, "outcome": "passed"}, "call": {"duration": 2.768917710520327, "outcome": "passed"}, "teardown": {"duration": 0.00017571542412042618, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd1", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd1", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00023982767015695572, "outcome": "passed"}, "call": {"duration": 11.199563842266798, "outcome": "passed"}, "teardown": {"duration": 0.00018996279686689377, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd2", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd2", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002611987292766571, "outcome": "passed"}, "call": {"duration": 2.589624705724418, "outcome": "passed"}, "teardown": {"duration": 0.0001911502331495285, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd3", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd3", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002683233469724655, "outcome": "passed"}, "call": {"duration": 3.2396737718954682, "outcome": "passed"}, "teardown": {"duration": 0.00018877536058425903, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd4", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd4", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00025882478803396225, "outcome": "passed"}, "call": {"duration": 4.396943390369415, "outcome": "passed"}, "teardown": {"duration": 0.00018283911049365997, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd5", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd5", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00026001129299402237, "outcome": "passed"}, "call": {"duration": 5.713206256739795, "outcome": "passed"}, "teardown": {"duration": 0.0001947116106748581, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd6", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd6", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002576364204287529, "outcome": "passed"}, "call": {"duration": 1.7704610852524638, "outcome": "passed"}, "teardown": {"duration": 0.00018165167421102524, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd7", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd7", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002457639202475548, "outcome": "passed"}, "call": {"duration": 9.397853558883071, "outcome": "passed"}, "teardown": {"duration": 0.00023626722395420074, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd8", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd8", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0003490569069981575, "outcome": "passed"}, "call": {"duration": 9.733449820429087, "outcome": "passed"}, "teardown": {"duration": 0.00014959648251533508, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd9", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd9", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00021845754235982895, "outcome": "passed"}, "call": {"duration": 1.7464652871713042, "outcome": "passed"}, "teardown": {"duration": 0.00014959648251533508, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd0", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd0", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00019946228712797165, "outcome": "passed"}, "call": {"duration": 2.806705379858613, "outcome": "passed"}, "teardown": {"duration": 0.0001507829874753952, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd1", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd1", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00020777154713869095, "outcome": "passed"}, "call": {"duration": 2.20291586779058, "outcome": "passed"}, "teardown": {"duration": 0.00014484673738479614, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd2", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd2", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00020421016961336136, "outcome": "passed"}, "call": {"duration": 1.400995976291597, "outcome": "passed"}, "teardown": {"duration": 0.00014959648251533508, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd3", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd3", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00021608266979455948, "outcome": "passed"}, "call": {"duration": 1.3277497561648488, "outcome": "passed"}, "teardown": {"duration": 0.00014840904623270035, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd4", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd4", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00021133385598659515, "outcome": "passed"}, "call": {"duration": 1.4915846614167094, "outcome": "passed"}, "teardown": {"duration": 0.00014603417366743088, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd5", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd5", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00021845754235982895, "outcome": "passed"}, "call": {"duration": 0.5779521781951189, "outcome": "passed"}, "teardown": {"duration": 0.00014603417366743088, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd6", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd6", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002089599147439003, "outcome": "passed"}, "call": {"duration": 0.5673747938126326, "outcome": "passed"}, "teardown": {"duration": 0.00014840811491012573, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd7", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd7", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00021133385598659515, "outcome": "passed"}, "call": {"duration": 0.5690832762047648, "outcome": "passed"}, "teardown": {"duration": 0.0001507829874753952, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd8", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd8", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00026238709688186646, "outcome": "passed"}, "call": {"duration": 0.5951106017455459, "outcome": "passed"}, "teardown": {"duration": 0.0001472216099500656, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd9", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd9", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00021370779722929, "outcome": "passed"}, "call": {"duration": 0.567821211181581, "outcome": "passed"}, "teardown": {"duration": 0.00016028154641389847, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_return_residuals_not_differentiable", "lineno": 308, "outcome": "passed", "keywords": ["test_return_residuals_not_differentiable", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002125212922692299, "outcome": "passed"}, "call": {"duration": 1.3032754017040133, "outcome": "passed"}, "teardown": {"duration": 0.0001721540465950966, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_bwd0", "lineno": 358, "outcome": "passed", "keywords": ["test_fused_layernorm_bwd0", "__wrapped__", "__x_params_repr__", "FusedLayerNormTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002457648515701294, "outcome": "passed"}, "call": {"duration": 2.3366274321451783, "outcome": "passed"}, "teardown": {"duration": 0.00014959648251533508, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_bwd1", "lineno": 358, "outcome": "passed", "keywords": ["test_fused_layernorm_bwd1", "__wrapped__", "__x_params_repr__", "FusedLayerNormTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00026238616555929184, "outcome": "passed"}, "call": {"duration": 2.1473434176295996, "outcome": "passed"}, "teardown": {"duration": 0.00015672016888856888, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_fwd0", "lineno": 344, "outcome": "passed", "keywords": ["test_fused_layernorm_fwd0", "__wrapped__", "__x_params_repr__", "FusedLayerNormTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002160836011171341, "outcome": "passed"}, "call": {"duration": 0.3464536014944315, "outcome": "passed"}, "teardown": {"duration": 0.0001507829874753952, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_fwd1", "lineno": 344, "outcome": "passed", "keywords": ["test_fused_layernorm_fwd1", "__wrapped__", "__x_params_repr__", "FusedLayerNormTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00022558216005563736, "outcome": "passed"}, "call": {"duration": 0.33851432986557484, "outcome": "passed"}, "teardown": {"duration": 0.00018758885562419891, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_bwd0", "lineno": 358, "outcome": "passed", "keywords": ["test_fused_layernorm_bwd0", "__wrapped__", "__x_params_repr__", "FusedLayerNormInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002932557836174965, "outcome": "passed"}, "call": {"duration": 0.1994470590725541, "outcome": "passed"}, "teardown": {"duration": 0.0001472216099500656, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_bwd1", "lineno": 358, "outcome": "passed", "keywords": ["test_fused_layernorm_bwd1", "__wrapped__", "__x_params_repr__", "FusedLayerNormInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00021252036094665527, "outcome": "passed"}, "call": {"duration": 0.21668383944779634, "outcome": "passed"}, "teardown": {"duration": 0.00013891048729419708, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_fwd0", "lineno": 344, "outcome": "passed", "keywords": ["test_fused_layernorm_fwd0", "__wrapped__", "__x_params_repr__", "FusedLayerNormInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00020064879208803177, "outcome": "passed"}, "call": {"duration": 0.0034585166722536087, "outcome": "passed"}, "teardown": {"duration": 0.00010091811418533325, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_fwd1", "lineno": 344, "outcome": "passed", "keywords": ["test_fused_layernorm_fwd1", "__wrapped__", "__x_params_repr__", "FusedLayerNormInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00016028247773647308, "outcome": "passed"}, "call": {"duration": 0.01199261099100113, "outcome": "passed"}, "teardown": {"duration": 0.00013059936463832855, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_fwd0", "lineno": 392, "outcome": "passed", "keywords": ["test_rms_fwd0", "__wrapped__", "__x_params_repr__", "RmsNormTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00020421110093593597, "outcome": "passed"}, "call": {"duration": 0.35049506928771734, "outcome": "passed"}, "teardown": {"duration": 0.00014840904623270035, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_fwd1", "lineno": 392, "outcome": "passed", "keywords": ["test_rms_fwd1", "__wrapped__", "__x_params_repr__", "RmsNormTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002065841108560562, "outcome": "passed"}, "call": {"duration": 0.37820950616151094, "outcome": "passed"}, "teardown": {"duration": 0.0001864023506641388, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_norm_bwd0", "lineno": 406, "outcome": "passed", "keywords": ["test_rms_norm_bwd0", "__wrapped__", "__x_params_repr__", "RmsNormTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002647610381245613, "outcome": "passed"}, "call": {"duration": 0.9302756609395146, "outcome": "passed"}, "teardown": {"duration": 0.0001543452963232994, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_norm_bwd1", "lineno": 406, "outcome": "passed", "keywords": ["test_rms_norm_bwd1", "__wrapped__", "__x_params_repr__", "RmsNormTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00020777247846126556, "outcome": "passed"}, "call": {"duration": 0.9425805257633328, "outcome": "passed"}, "teardown": {"duration": 0.0001911502331495285, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_fwd0", "lineno": 392, "outcome": "passed", "keywords": ["test_rms_fwd0", "__wrapped__", "__x_params_repr__", "RmsNormInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002540759742259979, "outcome": "passed"}, "call": {"duration": 0.0037755174562335014, "outcome": "passed"}, "teardown": {"duration": 9.854231029748917e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_fwd1", "lineno": 392, "outcome": "passed", "keywords": ["test_rms_fwd1", "__wrapped__", "__x_params_repr__", "RmsNormInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00017927773296833038, "outcome": "passed"}, "call": {"duration": 0.014148692600429058, "outcome": "passed"}, "teardown": {"duration": 0.000224393792450428, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_norm_bwd0", "lineno": 406, "outcome": "passed", "keywords": ["test_rms_norm_bwd0", "__wrapped__", "__x_params_repr__", "RmsNormInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0003739902749657631, "outcome": "passed"}, "call": {"duration": 0.19516576640307903, "outcome": "passed"}, "teardown": {"duration": 0.00015553273260593414, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_norm_bwd1", "lineno": 406, "outcome": "passed", "keywords": ["test_rms_norm_bwd1", "__wrapped__", "__x_params_repr__", "RmsNormInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001982739195227623, "outcome": "passed"}, "call": {"duration": 0.1812129719182849, "outcome": "passed"}, "teardown": {"duration": 0.000168592669069767, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax0", "lineno": 440, "outcome": "passed", "keywords": ["test_softmax0", "__wrapped__", "__x_params_repr__", "SoftmaxTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.000235079787671566, "outcome": "passed"}, "call": {"duration": 1.6092490833252668, "outcome": "passed"}, "teardown": {"duration": 0.0001947125419974327, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax1", "lineno": 440, "outcome": "passed", "keywords": ["test_softmax1", "__wrapped__", "__x_params_repr__", "SoftmaxTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002505127340555191, "outcome": "passed"}, "call": {"duration": 1.5212640948593616, "outcome": "passed"}, "teardown": {"duration": 0.0002825707197189331, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax2", "lineno": 440, "outcome": "passed", "keywords": ["test_softmax2", "__wrapped__", "__x_params_repr__", "SoftmaxTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00040842127054929733, "outcome": "passed"}, "call": {"duration": 1.435570535250008, "outcome": "passed"}, "teardown": {"duration": 0.00027782097458839417, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax3", "lineno": 440, "outcome": "passed", "keywords": ["test_softmax3", "__wrapped__", "__x_params_repr__", "SoftmaxTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00038705021142959595, "outcome": "passed"}, "call": {"duration": 1.4052417371422052, "outcome": "passed"}, "teardown": {"duration": 0.0003051292151212692, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax4", "lineno": 440, "outcome": "passed", "keywords": ["test_softmax4", "__wrapped__", "__x_params_repr__", "SoftmaxTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00039179902523756027, "outcome": "passed"}, "call": {"duration": 1.4534116489812732, "outcome": "passed"}, "teardown": {"duration": 0.00023745279759168625, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax5", "lineno": 440, "outcome": "passed", "keywords": ["test_softmax5", "__wrapped__", "__x_params_repr__", "SoftmaxTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0003514327108860016, "outcome": "passed"}, "call": {"duration": 1.237024625763297, "outcome": "passed"}, "teardown": {"duration": 0.0002730712294578552, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax0", "lineno": 440, "outcome": "passed", "keywords": ["test_softmax0", "__wrapped__", "__x_params_repr__", "SoftmaxInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00045116152614355087, "outcome": "passed"}, "call": {"duration": 0.007192480377852917, "outcome": "passed"}, "teardown": {"duration": 0.00018521398305892944, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax1", "lineno": 440, "outcome": "passed", "keywords": ["test_softmax1", "__wrapped__", "__x_params_repr__", "SoftmaxInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00030275341123342514, "outcome": "passed"}, "call": {"duration": 0.0049497270956635475, "outcome": "passed"}, "teardown": {"duration": 0.00017809122800827026, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax2", "lineno": 440, "outcome": "passed", "keywords": ["test_softmax2", "__wrapped__", "__x_params_repr__", "SoftmaxInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00029563065618276596, "outcome": "passed"}, "call": {"duration": 0.004231429658830166, "outcome": "passed"}, "teardown": {"duration": 0.00017809029668569565, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax3", "lineno": 440, "outcome": "passed", "keywords": ["test_softmax3", "__wrapped__", "__x_params_repr__", "SoftmaxInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.000290880911052227, "outcome": "passed"}, "call": {"duration": 0.007593777030706406, "outcome": "passed"}, "teardown": {"duration": 0.00017809029668569565, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax4", "lineno": 440, "outcome": "passed", "keywords": ["test_softmax4", "__wrapped__", "__x_params_repr__", "SoftmaxInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00031343940645456314, "outcome": "passed"}, "call": {"duration": 0.00632577296346426, "outcome": "passed"}, "teardown": {"duration": 0.0001840256154537201, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax5", "lineno": 440, "outcome": "passed", "keywords": ["test_softmax5", "__wrapped__", "__x_params_repr__", "SoftmaxInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002968171611428261, "outcome": "passed"}, "call": {"duration": 0.006071696989238262, "outcome": "passed"}, "teardown": {"duration": 0.0008845161646604538, "outcome": "passed"}}]}