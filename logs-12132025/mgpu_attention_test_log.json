{"created": 1765571025.5977662, "duration": 5.375652551651001, "exitcode": 0, "root": "/dockerx/rocm/rocm-jax/jax", "environment": {}, "summary": {"skipped": 270, "total": 270, "collected": 270}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention0", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention1", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention10", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention11", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention12", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention13", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention14", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention15", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention16", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention17", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention18", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention19", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention2", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention20", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention21", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention22", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention23", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention24", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention25", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention26", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention27", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention28", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention29", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention3", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention30", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention31", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention32", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention33", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention34", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention35", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention36", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention37", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention38", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention39", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention4", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention40", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention41", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention42", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention43", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention44", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention45", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention46", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention47", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention48", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention49", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention5", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention50", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention51", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention52", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention53", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention6", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention7", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention8", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention9", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention0", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention1", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention10", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention100", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention101", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention102", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention103", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention104", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention105", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention106", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention107", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention108", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention109", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention11", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention110", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention111", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention112", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention113", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention114", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention115", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention116", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention117", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention118", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention119", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention12", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention120", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention121", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention122", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention123", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention124", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention125", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention126", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention127", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention128", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention129", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention13", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention130", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention131", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention132", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention133", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention134", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention135", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention136", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention137", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention138", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention139", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention14", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention140", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention141", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention142", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention143", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention144", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention145", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention146", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention147", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention148", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention149", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention15", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention150", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention151", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention152", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention153", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention154", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention155", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention156", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention157", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention158", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention159", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention16", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention160", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention161", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention162", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention163", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention164", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention165", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention166", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention167", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention168", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention169", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention17", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention170", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention171", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention172", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention173", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention174", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention175", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention176", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention177", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention178", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention179", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention18", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention180", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention181", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention182", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention183", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention184", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention185", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention186", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention187", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention188", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention189", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention19", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention190", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention191", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention192", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention193", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention194", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention195", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention196", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention197", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention198", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention199", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention2", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention20", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention200", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention201", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention202", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention203", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention204", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention205", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention206", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention207", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention208", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention209", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention21", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention210", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention211", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention212", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention213", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention214", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention215", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention22", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention23", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention24", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention25", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention26", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention27", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention28", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention29", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention3", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention30", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention31", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention32", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention33", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention34", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention35", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention36", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention37", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention38", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention39", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention4", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention40", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention41", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention42", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention43", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention44", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention45", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention46", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention47", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention48", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention49", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention5", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention50", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention51", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention52", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention53", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention54", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention55", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention56", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention57", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention58", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention59", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention6", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention60", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention61", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention62", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention63", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention64", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention65", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention66", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention67", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention68", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention69", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention7", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention70", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention71", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention72", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention73", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention74", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention75", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention76", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention77", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention78", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention79", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention8", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention80", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention81", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention82", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention83", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention84", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention85", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention86", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention87", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention88", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention89", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention9", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention90", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention91", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention92", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention93", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention94", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention95", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention96", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention97", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention98", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention99", "type": "TestCaseFunction", "lineno": 53}]}], "tests": [{"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention0", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention0", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00044997502118349075, "outcome": "passed"}, "call": {"duration": 4.365902505815029, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.000157906673848629, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention1", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention1", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00021133385598659515, "outcome": "passed"}, "call": {"duration": 0.0005247732624411583, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.973067790269852e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention10", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention10", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00017927680164575577, "outcome": "passed"}, "call": {"duration": 0.0003276858478784561, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.548237383365631e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention11", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention11", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00016384385526180267, "outcome": "passed"}, "call": {"duration": 0.000490342266857624, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.260699152946472e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention12", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention12", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002053976058959961, "outcome": "passed"}, "call": {"duration": 0.0003181891515851021, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023211896419525e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention13", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention13", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015197135508060455, "outcome": "passed"}, "call": {"duration": 0.00032175052911043167, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667167276144028e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention14", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention14", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014840811491012573, "outcome": "passed"}, "call": {"duration": 0.0003181891515851021, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.00010566692799329758, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention15", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention15", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001472216099500656, "outcome": "passed"}, "call": {"duration": 0.00034786947071552277, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.548423647880554e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention16", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention16", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001543452963232994, "outcome": "passed"}, "call": {"duration": 0.00031106453388929367, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention17", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention17", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014484673738479614, "outcome": "passed"}, "call": {"duration": 0.0009735608473420143, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.260699152946472e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention18", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention18", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00019233673810958862, "outcome": "passed"}, "call": {"duration": 0.0003597429022192955, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667074143886566e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention19", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention19", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001472216099500656, "outcome": "passed"}, "call": {"duration": 0.00032175052911043167, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667074143886566e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention2", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention2", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001721549779176712, "outcome": "passed"}, "call": {"duration": 0.0003229370340704918, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904561400413513e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention20", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention20", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014603417366743088, "outcome": "passed"}, "call": {"duration": 0.00032887421548366547, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention21", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention21", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001436583697795868, "outcome": "passed"}, "call": {"duration": 0.0003181891515851021, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.141862392425537e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention22", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention22", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014840904623270035, "outcome": "passed"}, "call": {"duration": 0.0004618475213646889, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904561400413513e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention23", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention23", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001507829874753952, "outcome": "passed"}, "call": {"duration": 0.0003668665885925293, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.260699152946472e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention24", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention24", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001947116106748581, "outcome": "passed"}, "call": {"duration": 0.0003407467156648636, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.379535913467407e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention25", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention25", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00016028061509132385, "outcome": "passed"}, "call": {"duration": 0.00031818822026252747, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.00010091811418533325, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention26", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention26", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014840904623270035, "outcome": "passed"}, "call": {"duration": 0.00035499315708875656, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.260699152946472e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention27", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention27", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001472216099500656, "outcome": "passed"}, "call": {"duration": 0.00032174959778785706, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904561400413513e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention28", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention28", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014366023242473602, "outcome": "passed"}, "call": {"duration": 0.0003193756565451622, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.0002125212922692299, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention29", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention29", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015315692871809006, "outcome": "passed"}, "call": {"duration": 0.00031700078397989273, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.973067790269852e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention3", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention3", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014603324234485626, "outcome": "passed"}, "call": {"duration": 0.0003419341519474983, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.548423647880554e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention30", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention30", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001424727961421013, "outcome": "passed"}, "call": {"duration": 0.00032412540167570114, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention31", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention31", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014603417366743088, "outcome": "passed"}, "call": {"duration": 0.00031700171530246735, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.666981011629105e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention32", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention32", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001507829874753952, "outcome": "passed"}, "call": {"duration": 0.00032174959778785706, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.429586887359619e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention33", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention33", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014484766870737076, "outcome": "passed"}, "call": {"duration": 0.0003193756565451622, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.666981011629105e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention34", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention34", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015315786004066467, "outcome": "passed"}, "call": {"duration": 0.00031700078397989273, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904468268156052e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention35", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention35", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00017334148287773132, "outcome": "passed"}, "call": {"duration": 0.00031818822026252747, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.785724639892578e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention36", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention36", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014484673738479614, "outcome": "passed"}, "call": {"duration": 0.00031343940645456314, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.548423647880554e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention37", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention37", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00016028061509132385, "outcome": "passed"}, "call": {"duration": 0.0003134384751319885, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667074143886566e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention38", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention38", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00017808936536312103, "outcome": "passed"}, "call": {"duration": 0.0003300607204437256, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention39", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention39", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014484673738479614, "outcome": "passed"}, "call": {"duration": 0.00031343940645456314, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention4", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention4", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00026713497936725616, "outcome": "passed"}, "call": {"duration": 0.00031700171530246735, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention40", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention40", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001472216099500656, "outcome": "passed"}, "call": {"duration": 0.0003443080931901932, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.141955524682999e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention41", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention41", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014247186481952667, "outcome": "passed"}, "call": {"duration": 0.00030987709760665894, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023211896419525e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention42", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention42", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014840904623270035, "outcome": "passed"}, "call": {"duration": 0.0003181891515851021, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667074143886566e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention43", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention43", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014484766870737076, "outcome": "passed"}, "call": {"duration": 0.00031462591141462326, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.0001104157418012619, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention44", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention44", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014366023242473602, "outcome": "passed"}, "call": {"duration": 0.000315813347697258, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904561400413513e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention45", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention45", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.000147220678627491, "outcome": "passed"}, "call": {"duration": 0.00044878851622343063, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.379442781209946e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention46", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention46", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015315786004066467, "outcome": "passed"}, "call": {"duration": 0.00032174959778785706, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904561400413513e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention47", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention47", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014959555119276047, "outcome": "passed"}, "call": {"duration": 0.000315813347697258, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention48", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention48", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014722254127264023, "outcome": "passed"}, "call": {"duration": 0.0003134384751319885, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904561400413513e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention49", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention49", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001543452963232994, "outcome": "passed"}, "call": {"duration": 0.00031818822026252747, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667074143886566e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention5", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention5", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015909411013126373, "outcome": "passed"}, "call": {"duration": 0.0003193747252225876, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.429586887359619e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention50", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention50", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014840904623270035, "outcome": "passed"}, "call": {"duration": 0.00044641364365816116, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023211896419525e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention51", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention51", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001436593011021614, "outcome": "passed"}, "call": {"duration": 0.00031700078397989273, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023305028676987e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention52", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention52", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014366023242473602, "outcome": "passed"}, "call": {"duration": 0.0003336230292916298, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.429586887359619e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention53", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention53", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014366023242473602, "outcome": "passed"}, "call": {"duration": 0.00031106453388929367, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.429586887359619e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention6", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention6", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001424727961421013, "outcome": "passed"}, "call": {"duration": 0.000315813347697258, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.429586887359619e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention7", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention7", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001424727961421013, "outcome": "passed"}, "call": {"duration": 0.0003122510388493538, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904468268156052e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention8", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention8", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014484673738479614, "outcome": "passed"}, "call": {"duration": 0.0004630358889698982, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention9", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention9", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014603417366743088, "outcome": "passed"}, "call": {"duration": 0.0003134384751319885, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.666981011629105e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention0", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention0", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001424727961421013, "outcome": "passed"}, "call": {"duration": 0.00031344033777713776, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.00012466218322515488, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention1", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention1", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014840811491012573, "outcome": "passed"}, "call": {"duration": 0.0003158142790198326, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.310843259096146e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention10", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention10", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014247186481952667, "outcome": "passed"}, "call": {"duration": 0.00031343940645456314, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.141955524682999e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention100", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention100", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014366023242473602, "outcome": "passed"}, "call": {"duration": 0.000315813347697258, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904561400413513e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention101", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention101", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014128535985946655, "outcome": "passed"}, "call": {"duration": 0.0004321662709116936, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.548330515623093e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention102", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention102", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.000147220678627491, "outcome": "passed"}, "call": {"duration": 0.0003193756565451622, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904468268156052e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention103", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention103", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015196949243545532, "outcome": "passed"}, "call": {"duration": 0.0003193756565451622, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.548237383365631e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention104", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention104", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00017571542412042618, "outcome": "passed"}, "call": {"duration": 0.0003229370340704918, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667167276144028e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention105", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention105", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014722254127264023, "outcome": "passed"}, "call": {"duration": 0.00031700078397989273, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.548330515623093e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention106", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention106", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001424727961421013, "outcome": "passed"}, "call": {"duration": 0.0003122519701719284, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention107", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention107", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014247186481952667, "outcome": "passed"}, "call": {"duration": 0.00046778377145528793, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023211896419525e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention108", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention108", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001472216099500656, "outcome": "passed"}, "call": {"duration": 0.00033599790185689926, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.548423647880554e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention109", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention109", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.000147220678627491, "outcome": "passed"}, "call": {"duration": 0.0003146268427371979, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.785724639892578e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention11", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention11", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001424727961421013, "outcome": "passed"}, "call": {"duration": 0.00031700078397989273, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.00010566692799329758, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention110", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention110", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.000147220678627491, "outcome": "passed"}, "call": {"duration": 0.0003086896613240242, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention111", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention111", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001424727961421013, "outcome": "passed"}, "call": {"duration": 0.0003086896613240242, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.616930037736893e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention112", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention112", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014247186481952667, "outcome": "passed"}, "call": {"duration": 0.00043454114347696304, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.260792285203934e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention113", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention113", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001531587913632393, "outcome": "passed"}, "call": {"duration": 0.0003146268427371979, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667074143886566e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention114", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention114", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014247186481952667, "outcome": "passed"}, "call": {"duration": 0.00030987802892923355, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.429586887359619e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention115", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention115", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001436583697795868, "outcome": "passed"}, "call": {"duration": 0.00031343940645456314, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention116", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention116", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001650303602218628, "outcome": "passed"}, "call": {"duration": 0.00031700078397989273, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.310936391353607e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention117", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention117", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014484766870737076, "outcome": "passed"}, "call": {"duration": 0.00030987709760665894, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.429586887359619e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention118", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention118", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014484673738479614, "outcome": "passed"}, "call": {"duration": 0.0004297913983464241, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904561400413513e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention119", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention119", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015078391879796982, "outcome": "passed"}, "call": {"duration": 0.0003371844068169594, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023305028676987e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention12", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention12", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001436583697795868, "outcome": "passed"}, "call": {"duration": 0.0003110654652118683, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904468268156052e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention120", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention120", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014840904623270035, "outcome": "passed"}, "call": {"duration": 0.00032175052911043167, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention121", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention121", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014128535985946655, "outcome": "passed"}, "call": {"duration": 0.00031106453388929367, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.666981011629105e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention122", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention122", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015197042375802994, "outcome": "passed"}, "call": {"duration": 0.0003146277740597725, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention123", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention123", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014840811491012573, "outcome": "passed"}, "call": {"duration": 0.0008512726053595543, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.260699152946472e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention124", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention124", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015909317880868912, "outcome": "passed"}, "call": {"duration": 0.0003383718430995941, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.141955524682999e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention125", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention125", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014484766870737076, "outcome": "passed"}, "call": {"duration": 0.00031700078397989273, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667074143886566e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention126", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention126", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014484766870737076, "outcome": "passed"}, "call": {"duration": 0.0003146268427371979, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023118764162064e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention127", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention127", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001472216099500656, "outcome": "passed"}, "call": {"duration": 0.00031818822026252747, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904468268156052e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention128", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention128", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014603417366743088, "outcome": "passed"}, "call": {"duration": 0.00031700171530246735, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.498186409473419e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention129", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention129", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014603417366743088, "outcome": "passed"}, "call": {"duration": 0.0003146268427371979, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.616930037736893e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention13", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention13", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00028138328343629837, "outcome": "passed"}, "call": {"duration": 0.0003229370340704918, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.260699152946472e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention130", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention130", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014959555119276047, "outcome": "passed"}, "call": {"duration": 0.0003122519701719284, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667074143886566e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention131", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention131", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001436593011021614, "outcome": "passed"}, "call": {"duration": 0.0003158142790198326, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.548330515623093e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention132", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention132", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014366023242473602, "outcome": "passed"}, "call": {"duration": 0.00030631665140390396, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.548330515623093e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention133", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention133", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014128442853689194, "outcome": "passed"}, "call": {"duration": 0.00032531190663576126, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023305028676987e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention134", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention134", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014366023242473602, "outcome": "passed"}, "call": {"duration": 0.00031462591141462326, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667074143886566e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention135", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention135", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014959555119276047, "outcome": "passed"}, "call": {"duration": 0.000448787584900856, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023211896419525e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention136", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention136", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014959555119276047, "outcome": "passed"}, "call": {"duration": 0.0003383718430995941, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904561400413513e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention137", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention137", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014959648251533508, "outcome": "passed"}, "call": {"duration": 0.0003146268427371979, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667074143886566e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention138", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention138", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014484673738479614, "outcome": "passed"}, "call": {"duration": 0.0003122519701719284, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023211896419525e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention139", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention139", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014366023242473602, "outcome": "passed"}, "call": {"duration": 0.0003122519701719284, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.00010685436427593231, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention14", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention14", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014366023242473602, "outcome": "passed"}, "call": {"duration": 0.00031818822026252747, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023211896419525e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention140", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention140", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001436583697795868, "outcome": "passed"}, "call": {"duration": 0.0004476010799407959, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.260699152946472e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention141", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention141", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014484673738479614, "outcome": "passed"}, "call": {"duration": 0.00031343940645456314, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.141862392425537e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention142", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention142", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.000147220678627491, "outcome": "passed"}, "call": {"duration": 0.0003146268427371979, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904468268156052e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention143", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention143", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014128442853689194, "outcome": "passed"}, "call": {"duration": 0.0003146268427371979, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023211896419525e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention144", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention144", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014603417366743088, "outcome": "passed"}, "call": {"duration": 0.0003193765878677368, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.548330515623093e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention145", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention145", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001769028604030609, "outcome": "passed"}, "call": {"duration": 0.00031344033777713776, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.785724639892578e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention146", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention146", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014366023242473602, "outcome": "passed"}, "call": {"duration": 0.00031106453388929367, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.00021964497864246368, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention147", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention147", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015671923756599426, "outcome": "passed"}, "call": {"duration": 0.000315813347697258, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.785724639892578e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention148", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention148", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015315786004066467, "outcome": "passed"}, "call": {"duration": 0.00034074578434228897, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.379442781209946e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention149", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention149", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015078485012054443, "outcome": "passed"}, "call": {"duration": 0.000315813347697258, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667167276144028e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention15", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention15", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014603417366743088, "outcome": "passed"}, "call": {"duration": 0.00031700078397989273, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667167276144028e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention150", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention150", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014247186481952667, "outcome": "passed"}, "call": {"duration": 0.0003122519701719284, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023305028676987e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention151", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention151", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014247186481952667, "outcome": "passed"}, "call": {"duration": 0.0003146268427371979, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.429586887359619e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention152", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention152", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015197135508060455, "outcome": "passed"}, "call": {"duration": 0.00031343940645456314, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.785631507635117e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention153", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention153", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001460351049900055, "outcome": "passed"}, "call": {"duration": 0.0003134384751319885, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.141955524682999e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention154", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention154", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014366023242473602, "outcome": "passed"}, "call": {"duration": 0.00031700078397989273, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.429586887359619e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention155", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention155", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014603417366743088, "outcome": "passed"}, "call": {"duration": 0.0003075031563639641, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667074143886566e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention156", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention156", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015197135508060455, "outcome": "passed"}, "call": {"duration": 0.00030987709760665894, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.548330515623093e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention157", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention157", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015553273260593414, "outcome": "passed"}, "call": {"duration": 0.00031344033777713776, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023211896419525e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention158", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention158", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00027663446962833405, "outcome": "passed"}, "call": {"duration": 0.00031818822026252747, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904561400413513e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention159", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention159", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001543452963232994, "outcome": "passed"}, "call": {"duration": 0.0003158142790198326, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.260699152946472e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention16", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention16", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014722254127264023, "outcome": "passed"}, "call": {"duration": 0.0003348095342516899, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention160", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention160", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014484673738479614, "outcome": "passed"}, "call": {"duration": 0.00031343940645456314, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904561400413513e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention161", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention161", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014603324234485626, "outcome": "passed"}, "call": {"duration": 0.0003122519701719284, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention162", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention162", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001424737274646759, "outcome": "passed"}, "call": {"duration": 0.000315813347697258, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.854324162006378e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention163", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention163", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001436593011021614, "outcome": "passed"}, "call": {"duration": 0.0004369150847196579, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023211896419525e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention164", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention164", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015196949243545532, "outcome": "passed"}, "call": {"duration": 0.0003158142790198326, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667074143886566e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention165", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention165", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014603324234485626, "outcome": "passed"}, "call": {"duration": 0.00031700078397989273, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.00011516455560922623, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention166", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention166", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.000147220678627491, "outcome": "passed"}, "call": {"duration": 0.000315813347697258, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667074143886566e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention167", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention167", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014247186481952667, "outcome": "passed"}, "call": {"duration": 0.000315813347697258, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.141862392425537e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention168", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention168", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014603417366743088, "outcome": "passed"}, "call": {"duration": 0.00031700078397989273, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667074143886566e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention169", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention169", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014484673738479614, "outcome": "passed"}, "call": {"duration": 0.0004476010799407959, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023305028676987e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention17", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention17", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014840904623270035, "outcome": "passed"}, "call": {"duration": 0.00031818822026252747, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904468268156052e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention170", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention170", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00016265641897916794, "outcome": "passed"}, "call": {"duration": 0.0003526192158460617, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.498186409473419e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention171", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention171", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015196949243545532, "outcome": "passed"}, "call": {"duration": 0.00032531097531318665, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.00011516455560922623, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention172", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention172", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001472216099500656, "outcome": "passed"}, "call": {"duration": 0.0003181891515851021, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.0001104157418012619, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention173", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention173", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015078391879796982, "outcome": "passed"}, "call": {"duration": 0.00034668203443288803, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.0001543452963232994, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention174", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention174", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00018165167421102524, "outcome": "passed"}, "call": {"duration": 0.00044997502118349075, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.00011516548693180084, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention175", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention175", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00016265641897916794, "outcome": "passed"}, "call": {"duration": 0.0003205621615052223, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023118764162064e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention176", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention176", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015315786004066467, "outcome": "passed"}, "call": {"duration": 0.00032175052911043167, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.141955524682999e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention177", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention177", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001840265467762947, "outcome": "passed"}, "call": {"duration": 0.0003490569069981575, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023305028676987e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention178", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention178", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001472216099500656, "outcome": "passed"}, "call": {"duration": 0.0003229370340704918, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.0001211017370223999, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention179", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention179", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014484673738479614, "outcome": "passed"}, "call": {"duration": 0.0003265002742409706, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023211896419525e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention18", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention18", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001436593011021614, "outcome": "passed"}, "call": {"duration": 0.0004962794482707977, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.260699152946472e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention180", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention180", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001543452963232994, "outcome": "passed"}, "call": {"duration": 0.0003443080931901932, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.14204865694046e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention181", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention181", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015553273260593414, "outcome": "passed"}, "call": {"duration": 0.00034668203443288803, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904468268156052e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention182", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention182", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.000157906673848629, "outcome": "passed"}, "call": {"duration": 0.00037636421620845795, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.260699152946472e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention183", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention183", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014603417366743088, "outcome": "passed"}, "call": {"duration": 0.00032174959778785706, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.973067790269852e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention184", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention184", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001614680513739586, "outcome": "passed"}, "call": {"duration": 0.0003229370340704918, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.854324162006378e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention185", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention185", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015553180128335953, "outcome": "passed"}, "call": {"duration": 0.0004772823303937912, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.260699152946472e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention186", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention186", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00016977917402982712, "outcome": "passed"}, "call": {"duration": 0.0003193747252225876, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.00011991336941719055, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention187", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention187", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014959555119276047, "outcome": "passed"}, "call": {"duration": 0.0003241244703531265, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.00011991430073976517, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention188", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention188", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015672016888856888, "outcome": "passed"}, "call": {"duration": 0.00032056309282779694, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.00011991523206233978, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention189", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention189", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001662168651819229, "outcome": "passed"}, "call": {"duration": 0.00032887328416109085, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023305028676987e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention19", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention19", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015197042375802994, "outcome": "passed"}, "call": {"duration": 0.00034312065690755844, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.14204865694046e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention190", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention190", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015315786004066467, "outcome": "passed"}, "call": {"duration": 0.0009260699152946472, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.00011635199189186096, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention191", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention191", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00016740523278713226, "outcome": "passed"}, "call": {"duration": 0.0003336230292916298, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.498186409473419e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention192", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention192", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00016265641897916794, "outcome": "passed"}, "call": {"duration": 0.00031818822026252747, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.141955524682999e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention193", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention193", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014840904623270035, "outcome": "passed"}, "call": {"duration": 0.0003668665885925293, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.141862392425537e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention194", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention194", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015315786004066467, "outcome": "passed"}, "call": {"duration": 0.00031700078397989273, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.260699152946472e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention195", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention195", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001507829874753952, "outcome": "passed"}, "call": {"duration": 0.0003229379653930664, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.379442781209946e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention196", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention196", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014959555119276047, "outcome": "passed"}, "call": {"duration": 0.0003075031563639641, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.498093277215958e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention197", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention197", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002861320972442627, "outcome": "passed"}, "call": {"duration": 0.00032768677920103073, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.498279541730881e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention198", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention198", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015197135508060455, "outcome": "passed"}, "call": {"duration": 0.00031700078397989273, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention199", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention199", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014840904623270035, "outcome": "passed"}, "call": {"duration": 0.00031700078397989273, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.0001246640458703041, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention2", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention2", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014603417366743088, "outcome": "passed"}, "call": {"duration": 0.0003241235390305519, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.260792285203934e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention20", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention20", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014840811491012573, "outcome": "passed"}, "call": {"duration": 0.00032887328416109085, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.379442781209946e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention200", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention200", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014840904623270035, "outcome": "passed"}, "call": {"duration": 0.0003276858478784561, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.14204865694046e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention201", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention201", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00028019584715366364, "outcome": "passed"}, "call": {"duration": 0.00033599790185689926, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023211896419525e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention202", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention202", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015671923756599426, "outcome": "passed"}, "call": {"duration": 0.00032768677920103073, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.379535913467407e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention203", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention203", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.000168592669069767, "outcome": "passed"}, "call": {"duration": 0.00032887328416109085, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023305028676987e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention204", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention204", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015671923756599426, "outcome": "passed"}, "call": {"duration": 0.0003443080931901932, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.548330515623093e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention205", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention205", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014247186481952667, "outcome": "passed"}, "call": {"duration": 0.0003229379653930664, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023211896419525e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention206", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention206", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014840811491012573, "outcome": "passed"}, "call": {"duration": 0.00032056309282779694, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904654532670975e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention207", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention207", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015197135508060455, "outcome": "passed"}, "call": {"duration": 0.0004416638985276222, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.854231029748917e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention208", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention208", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001507829874753952, "outcome": "passed"}, "call": {"duration": 0.00031700078397989273, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.0001282254233956337, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention209", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention209", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014484673738479614, "outcome": "passed"}, "call": {"duration": 0.00032412540167570114, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.785724639892578e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention21", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention21", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.000157906673848629, "outcome": "passed"}, "call": {"duration": 0.00032174959778785706, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667074143886566e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention210", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention210", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00016740523278713226, "outcome": "passed"}, "call": {"duration": 0.00031700171530246735, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.548423647880554e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention211", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention211", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001436593011021614, "outcome": "passed"}, "call": {"duration": 0.00031462591141462326, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667074143886566e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention212", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention212", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00016384292393922806, "outcome": "passed"}, "call": {"duration": 0.00047490745782852173, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.260699152946472e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention213", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention213", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001662168651819229, "outcome": "passed"}, "call": {"duration": 0.0003146268427371979, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention214", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention214", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015315786004066467, "outcome": "passed"}, "call": {"duration": 0.0003383718430995941, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667074143886566e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention215", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention215", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014247186481952667, "outcome": "passed"}, "call": {"duration": 0.0003205621615052223, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023211896419525e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention22", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention22", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00016740523278713226, "outcome": "passed"}, "call": {"duration": 0.00034312065690755844, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.548330515623093e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention23", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention23", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001436593011021614, "outcome": "passed"}, "call": {"duration": 0.00031462591141462326, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.00011160410940647125, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention24", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention24", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001436593011021614, "outcome": "passed"}, "call": {"duration": 0.0004440387710928917, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.00011991430073976517, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention25", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention25", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014959648251533508, "outcome": "passed"}, "call": {"duration": 0.00031700078397989273, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention26", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention26", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014840811491012573, "outcome": "passed"}, "call": {"duration": 0.0003158124163746834, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904468268156052e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention27", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention27", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.000157906673848629, "outcome": "passed"}, "call": {"duration": 0.0003193747252225876, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.14204865694046e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention28", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention28", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00016384385526180267, "outcome": "passed"}, "call": {"duration": 0.0003193756565451622, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.666981011629105e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention29", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention29", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00016028154641389847, "outcome": "passed"}, "call": {"duration": 0.0003383718430995941, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023211896419525e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention3", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention3", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001721540465950966, "outcome": "passed"}, "call": {"duration": 0.000448787584900856, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.00011872686445713043, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention30", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention30", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014484766870737076, "outcome": "passed"}, "call": {"duration": 0.00031700078397989273, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.00011397805064916611, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention31", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention31", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014484673738479614, "outcome": "passed"}, "call": {"duration": 0.0003193747252225876, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.61674377322197e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention32", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention32", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001424727961421013, "outcome": "passed"}, "call": {"duration": 0.00031700078397989273, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.666981011629105e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention33", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention33", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00016977917402982712, "outcome": "passed"}, "call": {"duration": 0.0003158142790198326, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667074143886566e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention34", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention34", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00020064879208803177, "outcome": "passed"}, "call": {"duration": 0.0003158142790198326, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.14204865694046e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention35", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention35", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001828400418162346, "outcome": "passed"}, "call": {"duration": 0.00044166482985019684, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023305028676987e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention36", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention36", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014840904623270035, "outcome": "passed"}, "call": {"duration": 0.0003668656572699547, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention37", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention37", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014484673738479614, "outcome": "passed"}, "call": {"duration": 0.00031462591141462326, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.00011754035949707031, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention38", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention38", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014128535985946655, "outcome": "passed"}, "call": {"duration": 0.00031700078397989273, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.498093277215958e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention39", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention39", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015197042375802994, "outcome": "passed"}, "call": {"duration": 0.00035024434328079224, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023305028676987e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention4", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention4", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015553180128335953, "outcome": "passed"}, "call": {"duration": 0.00031700078397989273, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023305028676987e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention40", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention40", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014603417366743088, "outcome": "passed"}, "call": {"duration": 0.0004476001486182213, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023305028676987e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention41", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention41", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001721540465950966, "outcome": "passed"}, "call": {"duration": 0.0003312481567263603, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.854324162006378e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention42", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention42", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001507829874753952, "outcome": "passed"}, "call": {"duration": 0.000322938896715641, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.14204865694046e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention43", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention43", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001472216099500656, "outcome": "passed"}, "call": {"duration": 0.0003146268427371979, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention44", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention44", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015315786004066467, "outcome": "passed"}, "call": {"duration": 0.0003229379653930664, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904561400413513e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention45", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention45", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014840904623270035, "outcome": "passed"}, "call": {"duration": 0.0003348095342516899, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.666981011629105e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention46", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention46", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001472216099500656, "outcome": "passed"}, "call": {"duration": 0.00044522620737552643, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.00010448042303323746, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention47", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention47", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014840811491012573, "outcome": "passed"}, "call": {"duration": 0.000315813347697258, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.0001258496195077896, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention48", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention48", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014603417366743088, "outcome": "passed"}, "call": {"duration": 0.0003205621615052223, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.548330515623093e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention49", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention49", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001472216099500656, "outcome": "passed"}, "call": {"duration": 0.00031818728893995285, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.14204865694046e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention5", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention5", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00016028247773647308, "outcome": "passed"}, "call": {"duration": 0.00031462591141462326, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023211896419525e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention50", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention50", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00021370872855186462, "outcome": "passed"}, "call": {"duration": 0.00032175052911043167, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention51", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention51", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014959555119276047, "outcome": "passed"}, "call": {"duration": 0.0004547238349914551, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.379442781209946e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention52", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention52", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00016740523278713226, "outcome": "passed"}, "call": {"duration": 0.0003443090245127678, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention53", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention53", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001982739195227623, "outcome": "passed"}, "call": {"duration": 0.0003419322893023491, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.429586887359619e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention54", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention54", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001472216099500656, "outcome": "passed"}, "call": {"duration": 0.0003193747252225876, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.141862392425537e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention55", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention55", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014128629118204117, "outcome": "passed"}, "call": {"duration": 0.0003336230292916298, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention56", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention56", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014603417366743088, "outcome": "passed"}, "call": {"duration": 0.00035380758345127106, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023305028676987e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention57", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention57", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014603417366743088, "outcome": "passed"}, "call": {"duration": 0.000428604893386364, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904654532670975e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention58", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention58", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00016740523278713226, "outcome": "passed"}, "call": {"duration": 0.00033599697053432465, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667074143886566e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention59", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention59", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00016265548765659332, "outcome": "passed"}, "call": {"duration": 0.0003241244703531265, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904654532670975e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention6", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention6", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001507829874753952, "outcome": "passed"}, "call": {"duration": 0.00033599790185689926, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention60", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention60", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015197135508060455, "outcome": "passed"}, "call": {"duration": 0.0003205621615052223, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904561400413513e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention61", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention61", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014366023242473602, "outcome": "passed"}, "call": {"duration": 0.0003229370340704918, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667074143886566e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention62", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention62", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014840904623270035, "outcome": "passed"}, "call": {"duration": 0.0008204029873013496, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.14204865694046e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention63", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention63", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015197135508060455, "outcome": "passed"}, "call": {"duration": 0.00031700078397989273, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904468268156052e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention64", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention64", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014484766870737076, "outcome": "passed"}, "call": {"duration": 0.0003146268427371979, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.0001246631145477295, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention65", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention65", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00017452891916036606, "outcome": "passed"}, "call": {"duration": 0.0003158142790198326, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667074143886566e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention66", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention66", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00017334241420030594, "outcome": "passed"}, "call": {"duration": 0.000315813347697258, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667074143886566e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention67", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention67", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001436593011021614, "outcome": "passed"}, "call": {"duration": 0.00033124908804893494, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904468268156052e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention68", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention68", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015315786004066467, "outcome": "passed"}, "call": {"duration": 0.000315813347697258, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667074143886566e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention69", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention69", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002790084108710289, "outcome": "passed"}, "call": {"duration": 0.00031700171530246735, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667074143886566e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention7", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention7", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015197135508060455, "outcome": "passed"}, "call": {"duration": 0.0003205621615052223, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.785631507635117e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention70", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention70", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001507829874753952, "outcome": "passed"}, "call": {"duration": 0.00031818822026252747, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904561400413513e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention71", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention71", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015078391879796982, "outcome": "passed"}, "call": {"duration": 0.0003158142790198326, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023211896419525e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention72", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention72", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014959461987018585, "outcome": "passed"}, "call": {"duration": 0.0003122519701719284, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904468268156052e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention73", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention73", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00018758885562419891, "outcome": "passed"}, "call": {"duration": 0.0003122519701719284, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.548330515623093e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention74", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention74", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001543443650007248, "outcome": "passed"}, "call": {"duration": 0.00044285133481025696, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.00010091811418533325, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention75", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention75", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015315786004066467, "outcome": "passed"}, "call": {"duration": 0.00032174959778785706, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.00011397805064916611, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention76", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention76", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.000147220678627491, "outcome": "passed"}, "call": {"duration": 0.000337185338139534, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.973067790269852e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention77", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention77", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014959555119276047, "outcome": "passed"}, "call": {"duration": 0.0003181891515851021, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.00010804180055856705, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention78", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention78", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014603417366743088, "outcome": "passed"}, "call": {"duration": 0.0003122519701719284, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.00010329298675060272, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention79", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention79", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014959555119276047, "outcome": "passed"}, "call": {"duration": 0.00035380665212869644, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.0001211017370223999, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention8", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention8", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014959555119276047, "outcome": "passed"}, "call": {"duration": 0.0004808437079191208, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904468268156052e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention80", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention80", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014603417366743088, "outcome": "passed"}, "call": {"duration": 0.00032412540167570114, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.00010804180055856705, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention81", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention81", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014840904623270035, "outcome": "passed"}, "call": {"duration": 0.0003739902749657631, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904468268156052e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention82", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention82", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.000147220678627491, "outcome": "passed"}, "call": {"duration": 0.00031818822026252747, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023305028676987e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention83", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention83", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014128535985946655, "outcome": "passed"}, "call": {"duration": 0.0003312481567263603, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention84", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention84", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014603417366743088, "outcome": "passed"}, "call": {"duration": 0.0003265002742409706, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.548330515623093e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention85", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention85", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015197135508060455, "outcome": "passed"}, "call": {"duration": 0.000448787584900856, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023211896419525e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention86", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention86", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014840811491012573, "outcome": "passed"}, "call": {"duration": 0.0003336230292916298, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention87", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention87", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001650303602218628, "outcome": "passed"}, "call": {"duration": 0.00033599697053432465, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023211896419525e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention88", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention88", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002018352970480919, "outcome": "passed"}, "call": {"duration": 0.00035855546593666077, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904468268156052e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention89", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention89", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00016978010535240173, "outcome": "passed"}, "call": {"duration": 0.0003146268427371979, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.310936391353607e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention9", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention9", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014840904623270035, "outcome": "passed"}, "call": {"duration": 0.0003253128379583359, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.904561400413513e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention90", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention90", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014840811491012573, "outcome": "passed"}, "call": {"duration": 0.0004369150847196579, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.14204865694046e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention91", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention91", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001543452963232994, "outcome": "passed"}, "call": {"duration": 0.00032175052911043167, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention92", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention92", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014840811491012573, "outcome": "passed"}, "call": {"duration": 0.0003264984115958214, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.023211896419525e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention93", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention93", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00018165260553359985, "outcome": "passed"}, "call": {"duration": 0.0003193747252225876, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention94", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention94", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015197042375802994, "outcome": "passed"}, "call": {"duration": 0.0003371844068169594, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.785724639892578e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention95", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention95", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001436593011021614, "outcome": "passed"}, "call": {"duration": 0.0003419332206249237, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.548237383365631e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention96", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention96", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014484580606222153, "outcome": "passed"}, "call": {"duration": 0.0004582861438393593, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention97", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention97", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014840904623270035, "outcome": "passed"}, "call": {"duration": 0.0003490569069981575, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.667167276144028e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention98", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention98", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014366023242473602, "outcome": "passed"}, "call": {"duration": 0.00034668296575546265, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.78581777215004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention99", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention99", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014484673738479614, "outcome": "passed"}, "call": {"duration": 0.0003882376477122307, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.0011860821396112442, "outcome": "passed"}}]}