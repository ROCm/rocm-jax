{"created": 1763581316.7543783, "duration": 195.43555784225464, "exitcode": 1, "root": "/dockerx/rocm/rocm-jax/jax", "environment": {}, "summary": {"passed": 61, "failed": 9, "total": 70, "collected": 70}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd0", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd1", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd2", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd3", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd4", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd5", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd6", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd7", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd8", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd9", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd0", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd1", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd2", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd3", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd4", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd5", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd6", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd7", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd8", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd9", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_return_residuals_not_differentiable", "type": "TestCaseFunction", "lineno": 308}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd0", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd1", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd2", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd3", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd4", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd5", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd6", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd7", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd8", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd9", "type": "TestCaseFunction", "lineno": 224}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd0", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd1", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd2", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd3", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd4", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd5", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd6", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd7", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd8", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd9", "type": "TestCaseFunction", "lineno": 151}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_return_residuals_not_differentiable", "type": "TestCaseFunction", "lineno": 308}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_bwd0", "type": "TestCaseFunction", "lineno": 358}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_bwd1", "type": "TestCaseFunction", "lineno": 358}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_fwd0", "type": "TestCaseFunction", "lineno": 344}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_fwd1", "type": "TestCaseFunction", "lineno": 344}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_bwd0", "type": "TestCaseFunction", "lineno": 358}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_bwd1", "type": "TestCaseFunction", "lineno": 358}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_fwd0", "type": "TestCaseFunction", "lineno": 344}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_fwd1", "type": "TestCaseFunction", "lineno": 344}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_fwd0", "type": "TestCaseFunction", "lineno": 392}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_fwd1", "type": "TestCaseFunction", "lineno": 392}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_norm_bwd0", "type": "TestCaseFunction", "lineno": 406}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_norm_bwd1", "type": "TestCaseFunction", "lineno": 406}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_fwd0", "type": "TestCaseFunction", "lineno": 392}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_fwd1", "type": "TestCaseFunction", "lineno": 392}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_norm_bwd0", "type": "TestCaseFunction", "lineno": 406}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_norm_bwd1", "type": "TestCaseFunction", "lineno": 406}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax0", "type": "TestCaseFunction", "lineno": 440}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax1", "type": "TestCaseFunction", "lineno": 440}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax2", "type": "TestCaseFunction", "lineno": 440}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax3", "type": "TestCaseFunction", "lineno": 440}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax4", "type": "TestCaseFunction", "lineno": 440}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax5", "type": "TestCaseFunction", "lineno": 440}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax0", "type": "TestCaseFunction", "lineno": 440}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax1", "type": "TestCaseFunction", "lineno": 440}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax2", "type": "TestCaseFunction", "lineno": 440}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax3", "type": "TestCaseFunction", "lineno": 440}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax4", "type": "TestCaseFunction", "lineno": 440}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax5", "type": "TestCaseFunction", "lineno": 440}]}], "tests": [{"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd0", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd0", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0003131916746497154, "outcome": "passed"}, "call": {"duration": 13.186752691864967, "outcome": "passed"}, "teardown": {"duration": 0.00022672396153211594, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd1", "lineno": 224, "outcome": "failed", "keywords": ["test_fused_attention_bwd1", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002287980169057846, "outcome": "passed"}, "call": {"duration": 0.48401958029717207, "outcome": "failed", "crash": {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536"}, "traceback": [{"path": "jax/tests/pallas/gpu_ops_test.py", "lineno": 302, "message": ""}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 427, "message": "in grad_f"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 505, "message": "in value_and_grad_f"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py", "lineno": 488, "message": "in __call__"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 2105, "message": "in _vjp_pullback_wrapper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py", "lineno": 488, "message": "in __call__"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 320, "message": "in unbound_vjp"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 438, "message": "in backward_pass"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 1559, "message": "in _custom_lin_transpose"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 212, "message": "in call_wrapped"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 921, "message": "in _flatten_bwd"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py", "lineno": 234, "message": "in _prepend_static_args"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 421, "message": "in _get_result_paths_thunk"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py", "lineno": 602, "message": "in _mha_backward"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 263, "message": "in cache_miss"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 146, "message": "in _python_pjit_helper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 1600, "message": "in _pjit_call_impl_python"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2527, "message": "in compile"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 3073, "message": "in from_hlo"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2854, "message": "in _cached_compilation"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 491, "message": "in compile_or_get_cached"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 759, "message": "in _compile_and_write_cache"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py", "lineno": 359, "message": "in wrapper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 385, "message": "in backend_compile_and_load"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "JaxRuntimeError"}], "longrepr": ">   dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n>   return attention.mha(\n        q, k, v,\n        block_sizes=BlockSizes(**dict(block_sizes)),\n        causal=causal,\n        segment_ids=segment_ids,\n        interpret=self.INTERPRET).sum()\nE   jax._src.source_info_util.JaxStackTraceBeforeTransformation: jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\nE   \nE   The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\nE   \nE   --------------------\n\njax/tests/pallas/gpu_ops_test.py:292: JaxStackTraceBeforeTransformation\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd1>\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (\"block_q\", 128),\n                (\"block_k\", 128),\n                (\"block_q_dkv\", 32),\n                (\"block_kv_dkv\", 32),\n                (\"block_q_dq\", 32),\n                (\"block_kv_dq\", 128),\n            ),\n            (\n                (\"block_q\", 64),\n                (\"block_k\", 64),\n                (\"block_q_dkv\", 64),\n                (\"block_kv_dkv\", 64),\n                (\"block_q_dq\", 64),\n                (\"block_kv_dq\", 64),\n            ),\n            (\n                (\"block_q\", 64),\n                (\"block_k\", 128),\n                (\"block_q_dkv\", 64),\n                (\"block_kv_dkv\", 32),\n                (\"block_q_dq\", 32),\n                (\"block_kv_dq\", 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(\"8.0\"):\n        # TODO(b/416306534)\n        self.skipTest(\"Precision issues after CUDA 12.8.1 upgrade\")\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n>     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:505: in value_and_grad_f\n    g = vjp_py(lax_internal._one(ans))\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2105: in _vjp_pullback_wrapper\n    ans = fun(*args)\n          ^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:320: in unbound_vjp\n    arg_cts = backward_pass(jaxpr, True, consts, dummy_args, cts)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:438: in backward_pass\n    cts_out = get_primitive_transpose(eqn.primitive)(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:1559: in _custom_lin_transpose\n    cts_in = bwd.call_wrapped(*res, *cts_out)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:921: in _flatten_bwd\n    py_cts_in = f(py_res, py_cts_out)\n                ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:234: in _prepend_static_args\n    return f(*all_args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:602: in _mha_backward\n    dq, dk, dv = pl.pallas_call(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = <jaxlib._jax.Client object at 0x7f38df090040>\nmodule = <jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f37843dcfe0>\nexecutable_devices = (RocmDevice(id=0),)\noptions = <jaxlib._jax.CompileOptions object at 0x22dd3e40>, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -> xc.LoadedExecutable:\n      sym_name = module.operation.attributes['sym_name']\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version < 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            \"Compiling module %s with FDO profile of length %d\",\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n>         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError"}, "teardown": {"duration": 0.000233454629778862, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd2", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd2", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00023024063557386398, "outcome": "passed"}, "call": {"duration": 9.638835178688169, "outcome": "passed"}, "teardown": {"duration": 0.00016327109187841415, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd3", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd3", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00029498524963855743, "outcome": "passed"}, "call": {"duration": 8.323878918774426, "outcome": "passed"}, "teardown": {"duration": 0.00015721283853054047, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd4", "lineno": 224, "outcome": "failed", "keywords": ["test_fused_attention_bwd4", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002810554578900337, "outcome": "passed"}, "call": {"duration": 0.5300677232444286, "outcome": "failed", "crash": {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536"}, "traceback": [{"path": "jax/tests/pallas/gpu_ops_test.py", "lineno": 302, "message": ""}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 427, "message": "in grad_f"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 505, "message": "in value_and_grad_f"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py", "lineno": 488, "message": "in __call__"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 2105, "message": "in _vjp_pullback_wrapper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py", "lineno": 488, "message": "in __call__"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 320, "message": "in unbound_vjp"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 438, "message": "in backward_pass"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 1559, "message": "in _custom_lin_transpose"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 212, "message": "in call_wrapped"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 921, "message": "in _flatten_bwd"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py", "lineno": 234, "message": "in _prepend_static_args"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 421, "message": "in _get_result_paths_thunk"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py", "lineno": 602, "message": "in _mha_backward"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 263, "message": "in cache_miss"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 146, "message": "in _python_pjit_helper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 1600, "message": "in _pjit_call_impl_python"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2527, "message": "in compile"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 3073, "message": "in from_hlo"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2854, "message": "in _cached_compilation"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 491, "message": "in compile_or_get_cached"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 759, "message": "in _compile_and_write_cache"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py", "lineno": 359, "message": "in wrapper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 385, "message": "in backend_compile_and_load"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "JaxRuntimeError"}], "longrepr": ">   dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n>   return attention.mha(\n        q, k, v,\n        block_sizes=BlockSizes(**dict(block_sizes)),\n        causal=causal,\n        segment_ids=segment_ids,\n        interpret=self.INTERPRET).sum()\nE   jax._src.source_info_util.JaxStackTraceBeforeTransformation: jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\nE   \nE   The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\nE   \nE   --------------------\n\njax/tests/pallas/gpu_ops_test.py:292: JaxStackTraceBeforeTransformation\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd4>\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (\"block_q\", 128),\n                (\"block_k\", 128),\n                (\"block_q_dkv\", 32),\n                (\"block_kv_dkv\", 32),\n                (\"block_q_dq\", 32),\n                (\"block_kv_dq\", 128),\n            ),\n            (\n                (\"block_q\", 64),\n                (\"block_k\", 64),\n                (\"block_q_dkv\", 64),\n                (\"block_kv_dkv\", 64),\n                (\"block_q_dq\", 64),\n                (\"block_kv_dq\", 64),\n            ),\n            (\n                (\"block_q\", 64),\n                (\"block_k\", 128),\n                (\"block_q_dkv\", 64),\n                (\"block_kv_dkv\", 32),\n                (\"block_q_dq\", 32),\n                (\"block_kv_dq\", 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(\"8.0\"):\n        # TODO(b/416306534)\n        self.skipTest(\"Precision issues after CUDA 12.8.1 upgrade\")\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n>     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:505: in value_and_grad_f\n    g = vjp_py(lax_internal._one(ans))\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2105: in _vjp_pullback_wrapper\n    ans = fun(*args)\n          ^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:320: in unbound_vjp\n    arg_cts = backward_pass(jaxpr, True, consts, dummy_args, cts)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:438: in backward_pass\n    cts_out = get_primitive_transpose(eqn.primitive)(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:1559: in _custom_lin_transpose\n    cts_in = bwd.call_wrapped(*res, *cts_out)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:921: in _flatten_bwd\n    py_cts_in = f(py_res, py_cts_out)\n                ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:234: in _prepend_static_args\n    return f(*all_args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:602: in _mha_backward\n    dq, dk, dv = pl.pallas_call(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = <jaxlib._jax.Client object at 0x7f38df090040>\nmodule = <jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7ef6a0197a60>\nexecutable_devices = (RocmDevice(id=0),)\noptions = <jaxlib._jax.CompileOptions object at 0x2b2c2fd0>, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -> xc.LoadedExecutable:\n      sym_name = module.operation.attributes['sym_name']\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version < 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            \"Compiling module %s with FDO profile of length %d\",\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n>         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError"}, "teardown": {"duration": 0.00026570260524749756, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd5", "lineno": 224, "outcome": "failed", "keywords": ["test_fused_attention_bwd5", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00027390383183956146, "outcome": "passed"}, "call": {"duration": 0.33072990737855434, "outcome": "failed", "crash": {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 81920, available: 65536"}, "traceback": [{"path": "jax/tests/pallas/gpu_ops_test.py", "lineno": 302, "message": ""}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 427, "message": "in grad_f"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 505, "message": "in value_and_grad_f"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py", "lineno": 488, "message": "in __call__"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 2105, "message": "in _vjp_pullback_wrapper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py", "lineno": 488, "message": "in __call__"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 320, "message": "in unbound_vjp"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 438, "message": "in backward_pass"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 1559, "message": "in _custom_lin_transpose"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 212, "message": "in call_wrapped"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 921, "message": "in _flatten_bwd"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py", "lineno": 234, "message": "in _prepend_static_args"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 421, "message": "in _get_result_paths_thunk"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py", "lineno": 602, "message": "in _mha_backward"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 263, "message": "in cache_miss"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 146, "message": "in _python_pjit_helper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 1600, "message": "in _pjit_call_impl_python"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2527, "message": "in compile"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 3073, "message": "in from_hlo"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2854, "message": "in _cached_compilation"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 491, "message": "in compile_or_get_cached"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 759, "message": "in _compile_and_write_cache"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py", "lineno": 359, "message": "in wrapper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 385, "message": "in backend_compile_and_load"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "JaxRuntimeError"}], "longrepr": ">   dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n>   return attention.mha(\n        q, k, v,\n        block_sizes=BlockSizes(**dict(block_sizes)),\n        causal=causal,\n        segment_ids=segment_ids,\n        interpret=self.INTERPRET).sum()\nE   jax._src.source_info_util.JaxStackTraceBeforeTransformation: jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 81920, available: 65536\nE   \nE   The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\nE   \nE   --------------------\n\njax/tests/pallas/gpu_ops_test.py:292: JaxStackTraceBeforeTransformation\n\nThe above exception was the direct cause of the following exception:\n\nself = <tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd5>\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (\"block_q\", 128),\n                (\"block_k\", 128),\n                (\"block_q_dkv\", 32),\n                (\"block_kv_dkv\", 32),\n                (\"block_q_dq\", 32),\n                (\"block_kv_dq\", 128),\n            ),\n            (\n                (\"block_q\", 64),\n                (\"block_k\", 64),\n                (\"block_q_dkv\", 64),\n                (\"block_kv_dkv\", 64),\n                (\"block_q_dq\", 64),\n                (\"block_kv_dq\", 64),\n            ),\n            (\n                (\"block_q\", 64),\n                (\"block_k\", 128),\n                (\"block_q_dkv\", 64),\n                (\"block_kv_dkv\", 32),\n                (\"block_q_dq\", 32),\n                (\"block_kv_dq\", 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(\"8.0\"):\n        # TODO(b/416306534)\n        self.skipTest(\"Precision issues after CUDA 12.8.1 upgrade\")\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n>     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:505: in value_and_grad_f\n    g = vjp_py(lax_internal._one(ans))\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2105: in _vjp_pullback_wrapper\n    ans = fun(*args)\n          ^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py:488: in __call__\n    return self.fun(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:320: in unbound_vjp\n    arg_cts = backward_pass(jaxpr, True, consts, dummy_args, cts)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:438: in backward_pass\n    cts_out = get_primitive_transpose(eqn.primitive)(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:1559: in _custom_lin_transpose\n    cts_in = bwd.call_wrapped(*res, *cts_out)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:921: in _flatten_bwd\n    py_cts_in = f(py_res, py_cts_out)\n                ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:234: in _prepend_static_args\n    return f(*all_args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:602: in _mha_backward\n    dq, dk, dv = pl.pallas_call(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = <jaxlib._jax.Client object at 0x7f38df090040>\nmodule = <jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7ef6801f9a80>\nexecutable_devices = (RocmDevice(id=0),)\noptions = <jaxlib._jax.CompileOptions object at 0x296fb1c0>, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -> xc.LoadedExecutable:\n      sym_name = module.operation.attributes['sym_name']\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version < 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            \"Compiling module %s with FDO profile of length %d\",\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n>         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 81920, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError"}, "teardown": {"duration": 0.00028868671506643295, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd6", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd6", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002516312524676323, "outcome": "passed"}, "call": {"duration": 5.905408648774028, "outcome": "passed"}, "teardown": {"duration": 0.00017980486154556274, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd7", "lineno": 224, "outcome": "failed", "keywords": ["test_fused_attention_bwd7", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00025236140936613083, "outcome": "passed"}, "call": {"duration": 0.21649867109954357, "outcome": "failed", "crash": {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536"}, "traceback": [{"path": "jax/tests/pallas/gpu_ops_test.py", "lineno": 302, "message": ""}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 427, "message": "in grad_f"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 500, "message": "in value_and_grad_f"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 2190, "message": "in _vjp"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 313, "message": "in vjp"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 287, "message": "in linearize"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 261, "message": "in direct_linearize"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 212, "message": "in call_wrapped"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py", "lineno": 90, "message": "in flatten_fun_nokwargs"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py", "lineno": 292, "message": "in _argnums_partial"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 421, "message": "in _get_result_paths_thunk"}, {"path": "jax/tests/pallas/gpu_ops_test.py", "lineno": 292, "message": "in f"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 749, "message": "in __call__"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/core.py", "lineno": 632, "message": "in bind"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/core.py", "lineno": 648, "message": "in _true_bind"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 1007, "message": "in bind_with_trace"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 979, "message": "in process_custom_vjp_call"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 212, "message": "in call_wrapped"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 850, "message": "in _flatten_fwd"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py", "lineno": 292, "message": "in _argnums_partial"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 421, "message": "in _get_result_paths_thunk"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py", "lineno": 314, "message": "in _mha_forward"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 749, "message": "in __call__"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/core.py", "lineno": 632, "message": "in bind"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/core.py", "lineno": 648, "message": "in _true_bind"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 1007, "message": "in bind_with_trace"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/core.py", "lineno": 1210, "message": "in process_custom_vjp_call"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 212, "message": "in call_wrapped"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 87, "message": "in _flatten_fun_nokwargs"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 759, "message": "in _check_primal_refs"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py", "lineno": 292, "message": "in _argnums_partial"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 421, "message": "in _get_result_paths_thunk"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 263, "message": "in cache_miss"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 146, "message": "in _python_pjit_helper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 1600, "message": "in _pjit_call_impl_python"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2527, "message": "in compile"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 3073, "message": "in from_hlo"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2854, "message": "in _cached_compilation"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 491, "message": "in compile_or_get_cached"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 759, "message": "in _compile_and_write_cache"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py", "lineno": 359, "message": "in wrapper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 385, "message": "in backend_compile_and_load"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "JaxRuntimeError"}], "longrepr": "self = <tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd7>\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (\"block_q\", 128),\n                (\"block_k\", 128),\n                (\"block_q_dkv\", 32),\n                (\"block_kv_dkv\", 32),\n                (\"block_q_dq\", 32),\n                (\"block_kv_dq\", 128),\n            ),\n            (\n                (\"block_q\", 64),\n                (\"block_k\", 64),\n                (\"block_q_dkv\", 64),\n                (\"block_kv_dkv\", 64),\n                (\"block_q_dq\", 64),\n                (\"block_kv_dq\", 64),\n            ),\n            (\n                (\"block_q\", 64),\n                (\"block_k\", 128),\n                (\"block_q_dkv\", 64),\n                (\"block_kv_dkv\", 32),\n                (\"block_q_dq\", 32),\n                (\"block_kv_dq\", 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(\"8.0\"):\n        # TODO(b/416306534)\n        self.skipTest(\"Precision issues after CUDA 12.8.1 upgrade\")\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n>     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:500: in value_and_grad_f\n    ans, vjp_py = _vjp(f_partial, *dyn_args)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2190: in _vjp\n    out_primals, vjp = ad.vjp(flat_fun, primals_flat)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:313: in vjp\n    out_primals, pvals, jaxpr, consts = linearize(traceable, *primals)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:287: in linearize\n    return direct_linearize(traceable, primals, kwargs, has_aux=has_aux)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:261: in direct_linearize\n    ans = traceable.call_wrapped(*tracers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:90: in flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\njax/tests/pallas/gpu_ops_test.py:292: in f\n    return attention.mha(\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:979: in process_custom_vjp_call\n    res_and_primals_out = fwd.call_wrapped(*fwd_in_flat)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:850: in _flatten_fwd\n    pair_out = f(*py_args)\n               ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:314: in _mha_forward\n    out, lse = mha(q, k, v, segment_ids=segment_ids, sm_scale=sm_scale,\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:1210: in process_custom_vjp_call\n    return fun.call_wrapped(*tracers)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:87: in _flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:759: in _check_primal_refs\n    out = f(*args)\n          ^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = <jaxlib._jax.Client object at 0x7f38df090040>\nmodule = <jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7ef68005a1b0>\nexecutable_devices = (RocmDevice(id=0),)\noptions = <jaxlib._jax.CompileOptions object at 0x2a7c39d0>, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -> xc.LoadedExecutable:\n      sym_name = module.operation.attributes['sym_name']\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version < 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            \"Compiling module %s with FDO profile of length %d\",\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n>         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError"}, "teardown": {"duration": 0.00026752520352602005, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd8", "lineno": 224, "outcome": "failed", "keywords": ["test_fused_attention_bwd8", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00025788135826587677, "outcome": "passed"}, "call": {"duration": 0.14017072226852179, "outcome": "failed", "crash": {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 98304, available: 65536"}, "traceback": [{"path": "jax/tests/pallas/gpu_ops_test.py", "lineno": 302, "message": ""}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 427, "message": "in grad_f"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 500, "message": "in value_and_grad_f"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api.py", "lineno": 2190, "message": "in _vjp"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 313, "message": "in vjp"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 287, "message": "in linearize"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 261, "message": "in direct_linearize"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 212, "message": "in call_wrapped"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py", "lineno": 90, "message": "in flatten_fun_nokwargs"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py", "lineno": 292, "message": "in _argnums_partial"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 421, "message": "in _get_result_paths_thunk"}, {"path": "jax/tests/pallas/gpu_ops_test.py", "lineno": 292, "message": "in f"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 749, "message": "in __call__"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/core.py", "lineno": 632, "message": "in bind"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/core.py", "lineno": 648, "message": "in _true_bind"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 1007, "message": "in bind_with_trace"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py", "lineno": 979, "message": "in process_custom_vjp_call"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 212, "message": "in call_wrapped"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 850, "message": "in _flatten_fwd"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py", "lineno": 292, "message": "in _argnums_partial"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 421, "message": "in _get_result_paths_thunk"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py", "lineno": 314, "message": "in _mha_forward"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 749, "message": "in __call__"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/core.py", "lineno": 632, "message": "in bind"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/core.py", "lineno": 648, "message": "in _true_bind"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 1007, "message": "in bind_with_trace"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/core.py", "lineno": 1210, "message": "in process_custom_vjp_call"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 212, "message": "in call_wrapped"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 87, "message": "in _flatten_fun_nokwargs"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 759, "message": "in _check_primal_refs"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py", "lineno": 292, "message": "in _argnums_partial"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 421, "message": "in _get_result_paths_thunk"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 263, "message": "in cache_miss"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 146, "message": "in _python_pjit_helper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 1600, "message": "in _pjit_call_impl_python"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2527, "message": "in compile"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 3073, "message": "in from_hlo"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2854, "message": "in _cached_compilation"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 491, "message": "in compile_or_get_cached"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 759, "message": "in _compile_and_write_cache"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py", "lineno": 359, "message": "in wrapper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 385, "message": "in backend_compile_and_load"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "JaxRuntimeError"}], "longrepr": "self = <tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_bwd8>\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2),\n        head_dim=(32, 64, 72, 128,),\n        block_sizes=(\n            (\n                (\"block_q\", 128),\n                (\"block_k\", 128),\n                (\"block_q_dkv\", 32),\n                (\"block_kv_dkv\", 32),\n                (\"block_q_dq\", 32),\n                (\"block_kv_dq\", 128),\n            ),\n            (\n                (\"block_q\", 64),\n                (\"block_k\", 64),\n                (\"block_q_dkv\", 64),\n                (\"block_kv_dkv\", 64),\n                (\"block_q_dq\", 64),\n                (\"block_kv_dq\", 64),\n            ),\n            (\n                (\"block_q\", 64),\n                (\"block_k\", 128),\n                (\"block_q_dkv\", 64),\n                (\"block_kv_dkv\", 32),\n                (\"block_q_dq\", 32),\n                (\"block_kv_dq\", 64),\n            ),\n        ),\n        causal=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_bwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_segment_ids,\n    ):\n      if jtu.is_cuda_compute_capability_at_least(\"8.0\"):\n        # TODO(b/416306534)\n        self.skipTest(\"Precision issues after CUDA 12.8.1 upgrade\")\n    \n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      def f(q, k, v):\n        return attention.mha(\n            q, k, v,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET).sum()\n    \n      def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    \n>     dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:302: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:427: in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:500: in value_and_grad_f\n    ans, vjp_py = _vjp(f_partial, *dyn_args)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api.py:2190: in _vjp\n    out_primals, vjp = ad.vjp(flat_fun, primals_flat)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:313: in vjp\n    out_primals, pvals, jaxpr, consts = linearize(traceable, *primals)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:287: in linearize\n    return direct_linearize(traceable, primals, kwargs, has_aux=has_aux)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:261: in direct_linearize\n    ans = traceable.call_wrapped(*tracers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:90: in flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\njax/tests/pallas/gpu_ops_test.py:292: in f\n    return attention.mha(\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/ad.py:979: in process_custom_vjp_call\n    res_and_primals_out = fwd.call_wrapped(*fwd_in_flat)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:850: in _flatten_fwd\n    pair_out = f(*py_args)\n               ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/experimental/pallas/ops/gpu/attention.py:314: in _mha_forward\n    out, lse = mha(q, k, v, segment_ids=segment_ids, sm_scale=sm_scale,\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:1210: in process_custom_vjp_call\n    return fun.call_wrapped(*tracers)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:87: in _flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:759: in _check_primal_refs\n    out = f(*args)\n          ^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = <jaxlib._jax.Client object at 0x7f38df090040>\nmodule = <jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7ef6800cdcb0>\nexecutable_devices = (RocmDevice(id=0),)\noptions = <jaxlib._jax.CompileOptions object at 0x2a98ab20>, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -> xc.LoadedExecutable:\n      sym_name = module.operation.attributes['sym_name']\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version < 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            \"Compiling module %s with FDO profile of length %d\",\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n>         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 98304, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError"}, "teardown": {"duration": 0.00028993841260671616, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_bwd9", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd9", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00025567691773176193, "outcome": "passed"}, "call": {"duration": 14.40973202418536, "outcome": "passed"}, "teardown": {"duration": 0.0001666257157921791, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd0", "lineno": 151, "outcome": "failed", "keywords": ["test_fused_attention_fwd0", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00020460225641727448, "outcome": "passed"}, "call": {"duration": 0.24385590385645628, "outcome": "failed", "crash": {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536"}, "traceback": [{"path": "jax/tests/pallas/gpu_ops_test.py", "lineno": 221, "message": ""}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 263, "message": "in cache_miss"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 146, "message": "in _python_pjit_helper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 1600, "message": "in _pjit_call_impl_python"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2527, "message": "in compile"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 3073, "message": "in from_hlo"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2854, "message": "in _cached_compilation"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 491, "message": "in compile_or_get_cached"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 759, "message": "in _compile_and_write_cache"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py", "lineno": 359, "message": "in wrapper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 385, "message": "in backend_compile_and_load"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "JaxRuntimeError"}], "longrepr": "self = <tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_fwd0>\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2, 8),\n        head_dim=(32, 64, 72, 128),\n        block_sizes=(\n          ((\"block_q\", 128), (\"block_k\", 128)),\n          ((\"block_q\", 64), (\"block_k\", 64)),\n          ((\"block_q\", 64), (\"block_k\", 128)),\n        ),\n        causal=(True, False),\n        use_fwd=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_fwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_fwd,\n        use_segment_ids,\n    ):\n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      if use_fwd:\n    \n        @jax.jit\n        def impl(q, k, v):\n          v, _ = jax.vjp(\n              functools.partial(\n                  attention.mha,\n                  block_sizes=BlockSizes(**dict(block_sizes)),\n                  causal=causal,\n                  segment_ids=segment_ids,\n                  interpret=self.INTERPRET,\n              ),\n              q,\n              k,\n              v,\n          )\n          return v\n    \n      else:\n        impl = functools.partial(\n            attention.mha,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET,\n        )\n>     o = impl(q, k, v)\n          ^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:221: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = <jaxlib._jax.Client object at 0x7f38df090040>\nmodule = <jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f0f7f4eeca0>\nexecutable_devices = (RocmDevice(id=0),)\noptions = <jaxlib._jax.CompileOptions object at 0x30043890>, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -> xc.LoadedExecutable:\n      sym_name = module.operation.attributes['sym_name']\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version < 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            \"Compiling module %s with FDO profile of length %d\",\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n>         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError"}, "teardown": {"duration": 0.00022606458514928818, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd1", "lineno": 151, "outcome": "failed", "keywords": ["test_fused_attention_fwd1", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002092793583869934, "outcome": "passed"}, "call": {"duration": 0.23594362754374743, "outcome": "failed", "crash": {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536"}, "traceback": [{"path": "jax/tests/pallas/gpu_ops_test.py", "lineno": 221, "message": ""}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 263, "message": "in cache_miss"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 146, "message": "in _python_pjit_helper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 1600, "message": "in _pjit_call_impl_python"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2527, "message": "in compile"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 3073, "message": "in from_hlo"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2854, "message": "in _cached_compilation"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 491, "message": "in compile_or_get_cached"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 759, "message": "in _compile_and_write_cache"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py", "lineno": 359, "message": "in wrapper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 385, "message": "in backend_compile_and_load"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "JaxRuntimeError"}], "longrepr": "self = <tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_fwd1>\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2, 8),\n        head_dim=(32, 64, 72, 128),\n        block_sizes=(\n          ((\"block_q\", 128), (\"block_k\", 128)),\n          ((\"block_q\", 64), (\"block_k\", 64)),\n          ((\"block_q\", 64), (\"block_k\", 128)),\n        ),\n        causal=(True, False),\n        use_fwd=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_fwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_fwd,\n        use_segment_ids,\n    ):\n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      if use_fwd:\n    \n        @jax.jit\n        def impl(q, k, v):\n          v, _ = jax.vjp(\n              functools.partial(\n                  attention.mha,\n                  block_sizes=BlockSizes(**dict(block_sizes)),\n                  causal=causal,\n                  segment_ids=segment_ids,\n                  interpret=self.INTERPRET,\n              ),\n              q,\n              k,\n              v,\n          )\n          return v\n    \n      else:\n        impl = functools.partial(\n            attention.mha,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET,\n        )\n>     o = impl(q, k, v)\n          ^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:221: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = <jaxlib._jax.Client object at 0x7f38df090040>\nmodule = <jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7ef6a01dde90>\nexecutable_devices = (RocmDevice(id=0),)\noptions = <jaxlib._jax.CompileOptions object at 0x226d2210>, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -> xc.LoadedExecutable:\n      sym_name = module.operation.attributes['sym_name']\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version < 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            \"Compiling module %s with FDO profile of length %d\",\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n>         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 131072, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError"}, "teardown": {"duration": 0.00021079089492559433, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd2", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd2", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00021136179566383362, "outcome": "passed"}, "call": {"duration": 3.516960049048066, "outcome": "passed"}, "teardown": {"duration": 0.00013077352195978165, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd3", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd3", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00021809246391057968, "outcome": "passed"}, "call": {"duration": 2.0191930662840605, "outcome": "passed"}, "teardown": {"duration": 0.0001317942515015602, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd4", "lineno": 151, "outcome": "failed", "keywords": ["test_fused_attention_fwd4", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00023542623966932297, "outcome": "passed"}, "call": {"duration": 0.17577781155705452, "outcome": "failed", "crash": {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 98304, available: 65536"}, "traceback": [{"path": "jax/tests/pallas/gpu_ops_test.py", "lineno": 221, "message": ""}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 263, "message": "in cache_miss"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 146, "message": "in _python_pjit_helper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 1600, "message": "in _pjit_call_impl_python"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2527, "message": "in compile"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 3073, "message": "in from_hlo"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2854, "message": "in _cached_compilation"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 491, "message": "in compile_or_get_cached"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 759, "message": "in _compile_and_write_cache"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py", "lineno": 359, "message": "in wrapper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 385, "message": "in backend_compile_and_load"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "JaxRuntimeError"}], "longrepr": "self = <tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_fwd4>\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2, 8),\n        head_dim=(32, 64, 72, 128),\n        block_sizes=(\n          ((\"block_q\", 128), (\"block_k\", 128)),\n          ((\"block_q\", 64), (\"block_k\", 64)),\n          ((\"block_q\", 64), (\"block_k\", 128)),\n        ),\n        causal=(True, False),\n        use_fwd=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_fwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_fwd,\n        use_segment_ids,\n    ):\n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      if use_fwd:\n    \n        @jax.jit\n        def impl(q, k, v):\n          v, _ = jax.vjp(\n              functools.partial(\n                  attention.mha,\n                  block_sizes=BlockSizes(**dict(block_sizes)),\n                  causal=causal,\n                  segment_ids=segment_ids,\n                  interpret=self.INTERPRET,\n              ),\n              q,\n              k,\n              v,\n          )\n          return v\n    \n      else:\n        impl = functools.partial(\n            attention.mha,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET,\n        )\n>     o = impl(q, k, v)\n          ^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:221: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = <jaxlib._jax.Client object at 0x7f38df090040>\nmodule = <jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7ef5e00626b0>\nexecutable_devices = (RocmDevice(id=0),)\noptions = <jaxlib._jax.CompileOptions object at 0x30b59c40>, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -> xc.LoadedExecutable:\n      sym_name = module.operation.attributes['sym_name']\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version < 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            \"Compiling module %s with FDO profile of length %d\",\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n>         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 98304, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError"}, "teardown": {"duration": 0.00022016558796167374, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd5", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd5", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002065049484372139, "outcome": "passed"}, "call": {"duration": 4.45853575039655, "outcome": "passed"}, "teardown": {"duration": 0.000161709263920784, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd6", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd6", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00023847166448831558, "outcome": "passed"}, "call": {"duration": 2.314480229280889, "outcome": "passed"}, "teardown": {"duration": 0.0001597357913851738, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd7", "lineno": 151, "outcome": "failed", "keywords": ["test_fused_attention_fwd7", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002569882199168205, "outcome": "passed"}, "call": {"duration": 0.14055754616856575, "outcome": "failed", "crash": {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 98304, available: 65536"}, "traceback": [{"path": "jax/tests/pallas/gpu_ops_test.py", "lineno": 221, "message": ""}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 749, "message": "in __call__"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/core.py", "lineno": 632, "message": "in bind"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/core.py", "lineno": 648, "message": "in _true_bind"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 1007, "message": "in bind_with_trace"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/core.py", "lineno": 1210, "message": "in process_custom_vjp_call"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 212, "message": "in call_wrapped"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 87, "message": "in _flatten_fun_nokwargs"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py", "lineno": 759, "message": "in _check_primal_refs"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py", "lineno": 292, "message": "in _argnums_partial"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py", "lineno": 421, "message": "in _get_result_paths_thunk"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 263, "message": "in cache_miss"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 146, "message": "in _python_pjit_helper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 1600, "message": "in _pjit_call_impl_python"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2527, "message": "in compile"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 3073, "message": "in from_hlo"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2854, "message": "in _cached_compilation"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 491, "message": "in compile_or_get_cached"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 759, "message": "in _compile_and_write_cache"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py", "lineno": 359, "message": "in wrapper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 385, "message": "in backend_compile_and_load"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "JaxRuntimeError"}], "longrepr": "self = <tests.pallas.gpu_ops_test.FusedAttentionTest testMethod=test_fused_attention_fwd7>\n\n    @jtu.sample_product(\n        batch_size=(1, 2),\n        seq_len=(128, 384),\n        num_heads=(1, 2, 8),\n        head_dim=(32, 64, 72, 128),\n        block_sizes=(\n          ((\"block_q\", 128), (\"block_k\", 128)),\n          ((\"block_q\", 64), (\"block_k\", 64)),\n          ((\"block_q\", 64), (\"block_k\", 128)),\n        ),\n        causal=(True, False),\n        use_fwd=(True, False),\n        use_segment_ids=(True, False),\n    )\n    def test_fused_attention_fwd(\n        self,\n        *,\n        batch_size,\n        seq_len,\n        num_heads,\n        head_dim,\n        block_sizes,\n        causal,\n        use_fwd,\n        use_segment_ids,\n    ):\n      k1, k2, k3 = random.split(random.key(0), 3)\n      q = random.normal(\n          k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      k = random.normal(\n          k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      v = random.normal(\n          k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n      )\n      if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n      else:\n        segment_ids = None\n    \n      if use_fwd:\n    \n        @jax.jit\n        def impl(q, k, v):\n          v, _ = jax.vjp(\n              functools.partial(\n                  attention.mha,\n                  block_sizes=BlockSizes(**dict(block_sizes)),\n                  causal=causal,\n                  segment_ids=segment_ids,\n                  interpret=self.INTERPRET,\n              ),\n              q,\n              k,\n              v,\n          )\n          return v\n    \n      else:\n        impl = functools.partial(\n            attention.mha,\n            block_sizes=BlockSizes(**dict(block_sizes)),\n            causal=causal,\n            segment_ids=segment_ids,\n            interpret=self.INTERPRET,\n        )\n>     o = impl(q, k, v)\n          ^^^^^^^^^^^^^\n\njax/tests/pallas/gpu_ops_test.py:221: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:749: in __call__\n    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:632: in bind\n    return self._true_bind(*args, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:648: in _true_bind\n    return self.bind_with_trace(prev_trace, args, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:1007: in bind_with_trace\n    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/core.py:1210: in process_custom_vjp_call\n    return fun.call_wrapped(*tracers)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:212: in call_wrapped\n    return self.f_transformed(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:87: in _flatten_fun_nokwargs\n    ans = f(*py_args)\n          ^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/custom_derivatives.py:759: in _check_primal_refs\n    out = f(*args)\n          ^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/api_util.py:292: in _argnums_partial\n    return _fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/linear_util.py:421: in _get_result_paths_thunk\n    ans = _fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = <jaxlib._jax.Client object at 0x7f38df090040>\nmodule = <jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7ef60013a930>\nexecutable_devices = (RocmDevice(id=0),)\noptions = <jaxlib._jax.CompileOptions object at 0x30df93e0>, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -> xc.LoadedExecutable:\n      sym_name = module.operation.attributes['sym_name']\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version < 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            \"Compiling module %s with FDO profile of length %d\",\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n>         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 98304, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError"}, "teardown": {"duration": 0.0002676350995898247, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd8", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd8", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002543153241276741, "outcome": "passed"}, "call": {"duration": 4.543047449551523, "outcome": "passed"}, "teardown": {"duration": 0.0001873774453997612, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_fused_attention_fwd9", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd9", "__wrapped__", "__x_params_repr__", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002764575183391571, "outcome": "passed"}, "call": {"duration": 6.357062160037458, "outcome": "passed"}, "teardown": {"duration": 0.0001616794615983963, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionTest::test_return_residuals_not_differentiable", "lineno": 308, "outcome": "passed", "keywords": ["test_return_residuals_not_differentiable", "FusedAttentionTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00028385967016220093, "outcome": "passed"}, "call": {"duration": 4.6943336529657245, "outcome": "passed"}, "teardown": {"duration": 0.00020410027354955673, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd0", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd0", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00029229093343019485, "outcome": "passed"}, "call": {"duration": 2.647285538725555, "outcome": "passed"}, "teardown": {"duration": 0.00015883427113294601, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd1", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd1", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00022007431834936142, "outcome": "passed"}, "call": {"duration": 11.29375343862921, "outcome": "passed"}, "teardown": {"duration": 0.00016824807971715927, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd2", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd2", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00023739971220493317, "outcome": "passed"}, "call": {"duration": 2.540117010474205, "outcome": "passed"}, "teardown": {"duration": 0.00016198866069316864, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd3", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd3", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00022086594253778458, "outcome": "passed"}, "call": {"duration": 3.1643069442361593, "outcome": "passed"}, "teardown": {"duration": 0.0001588640734553337, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd4", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd4", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002235090360045433, "outcome": "passed"}, "call": {"duration": 4.310219325125217, "outcome": "passed"}, "teardown": {"duration": 0.00014960113912820816, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd5", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd5", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00022552255541086197, "outcome": "passed"}, "call": {"duration": 6.532955233938992, "outcome": "passed"}, "teardown": {"duration": 0.00019278470426797867, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd6", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd6", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002521323040127754, "outcome": "passed"}, "call": {"duration": 1.9102790830656886, "outcome": "passed"}, "teardown": {"duration": 0.0001834603026509285, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd7", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd7", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00025839079171419144, "outcome": "passed"}, "call": {"duration": 10.689798980019987, "outcome": "passed"}, "teardown": {"duration": 0.00016955099999904633, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd8", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd8", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00023499689996242523, "outcome": "passed"}, "call": {"duration": 9.722324631176889, "outcome": "passed"}, "teardown": {"duration": 0.00013961642980575562, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_bwd9", "lineno": 224, "outcome": "passed", "keywords": ["test_fused_attention_bwd9", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001986231654882431, "outcome": "passed"}, "call": {"duration": 1.8537289323285222, "outcome": "passed"}, "teardown": {"duration": 0.00022824667394161224, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd0", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd0", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0003643771633505821, "outcome": "passed"}, "call": {"duration": 2.882239155471325, "outcome": "passed"}, "teardown": {"duration": 0.00013856403529644012, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd1", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd1", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00019908417016267776, "outcome": "passed"}, "call": {"duration": 2.032077334821224, "outcome": "passed"}, "teardown": {"duration": 0.00013593025505542755, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd2", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd2", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00019558891654014587, "outcome": "passed"}, "call": {"duration": 1.3410013690590858, "outcome": "passed"}, "teardown": {"duration": 0.00013752281665802002, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd3", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd3", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00019537750631570816, "outcome": "passed"}, "call": {"duration": 1.9958497378975153, "outcome": "passed"}, "teardown": {"duration": 0.0001457948237657547, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd4", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd4", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00019684992730617523, "outcome": "passed"}, "call": {"duration": 2.564284129999578, "outcome": "passed"}, "teardown": {"duration": 0.00013516005128622055, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd5", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd5", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00019233394414186478, "outcome": "passed"}, "call": {"duration": 0.5687371827661991, "outcome": "passed"}, "teardown": {"duration": 0.00013347715139389038, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd6", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd6", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00020268000662326813, "outcome": "passed"}, "call": {"duration": 0.5573796583339572, "outcome": "passed"}, "teardown": {"duration": 0.0001345379278063774, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd7", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd7", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.000202057883143425, "outcome": "passed"}, "call": {"duration": 0.5688696075230837, "outcome": "passed"}, "teardown": {"duration": 0.00014298129826784134, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd8", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd8", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00019891373813152313, "outcome": "passed"}, "call": {"duration": 0.590957909822464, "outcome": "passed"}, "teardown": {"duration": 0.00018589571118354797, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_fused_attention_fwd9", "lineno": 151, "outcome": "passed", "keywords": ["test_fused_attention_fwd9", "__wrapped__", "__x_params_repr__", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00024345982819795609, "outcome": "passed"}, "call": {"duration": 0.5657296953722835, "outcome": "passed"}, "teardown": {"duration": 0.00015873461961746216, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedAttentionInterpretTest::test_return_residuals_not_differentiable", "lineno": 308, "outcome": "passed", "keywords": ["test_return_residuals_not_differentiable", "FusedAttentionInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002475650981068611, "outcome": "passed"}, "call": {"duration": 1.3272357322275639, "outcome": "passed"}, "teardown": {"duration": 0.00019433721899986267, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_bwd0", "lineno": 358, "outcome": "passed", "keywords": ["test_fused_layernorm_bwd0", "__wrapped__", "__x_params_repr__", "FusedLayerNormTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002765581011772156, "outcome": "passed"}, "call": {"duration": 2.3081808174028993, "outcome": "passed"}, "teardown": {"duration": 0.00013545062392950058, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_bwd1", "lineno": 358, "outcome": "passed", "keywords": ["test_fused_layernorm_bwd1", "__wrapped__", "__x_params_repr__", "FusedLayerNormTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001970110461115837, "outcome": "passed"}, "call": {"duration": 1.9352432852610946, "outcome": "passed"}, "teardown": {"duration": 0.00013554003089666367, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_fwd0", "lineno": 344, "outcome": "passed", "keywords": ["test_fused_layernorm_fwd0", "__wrapped__", "__x_params_repr__", "FusedLayerNormTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00019784178584814072, "outcome": "passed"}, "call": {"duration": 0.3373680217191577, "outcome": "passed"}, "teardown": {"duration": 0.00014042668044567108, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormTest::test_fused_layernorm_fwd1", "lineno": 344, "outcome": "passed", "keywords": ["test_fused_layernorm_fwd1", "__wrapped__", "__x_params_repr__", "FusedLayerNormTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00019576866179704666, "outcome": "passed"}, "call": {"duration": 0.32784234173595905, "outcome": "passed"}, "teardown": {"duration": 0.00016449298709630966, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_bwd0", "lineno": 358, "outcome": "passed", "keywords": ["test_fused_layernorm_bwd0", "__wrapped__", "__x_params_repr__", "FusedLayerNormInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00023864209651947021, "outcome": "passed"}, "call": {"duration": 0.19424615520983934, "outcome": "passed"}, "teardown": {"duration": 0.00013000238686800003, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_bwd1", "lineno": 358, "outcome": "passed", "keywords": ["test_fused_layernorm_bwd1", "__wrapped__", "__x_params_repr__", "FusedLayerNormInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001866547390818596, "outcome": "passed"}, "call": {"duration": 0.1803413536399603, "outcome": "passed"}, "teardown": {"duration": 0.0001290012151002884, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_fwd0", "lineno": 344, "outcome": "passed", "keywords": ["test_fused_layernorm_fwd0", "__wrapped__", "__x_params_repr__", "FusedLayerNormInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00018342118710279465, "outcome": "passed"}, "call": {"duration": 0.010835021734237671, "outcome": "passed"}, "teardown": {"duration": 9.143538773059845e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::FusedLayerNormInterpretTest::test_fused_layernorm_fwd1", "lineno": 344, "outcome": "passed", "keywords": ["test_fused_layernorm_fwd1", "__wrapped__", "__x_params_repr__", "FusedLayerNormInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015841331332921982, "outcome": "passed"}, "call": {"duration": 0.003395971842110157, "outcome": "passed"}, "teardown": {"duration": 0.00010627694427967072, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_fwd0", "lineno": 392, "outcome": "passed", "keywords": ["test_rms_fwd0", "__wrapped__", "__x_params_repr__", "RmsNormTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00018258951604366302, "outcome": "passed"}, "call": {"duration": 0.33758200891315937, "outcome": "passed"}, "teardown": {"duration": 0.0001416793093085289, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_fwd1", "lineno": 392, "outcome": "passed", "keywords": ["test_rms_fwd1", "__wrapped__", "__x_params_repr__", "RmsNormTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00019216351211071014, "outcome": "passed"}, "call": {"duration": 0.34749927558004856, "outcome": "passed"}, "teardown": {"duration": 0.00013625063002109528, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_norm_bwd0", "lineno": 406, "outcome": "passed", "keywords": ["test_rms_norm_bwd0", "__wrapped__", "__x_params_repr__", "RmsNormTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00019462686032056808, "outcome": "passed"}, "call": {"duration": 0.9076203322038054, "outcome": "passed"}, "teardown": {"duration": 0.00025313347578048706, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormTest::test_rms_norm_bwd1", "lineno": 406, "outcome": "passed", "keywords": ["test_rms_norm_bwd1", "__wrapped__", "__x_params_repr__", "RmsNormTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0004256078973412514, "outcome": "passed"}, "call": {"duration": 0.8972729006782174, "outcome": "passed"}, "teardown": {"duration": 0.00015912484377622604, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_fwd0", "lineno": 392, "outcome": "passed", "keywords": ["test_rms_fwd0", "__wrapped__", "__x_params_repr__", "RmsNormInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00023224297910928726, "outcome": "passed"}, "call": {"duration": 0.0036790594458580017, "outcome": "passed"}, "teardown": {"duration": 0.00016357097774744034, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_fwd1", "lineno": 392, "outcome": "passed", "keywords": ["test_rms_fwd1", "__wrapped__", "__x_params_repr__", "RmsNormInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00028139445930719376, "outcome": "passed"}, "call": {"duration": 0.005292821675539017, "outcome": "passed"}, "teardown": {"duration": 0.0002019079402089119, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_norm_bwd0", "lineno": 406, "outcome": "passed", "keywords": ["test_rms_norm_bwd0", "__wrapped__", "__x_params_repr__", "RmsNormInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00035547371953725815, "outcome": "passed"}, "call": {"duration": 0.17177068255841732, "outcome": "passed"}, "teardown": {"duration": 0.00012482330203056335, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::RmsNormInterpretTest::test_rms_norm_bwd1", "lineno": 406, "outcome": "passed", "keywords": ["test_rms_norm_bwd1", "__wrapped__", "__x_params_repr__", "RmsNormInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001944471150636673, "outcome": "passed"}, "call": {"duration": 0.16085833590477705, "outcome": "passed"}, "teardown": {"duration": 0.00014673639088869095, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax0", "lineno": 440, "outcome": "passed", "keywords": ["test_softmax0", "__wrapped__", "__x_params_repr__", "SoftmaxTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00022032484412193298, "outcome": "passed"}, "call": {"duration": 1.2915658988058567, "outcome": "passed"}, "teardown": {"duration": 0.00021398533135652542, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax1", "lineno": 440, "outcome": "passed", "keywords": ["test_softmax1", "__wrapped__", "__x_params_repr__", "SoftmaxTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00034183450043201447, "outcome": "passed"}, "call": {"duration": 1.290388373658061, "outcome": "passed"}, "teardown": {"duration": 0.0002487972378730774, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax2", "lineno": 440, "outcome": "passed", "keywords": ["test_softmax2", "__wrapped__", "__x_params_repr__", "SoftmaxTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00042102113366127014, "outcome": "passed"}, "call": {"duration": 1.196023472584784, "outcome": "passed"}, "teardown": {"duration": 0.0002521732822060585, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax3", "lineno": 440, "outcome": "passed", "keywords": ["test_softmax3", "__wrapped__", "__x_params_repr__", "SoftmaxTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00039747729897499084, "outcome": "passed"}, "call": {"duration": 1.3451403332874179, "outcome": "passed"}, "teardown": {"duration": 0.00027140043675899506, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax4", "lineno": 440, "outcome": "passed", "keywords": ["test_softmax4", "__wrapped__", "__x_params_repr__", "SoftmaxTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00039684586226940155, "outcome": "passed"}, "call": {"duration": 1.3446537796407938, "outcome": "passed"}, "teardown": {"duration": 0.00013405829668045044, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxTest::test_softmax5", "lineno": 440, "outcome": "passed", "keywords": ["test_softmax5", "__wrapped__", "__x_params_repr__", "SoftmaxTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001998450607061386, "outcome": "passed"}, "call": {"duration": 1.1943838018923998, "outcome": "passed"}, "teardown": {"duration": 0.00029483530670404434, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax0", "lineno": 440, "outcome": "passed", "keywords": ["test_softmax0", "__wrapped__", "__x_params_repr__", "SoftmaxInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0004906440153717995, "outcome": "passed"}, "call": {"duration": 0.0060263741761446, "outcome": "passed"}, "teardown": {"duration": 0.00020368117839097977, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax1", "lineno": 440, "outcome": "passed", "keywords": ["test_softmax1", "__wrapped__", "__x_params_repr__", "SoftmaxInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00035736802965402603, "outcome": "passed"}, "call": {"duration": 0.004133050329983234, "outcome": "passed"}, "teardown": {"duration": 0.00019293464720249176, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax2", "lineno": 440, "outcome": "passed", "keywords": ["test_softmax2", "__wrapped__", "__x_params_repr__", "SoftmaxInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0003440072759985924, "outcome": "passed"}, "call": {"duration": 0.0036853784695267677, "outcome": "passed"}, "teardown": {"duration": 0.0001937858760356903, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax3", "lineno": 440, "outcome": "passed", "keywords": ["test_softmax3", "__wrapped__", "__x_params_repr__", "SoftmaxInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0003515789285302162, "outcome": "passed"}, "call": {"duration": 0.006771694868803024, "outcome": "passed"}, "teardown": {"duration": 0.00020433217287063599, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax4", "lineno": 440, "outcome": "passed", "keywords": ["test_softmax4", "__wrapped__", "__x_params_repr__", "SoftmaxInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00035317055881023407, "outcome": "passed"}, "call": {"duration": 0.006532301194965839, "outcome": "passed"}, "teardown": {"duration": 0.00021753180772066116, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_ops_test.py::SoftmaxInterpretTest::test_softmax5", "lineno": 440, "outcome": "passed", "keywords": ["test_softmax5", "__wrapped__", "__x_params_repr__", "SoftmaxInterpretTest", "gpu_ops_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00034751277416944504, "outcome": "passed"}, "call": {"duration": 0.00665467232465744, "outcome": "passed"}, "teardown": {"duration": 0.0010243924334645271, "outcome": "passed"}}]}