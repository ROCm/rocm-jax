{"created": 1763581258.87619, "duration": 69.35270142555237, "exitcode": 1, "root": "/dockerx/rocm/rocm-jax/jax", "environment": {}, "summary": {"passed": 30, "skipped": 8, "failed": 2, "total": 40, "collected": 40}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_paged_attention0", "type": "TestCaseFunction", "lineno": 114}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_paged_attention1", "type": "TestCaseFunction", "lineno": 114}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_paged_attention2", "type": "TestCaseFunction", "lineno": 114}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_paged_attention3", "type": "TestCaseFunction", "lineno": 114}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_paged_attention4", "type": "TestCaseFunction", "lineno": 114}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_paged_attention5", "type": "TestCaseFunction", "lineno": 114}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_paged_attention6", "type": "TestCaseFunction", "lineno": 114}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_paged_attention7", "type": "TestCaseFunction", "lineno": 114}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_paged_attention8", "type": "TestCaseFunction", "lineno": 114}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_paged_attention9", "type": "TestCaseFunction", "lineno": 114}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_quantized_paged_attention0", "type": "TestCaseFunction", "lineno": 169}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_quantized_paged_attention1", "type": "TestCaseFunction", "lineno": 169}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_quantized_paged_attention2", "type": "TestCaseFunction", "lineno": 169}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_quantized_paged_attention3", "type": "TestCaseFunction", "lineno": 169}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_quantized_paged_attention4", "type": "TestCaseFunction", "lineno": 169}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_quantized_paged_attention5", "type": "TestCaseFunction", "lineno": 169}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_quantized_paged_attention6", "type": "TestCaseFunction", "lineno": 169}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_quantized_paged_attention7", "type": "TestCaseFunction", "lineno": 169}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_quantized_paged_attention8", "type": "TestCaseFunction", "lineno": 169}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_quantized_paged_attention9", "type": "TestCaseFunction", "lineno": 169}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_paged_attention0", "type": "TestCaseFunction", "lineno": 114}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_paged_attention1", "type": "TestCaseFunction", "lineno": 114}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_paged_attention2", "type": "TestCaseFunction", "lineno": 114}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_paged_attention3", "type": "TestCaseFunction", "lineno": 114}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_paged_attention4", "type": "TestCaseFunction", "lineno": 114}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_paged_attention5", "type": "TestCaseFunction", "lineno": 114}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_paged_attention6", "type": "TestCaseFunction", "lineno": 114}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_paged_attention7", "type": "TestCaseFunction", "lineno": 114}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_paged_attention8", "type": "TestCaseFunction", "lineno": 114}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_paged_attention9", "type": "TestCaseFunction", "lineno": 114}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_quantized_paged_attention0", "type": "TestCaseFunction", "lineno": 169}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_quantized_paged_attention1", "type": "TestCaseFunction", "lineno": 169}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_quantized_paged_attention2", "type": "TestCaseFunction", "lineno": 169}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_quantized_paged_attention3", "type": "TestCaseFunction", "lineno": 169}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_quantized_paged_attention4", "type": "TestCaseFunction", "lineno": 169}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_quantized_paged_attention5", "type": "TestCaseFunction", "lineno": 169}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_quantized_paged_attention6", "type": "TestCaseFunction", "lineno": 169}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_quantized_paged_attention7", "type": "TestCaseFunction", "lineno": 169}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_quantized_paged_attention8", "type": "TestCaseFunction", "lineno": 169}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_quantized_paged_attention9", "type": "TestCaseFunction", "lineno": 169}]}], "tests": [{"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_paged_attention0", "lineno": 114, "outcome": "passed", "keywords": ["test_paged_attention0", "__wrapped__", "__x_params_repr__", "PagedAttentionKernelTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0003017643466591835, "outcome": "passed"}, "call": {"duration": 9.88747676461935, "outcome": "passed"}, "teardown": {"duration": 0.00015671085566282272, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_paged_attention1", "lineno": 114, "outcome": "passed", "keywords": ["test_paged_attention1", "__wrapped__", "__x_params_repr__", "PagedAttentionKernelTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00023067928850650787, "outcome": "passed"}, "call": {"duration": 4.507334214635193, "outcome": "passed"}, "teardown": {"duration": 0.00018470268696546555, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_paged_attention2", "lineno": 114, "outcome": "passed", "keywords": ["test_paged_attention2", "__wrapped__", "__x_params_repr__", "PagedAttentionKernelTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00024737510830163956, "outcome": "passed"}, "call": {"duration": 3.3501764088869095, "outcome": "passed"}, "teardown": {"duration": 0.00028204545378685, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_paged_attention3", "lineno": 114, "outcome": "passed", "keywords": ["test_paged_attention3", "__wrapped__", "__x_params_repr__", "PagedAttentionKernelTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002515716478228569, "outcome": "passed"}, "call": {"duration": 2.9836514377966523, "outcome": "passed"}, "teardown": {"duration": 0.00013599079102277756, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_paged_attention4", "lineno": 114, "outcome": "passed", "keywords": ["test_paged_attention4", "__wrapped__", "__x_params_repr__", "PagedAttentionKernelTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00020783580839633942, "outcome": "passed"}, "call": {"duration": 1.8507901318371296, "outcome": "passed"}, "teardown": {"duration": 0.0001348387449979782, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_paged_attention5", "lineno": 114, "outcome": "passed", "keywords": ["test_paged_attention5", "__wrapped__", "__x_params_repr__", "PagedAttentionKernelTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00020320992916822433, "outcome": "passed"}, "call": {"duration": 0.31299500819295645, "outcome": "passed"}, "teardown": {"duration": 0.00025133974850177765, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_paged_attention6", "lineno": 114, "outcome": "passed", "keywords": ["test_paged_attention6", "__wrapped__", "__x_params_repr__", "PagedAttentionKernelTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00020273961126804352, "outcome": "passed"}, "call": {"duration": 0.4289404908195138, "outcome": "passed"}, "teardown": {"duration": 0.00025695934891700745, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_paged_attention7", "lineno": 114, "outcome": "passed", "keywords": ["test_paged_attention7", "__wrapped__", "__x_params_repr__", "PagedAttentionKernelTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0004408610984683037, "outcome": "passed"}, "call": {"duration": 3.0514654871076345, "outcome": "passed"}, "teardown": {"duration": 0.00016157794743776321, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_paged_attention8", "lineno": 114, "outcome": "passed", "keywords": ["test_paged_attention8", "__wrapped__", "__x_params_repr__", "PagedAttentionKernelTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00025185197591781616, "outcome": "passed"}, "call": {"duration": 3.3117595333606005, "outcome": "passed"}, "teardown": {"duration": 0.00019073206931352615, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_paged_attention9", "lineno": 114, "outcome": "passed", "keywords": ["test_paged_attention9", "__wrapped__", "__x_params_repr__", "PagedAttentionKernelTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002731727436184883, "outcome": "passed"}, "call": {"duration": 1.8269685823470354, "outcome": "passed"}, "teardown": {"duration": 0.00016141775995492935, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_quantized_paged_attention0", "lineno": 169, "outcome": "skipped", "keywords": ["test_quantized_paged_attention0", "__wrapped__", "__x_params_repr__", "PagedAttentionKernelTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002443511039018631, "outcome": "passed"}, "call": {"duration": 0.000540456734597683, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/gpu_paged_attention_test.py', 170, 'Skipped: Skipping since float8_e4m3fn is not supported on < sm89')"}, "teardown": {"duration": 0.00010479521006345749, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_quantized_paged_attention1", "lineno": 169, "outcome": "passed", "keywords": ["test_quantized_paged_attention1", "__wrapped__", "__x_params_repr__", "PagedAttentionKernelTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015074200928211212, "outcome": "passed"}, "call": {"duration": 6.902668023481965, "outcome": "passed"}, "teardown": {"duration": 0.0002411855384707451, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_quantized_paged_attention2", "lineno": 169, "outcome": "passed", "keywords": ["test_quantized_paged_attention2", "__wrapped__", "__x_params_repr__", "PagedAttentionKernelTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0004641050472855568, "outcome": "passed"}, "call": {"duration": 3.207095223478973, "outcome": "passed"}, "teardown": {"duration": 0.00014386232942342758, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_quantized_paged_attention3", "lineno": 169, "outcome": "passed", "keywords": ["test_quantized_paged_attention3", "__wrapped__", "__x_params_repr__", "PagedAttentionKernelTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00022928789258003235, "outcome": "passed"}, "call": {"duration": 4.253305033780634, "outcome": "passed"}, "teardown": {"duration": 0.0001416485756635666, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_quantized_paged_attention4", "lineno": 169, "outcome": "skipped", "keywords": ["test_quantized_paged_attention4", "__wrapped__", "__x_params_repr__", "PagedAttentionKernelTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00020965933799743652, "outcome": "passed"}, "call": {"duration": 0.00046810973435640335, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/gpu_paged_attention_test.py', 170, 'Skipped: Skipping since neither (k, v) quantization requested.')"}, "teardown": {"duration": 9.574089199304581e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_quantized_paged_attention5", "lineno": 169, "outcome": "failed", "keywords": ["test_quantized_paged_attention5", "__wrapped__", "__x_params_repr__", "PagedAttentionKernelTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002063848078250885, "outcome": "passed"}, "call": {"duration": 0.16144380811601877, "outcome": "failed", "crash": {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 106496, available: 65536"}, "traceback": [{"path": "jax/tests/pallas/gpu_paged_attention_test.py", "lineno": 224, "message": ""}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 263, "message": "in cache_miss"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 146, "message": "in _python_pjit_helper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 1600, "message": "in _pjit_call_impl_python"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2527, "message": "in compile"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 3073, "message": "in from_hlo"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2854, "message": "in _cached_compilation"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 491, "message": "in compile_or_get_cached"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 759, "message": "in _compile_and_write_cache"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py", "lineno": 359, "message": "in wrapper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 385, "message": "in backend_compile_and_load"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "JaxRuntimeError"}], "longrepr": "self = <tests.pallas.gpu_paged_attention_test.PagedAttentionKernelTest testMethod=test_quantized_paged_attention5>\ndtype = <class 'jax.numpy.float16'>, page_size = 32, num_kv_heads = 2\nq_kv_head_ratio = 16, head_dim = 64, block_h = 32, pages_per_compute_block = 8\nk_splits = 4, attn_logits_soft_cap = None, quantize_k = True, quantize_v = True\nquant_dtype = <class 'jax.numpy.int8'>\n\n    @jtu.sample_product(\n        dtype=(jnp.float16,),\n        page_size=(8, 16, 32),\n        num_kv_heads=(1, 2),\n        q_kv_head_ratio=(2, 16, 20),\n        head_dim=(32, 64),\n        block_h=(16, 32),\n        pages_per_compute_block=(4, 8),\n        k_splits=(4, 16),\n        attn_logits_soft_cap=(None,),\n        quantize_k=(True, False),\n        quantize_v=(True, False),\n        quant_dtype=(jnp.float8_e5m2, jnp.float8_e4m3fn, jnp.int8),\n    )\n    def test_quantized_paged_attention(\n        self,\n        dtype,\n        page_size,\n        num_kv_heads,\n        q_kv_head_ratio,\n        head_dim,\n        block_h,\n        pages_per_compute_block,\n        k_splits,\n        attn_logits_soft_cap,\n        quantize_k,\n        quantize_v,\n        quant_dtype,\n    ):\n      if not quantize_k and not quantize_v:\n        self.skipTest(\"Skipping since neither (k, v) quantization requested.\")\n      if (quant_dtype == jnp.float8_e4m3fn\n          and not jtu.is_cuda_compute_capability_at_least(\"8.9\")):\n        self.skipTest(\"Skipping since float8_e4m3fn is not supported on < sm89\")\n      max_kv_len = 2048\n      seq_lens = np.asarray([3, 256, 513, 1023, 2048], dtype=jnp.int32)\n      q, k_pages, v_pages, block_tables = _generate_qkv(\n          seq_lens.shape[0],\n          page_size,\n          max_kv_len,\n          num_kv_heads,\n          num_kv_heads * q_kv_head_ratio,\n          head_dim,\n          jax.random.key(0),\n          dtype,\n      )\n      k = _reconstruct_kv(block_tables, k_pages)\n      v = _reconstruct_kv(block_tables, v_pages)\n    \n      k_, k_scales = (_quantize(k_pages, quant_dtype)\n                      if quantize_k else (k_pages, None))\n      v_, v_scales = (_quantize(k_pages, quant_dtype)\n                      if quantize_v else (v_pages, None))\n    \n>     o = paged_attention.paged_attention(\n          q,\n          k_,\n          v_,\n          block_tables,\n          seq_lens,\n          k_scales_pages=k_scales,\n          v_scales_pages=v_scales,\n          block_h=block_h,\n          pages_per_compute_block=pages_per_compute_block,\n          k_splits=k_splits,\n          attn_logits_soft_cap=attn_logits_soft_cap,\n          interpret=self.INTERPRET,\n      )\n\njax/tests/pallas/gpu_paged_attention_test.py:224: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = <jaxlib._jax.Client object at 0x7fa45b165be0>\nmodule = <jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f63641b30b0>\nexecutable_devices = (RocmDevice(id=0),)\noptions = <jaxlib._jax.CompileOptions object at 0x2dbd8890>, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -> xc.LoadedExecutable:\n      sym_name = module.operation.attributes['sym_name']\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version < 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            \"Compiling module %s with FDO profile of length %d\",\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n>         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 106496, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError"}, "teardown": {"duration": 0.00023049023002386093, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_quantized_paged_attention6", "lineno": 169, "outcome": "skipped", "keywords": ["test_quantized_paged_attention6", "__wrapped__", "__x_params_repr__", "PagedAttentionKernelTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002765785902738571, "outcome": "passed"}, "call": {"duration": 0.0005609672516584396, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/gpu_paged_attention_test.py', 170, 'Skipped: Skipping since neither (k, v) quantization requested.')"}, "teardown": {"duration": 8.694734424352646e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_quantized_paged_attention7", "lineno": 169, "outcome": "failed", "keywords": ["test_quantized_paged_attention7", "__wrapped__", "__x_params_repr__", "PagedAttentionKernelTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00021413620561361313, "outcome": "passed"}, "call": {"duration": 0.15374424494802952, "outcome": "failed", "crash": {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 86016, available: 65536"}, "traceback": [{"path": "jax/tests/pallas/gpu_paged_attention_test.py", "lineno": 224, "message": ""}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 263, "message": "in cache_miss"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 146, "message": "in _python_pjit_helper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py", "lineno": 1600, "message": "in _pjit_call_impl_python"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2527, "message": "in compile"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 3073, "message": "in from_hlo"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py", "lineno": 2854, "message": "in _cached_compilation"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 491, "message": "in compile_or_get_cached"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 759, "message": "in _compile_and_write_cache"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py", "lineno": 359, "message": "in wrapper"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 385, "message": "in backend_compile_and_load"}, {"path": "/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py", "lineno": 375, "message": "JaxRuntimeError"}], "longrepr": "self = <tests.pallas.gpu_paged_attention_test.PagedAttentionKernelTest testMethod=test_quantized_paged_attention7>\ndtype = <class 'jax.numpy.float16'>, page_size = 32, num_kv_heads = 2\nq_kv_head_ratio = 20, head_dim = 64, block_h = 16, pages_per_compute_block = 8\nk_splits = 4, attn_logits_soft_cap = None, quantize_k = True, quantize_v = True\nquant_dtype = <class 'jax.numpy.int8'>\n\n    @jtu.sample_product(\n        dtype=(jnp.float16,),\n        page_size=(8, 16, 32),\n        num_kv_heads=(1, 2),\n        q_kv_head_ratio=(2, 16, 20),\n        head_dim=(32, 64),\n        block_h=(16, 32),\n        pages_per_compute_block=(4, 8),\n        k_splits=(4, 16),\n        attn_logits_soft_cap=(None,),\n        quantize_k=(True, False),\n        quantize_v=(True, False),\n        quant_dtype=(jnp.float8_e5m2, jnp.float8_e4m3fn, jnp.int8),\n    )\n    def test_quantized_paged_attention(\n        self,\n        dtype,\n        page_size,\n        num_kv_heads,\n        q_kv_head_ratio,\n        head_dim,\n        block_h,\n        pages_per_compute_block,\n        k_splits,\n        attn_logits_soft_cap,\n        quantize_k,\n        quantize_v,\n        quant_dtype,\n    ):\n      if not quantize_k and not quantize_v:\n        self.skipTest(\"Skipping since neither (k, v) quantization requested.\")\n      if (quant_dtype == jnp.float8_e4m3fn\n          and not jtu.is_cuda_compute_capability_at_least(\"8.9\")):\n        self.skipTest(\"Skipping since float8_e4m3fn is not supported on < sm89\")\n      max_kv_len = 2048\n      seq_lens = np.asarray([3, 256, 513, 1023, 2048], dtype=jnp.int32)\n      q, k_pages, v_pages, block_tables = _generate_qkv(\n          seq_lens.shape[0],\n          page_size,\n          max_kv_len,\n          num_kv_heads,\n          num_kv_heads * q_kv_head_ratio,\n          head_dim,\n          jax.random.key(0),\n          dtype,\n      )\n      k = _reconstruct_kv(block_tables, k_pages)\n      v = _reconstruct_kv(block_tables, v_pages)\n    \n      k_, k_scales = (_quantize(k_pages, quant_dtype)\n                      if quantize_k else (k_pages, None))\n      v_, v_scales = (_quantize(k_pages, quant_dtype)\n                      if quantize_v else (v_pages, None))\n    \n>     o = paged_attention.paged_attention(\n          q,\n          k_,\n          v_,\n          block_tables,\n          seq_lens,\n          k_scales_pages=k_scales,\n          v_scales_pages=v_scales,\n          block_h=block_h,\n          pages_per_compute_block=pages_per_compute_block,\n          k_splits=k_splits,\n          attn_logits_soft_cap=attn_logits_soft_cap,\n          interpret=self.INTERPRET,\n      )\n\njax/tests/pallas/gpu_paged_attention_test.py:224: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:263: in cache_miss\n    executable, pgle_profiler, const_args) = _python_pjit_helper(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:146: in _python_pjit_helper\n    out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n/usr/local/lib/python3.12/dist-packages/jax/_src/pjit.py:1600: in _pjit_call_impl_python\n    compiled = computation.compile()\n               ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2527: in compile\n    executable = UnloadedMeshExecutable.from_hlo(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:3073: in from_hlo\n    xla_executable = _cached_compilation(\n/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py:2854: in _cached_compilation\n    xla_executable = compiler.compile_or_get_cached(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:491: in compile_or_get_cached\n    return _compile_and_write_cache(\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:759: in _compile_and_write_cache\n    executable = backend_compile_and_load(\n/usr/local/lib/python3.12/dist-packages/jax/_src/profiler.py:359: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:385: in backend_compile_and_load\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nbackend = <jaxlib._jax.Client object at 0x7fa45b165be0>\nmodule = <jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7f634a3367f0>\nexecutable_devices = (RocmDevice(id=0),)\noptions = <jaxlib._jax.CompileOptions object at 0x2dc4d570>, host_callbacks = ()\n\n    @profiler.annotate_function\n    def backend_compile_and_load(\n        backend: xc.Client,\n        module: ir.Module,\n        executable_devices: xc.DeviceList,\n        options: xc.CompileOptions,\n        host_callbacks: Sequence[Any],\n    ) -> xc.LoadedExecutable:\n      sym_name = module.operation.attributes['sym_name']\n      module_name = ir.StringAttr(sym_name).value\n      # Convert ir.Module to a string representation, unless the backend\n      # explicitly flags the ability to handle a module directly (avoiding the\n      # overhead of back and forth conversions).\n      # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n      built_c: Any\n      if jaxlib_extension_version < 378:\n        built_c = mlir.module_to_bytecode(module)\n      else:\n        built_c = module\n    \n      if (options.executable_build_options.fdo_profile is not None\n          and len(options.executable_build_options.fdo_profile)):\n        logger.debug(\n            \"Compiling module %s with FDO profile of length %d\",\n            module_name,\n            len(options.executable_build_options.fdo_profile),\n        )\n    \n      try:\n        # we use a separate function call to ensure that XLA compilation appears\n        # separately in Python profiling results\n        # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n        if isinstance(backend, _jax.CompileOnlyPyClient):\n          if host_callbacks:\n            return backend.compile(\n                built_c,\n                executable_devices=executable_devices,  # type: ignore\n                compile_options=options,\n                host_callbacks=host_callbacks,  # type: ignore\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n          return backend.compile(\n              built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n        else:\n          if host_callbacks:\n            return backend.compile_and_load(\n                built_c,\n                executable_devices=executable_devices,\n                compile_options=options,\n                host_callbacks=host_callbacks,\n            )\n          # Some backends don't have `host_callbacks` option yet\n          # TODO(sharadmv): remove this fallback when all backends allow `compile`\n          # to take in `host_callbacks`\n>         return backend.compile_and_load(\n              built_c,\n              executable_devices=executable_devices,\n              compile_options=options,\n          )\nE         jax.errors.JaxRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 86016, available: 65536\n\n/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py:375: JaxRuntimeError"}, "teardown": {"duration": 0.00021370500326156616, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_quantized_paged_attention8", "lineno": 169, "outcome": "skipped", "keywords": ["test_quantized_paged_attention8", "__wrapped__", "__x_params_repr__", "PagedAttentionKernelTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002158079296350479, "outcome": "passed"}, "call": {"duration": 0.00048092007637023926, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/gpu_paged_attention_test.py', 170, 'Skipped: Skipping since neither (k, v) quantization requested.')"}, "teardown": {"duration": 7.73044303059578e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionKernelTest::test_quantized_paged_attention9", "lineno": 169, "outcome": "passed", "keywords": ["test_quantized_paged_attention9", "__wrapped__", "__x_params_repr__", "PagedAttentionKernelTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014664698392152786, "outcome": "passed"}, "call": {"duration": 2.4884820813313127, "outcome": "passed"}, "teardown": {"duration": 0.0001689000055193901, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_paged_attention0", "lineno": 114, "outcome": "passed", "keywords": ["test_paged_attention0", "__wrapped__", "__x_params_repr__", "PagedAttentionInterpretTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00040935445576906204, "outcome": "passed"}, "call": {"duration": 0.880596213042736, "outcome": "passed"}, "teardown": {"duration": 0.0001374734565615654, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_paged_attention1", "lineno": 114, "outcome": "passed", "keywords": ["test_paged_attention1", "__wrapped__", "__x_params_repr__", "PagedAttentionInterpretTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00020338967442512512, "outcome": "passed"}, "call": {"duration": 0.8587012002244592, "outcome": "passed"}, "teardown": {"duration": 0.00013616029173135757, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_paged_attention2", "lineno": 114, "outcome": "passed", "keywords": ["test_paged_attention2", "__wrapped__", "__x_params_repr__", "PagedAttentionInterpretTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002090083435177803, "outcome": "passed"}, "call": {"duration": 0.9865444228053093, "outcome": "passed"}, "teardown": {"duration": 0.00013416726142168045, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_paged_attention3", "lineno": 114, "outcome": "passed", "keywords": ["test_paged_attention3", "__wrapped__", "__x_params_repr__", "PagedAttentionInterpretTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00021325517445802689, "outcome": "passed"}, "call": {"duration": 0.5653840247541666, "outcome": "passed"}, "teardown": {"duration": 0.00013396702706813812, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_paged_attention4", "lineno": 114, "outcome": "passed", "keywords": ["test_paged_attention4", "__wrapped__", "__x_params_repr__", "PagedAttentionInterpretTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00020066741853952408, "outcome": "passed"}, "call": {"duration": 0.7575278654694557, "outcome": "passed"}, "teardown": {"duration": 0.00012892112135887146, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_paged_attention5", "lineno": 114, "outcome": "passed", "keywords": ["test_paged_attention5", "__wrapped__", "__x_params_repr__", "PagedAttentionInterpretTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00020076613873243332, "outcome": "passed"}, "call": {"duration": 0.7117109969258308, "outcome": "passed"}, "teardown": {"duration": 0.00013691186904907227, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_paged_attention6", "lineno": 114, "outcome": "passed", "keywords": ["test_paged_attention6", "__wrapped__", "__x_params_repr__", "PagedAttentionInterpretTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00020021572709083557, "outcome": "passed"}, "call": {"duration": 0.6067074052989483, "outcome": "passed"}, "teardown": {"duration": 0.00013327598571777344, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_paged_attention7", "lineno": 114, "outcome": "passed", "keywords": ["test_paged_attention7", "__wrapped__", "__x_params_repr__", "PagedAttentionInterpretTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00019964482635259628, "outcome": "passed"}, "call": {"duration": 0.776860591955483, "outcome": "passed"}, "teardown": {"duration": 0.00013491977006196976, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_paged_attention8", "lineno": 114, "outcome": "passed", "keywords": ["test_paged_attention8", "__wrapped__", "__x_params_repr__", "PagedAttentionInterpretTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00020973943173885345, "outcome": "passed"}, "call": {"duration": 1.3804746652022004, "outcome": "passed"}, "teardown": {"duration": 0.00014019664376974106, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_paged_attention9", "lineno": 114, "outcome": "passed", "keywords": ["test_paged_attention9", "__wrapped__", "__x_params_repr__", "PagedAttentionInterpretTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002057943493127823, "outcome": "passed"}, "call": {"duration": 0.6306851785629988, "outcome": "passed"}, "teardown": {"duration": 0.00014229025691747665, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_quantized_paged_attention0", "lineno": 169, "outcome": "skipped", "keywords": ["test_quantized_paged_attention0", "__wrapped__", "__x_params_repr__", "PagedAttentionInterpretTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002188635990023613, "outcome": "passed"}, "call": {"duration": 0.00048176106065511703, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/gpu_paged_attention_test.py', 170, 'Skipped: Skipping since float8_e4m3fn is not supported on < sm89')"}, "teardown": {"duration": 0.00010218098759651184, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_quantized_paged_attention1", "lineno": 169, "outcome": "passed", "keywords": ["test_quantized_paged_attention1", "__wrapped__", "__x_params_repr__", "PagedAttentionInterpretTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001615285873413086, "outcome": "passed"}, "call": {"duration": 0.7910675341263413, "outcome": "passed"}, "teardown": {"duration": 0.000136692076921463, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_quantized_paged_attention2", "lineno": 169, "outcome": "passed", "keywords": ["test_quantized_paged_attention2", "__wrapped__", "__x_params_repr__", "PagedAttentionInterpretTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00019791163504123688, "outcome": "passed"}, "call": {"duration": 0.8480467209592462, "outcome": "passed"}, "teardown": {"duration": 0.00013856496661901474, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_quantized_paged_attention3", "lineno": 169, "outcome": "passed", "keywords": ["test_quantized_paged_attention3", "__wrapped__", "__x_params_repr__", "PagedAttentionInterpretTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002096695825457573, "outcome": "passed"}, "call": {"duration": 0.8199328277260065, "outcome": "passed"}, "teardown": {"duration": 0.00013766344636678696, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_quantized_paged_attention4", "lineno": 169, "outcome": "skipped", "keywords": ["test_quantized_paged_attention4", "__wrapped__", "__x_params_repr__", "PagedAttentionInterpretTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0002048015594482422, "outcome": "passed"}, "call": {"duration": 0.0004521869122982025, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/gpu_paged_attention_test.py', 170, 'Skipped: Skipping since neither (k, v) quantization requested.')"}, "teardown": {"duration": 7.819663733243942e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_quantized_paged_attention5", "lineno": 169, "outcome": "passed", "keywords": ["test_quantized_paged_attention5", "__wrapped__", "__x_params_repr__", "PagedAttentionInterpretTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001472868025302887, "outcome": "passed"}, "call": {"duration": 2.9454492442309856, "outcome": "passed"}, "teardown": {"duration": 0.00014374125748872757, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_quantized_paged_attention6", "lineno": 169, "outcome": "skipped", "keywords": ["test_quantized_paged_attention6", "__wrapped__", "__x_params_repr__", "PagedAttentionInterpretTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00020287837833166122, "outcome": "passed"}, "call": {"duration": 0.00046161096543073654, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/gpu_paged_attention_test.py', 170, 'Skipped: Skipping since neither (k, v) quantization requested.')"}, "teardown": {"duration": 8.078943938016891e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_quantized_paged_attention7", "lineno": 169, "outcome": "passed", "keywords": ["test_quantized_paged_attention7", "__wrapped__", "__x_params_repr__", "PagedAttentionInterpretTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001406865194439888, "outcome": "passed"}, "call": {"duration": 1.087122700177133, "outcome": "passed"}, "teardown": {"duration": 0.0001384243369102478, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_quantized_paged_attention8", "lineno": 169, "outcome": "skipped", "keywords": ["test_quantized_paged_attention8", "__wrapped__", "__x_params_repr__", "PagedAttentionInterpretTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00020141713321208954, "outcome": "passed"}, "call": {"duration": 0.0004760725423693657, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/gpu_paged_attention_test.py', 170, 'Skipped: Skipping since neither (k, v) quantization requested.')"}, "teardown": {"duration": 7.979758083820343e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/gpu_paged_attention_test.py::PagedAttentionInterpretTest::test_quantized_paged_attention9", "lineno": 169, "outcome": "passed", "keywords": ["test_quantized_paged_attention9", "__wrapped__", "__x_params_repr__", "PagedAttentionInterpretTest", "gpu_paged_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014117732644081116, "outcome": "passed"}, "call": {"duration": 0.83382005430758, "outcome": "passed"}, "teardown": {"duration": 0.0004577953368425369, "outcome": "passed"}}]}