{"created": 1763581266.4106333, "duration": 4.207264184951782, "exitcode": 0, "root": "/dockerx/rocm/rocm-jax/jax", "environment": {}, "summary": {"skipped": 270, "total": 270, "collected": 270}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention0", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention1", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention10", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention11", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention12", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention13", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention14", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention15", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention16", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention17", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention18", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention19", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention2", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention20", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention21", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention22", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention23", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention24", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention25", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention26", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention27", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention28", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention29", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention3", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention30", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention31", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention32", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention33", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention34", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention35", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention36", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention37", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention38", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention39", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention4", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention40", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention41", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention42", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention43", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention44", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention45", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention46", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention47", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention48", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention49", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention5", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention50", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention51", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention52", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention53", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention6", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention7", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention8", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention9", "type": "TestCaseFunction", "lineno": 118}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention0", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention1", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention10", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention100", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention101", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention102", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention103", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention104", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention105", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention106", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention107", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention108", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention109", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention11", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention110", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention111", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention112", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention113", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention114", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention115", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention116", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention117", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention118", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention119", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention12", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention120", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention121", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention122", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention123", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention124", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention125", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention126", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention127", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention128", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention129", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention13", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention130", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention131", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention132", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention133", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention134", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention135", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention136", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention137", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention138", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention139", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention14", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention140", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention141", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention142", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention143", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention144", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention145", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention146", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention147", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention148", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention149", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention15", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention150", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention151", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention152", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention153", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention154", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention155", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention156", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention157", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention158", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention159", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention16", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention160", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention161", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention162", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention163", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention164", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention165", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention166", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention167", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention168", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention169", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention17", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention170", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention171", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention172", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention173", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention174", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention175", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention176", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention177", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention178", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention179", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention18", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention180", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention181", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention182", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention183", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention184", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention185", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention186", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention187", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention188", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention189", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention19", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention190", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention191", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention192", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention193", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention194", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention195", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention196", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention197", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention198", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention199", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention2", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention20", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention200", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention201", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention202", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention203", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention204", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention205", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention206", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention207", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention208", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention209", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention21", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention210", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention211", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention212", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention213", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention214", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention215", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention22", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention23", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention24", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention25", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention26", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention27", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention28", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention29", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention3", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention30", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention31", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention32", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention33", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention34", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention35", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention36", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention37", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention38", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention39", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention4", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention40", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention41", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention42", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention43", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention44", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention45", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention46", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention47", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention48", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention49", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention5", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention50", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention51", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention52", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention53", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention54", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention55", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention56", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention57", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention58", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention59", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention6", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention60", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention61", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention62", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention63", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention64", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention65", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention66", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention67", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention68", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention69", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention7", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention70", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention71", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention72", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention73", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention74", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention75", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention76", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention77", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention78", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention79", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention8", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention80", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention81", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention82", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention83", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention84", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention85", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention86", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention87", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention88", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention89", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention9", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention90", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention91", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention92", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention93", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention94", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention95", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention96", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention97", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention98", "type": "TestCaseFunction", "lineno": 53}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention99", "type": "TestCaseFunction", "lineno": 53}]}], "tests": [{"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention0", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention0", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0004384368658065796, "outcome": "passed"}, "call": {"duration": 3.2035667654126883, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.00017236452549695969, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention1", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention1", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00023352354764938354, "outcome": "passed"}, "call": {"duration": 0.00043863803148269653, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 9.887665510177612e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention10", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention10", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014326069504022598, "outcome": "passed"}, "call": {"duration": 0.0003130519762635231, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.396936416625977e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention11", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention11", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001432010903954506, "outcome": "passed"}, "call": {"duration": 0.0008349725976586342, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.93682411313057e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention12", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention12", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014343205839395523, "outcome": "passed"}, "call": {"duration": 0.000307413749396801, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.38687813282013e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention13", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention13", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013978593051433563, "outcome": "passed"}, "call": {"duration": 0.0003062421455979347, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.482059299945831e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention14", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention14", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013903528451919556, "outcome": "passed"}, "call": {"duration": 0.0002984898164868355, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.985291838645935e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention15", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention15", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013127364218235016, "outcome": "passed"}, "call": {"duration": 0.0002945149317383766, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.892159581184387e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention16", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention16", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013514887541532516, "outcome": "passed"}, "call": {"duration": 0.00029182154685258865, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.081497460603714e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention17", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention17", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013177469372749329, "outcome": "passed"}, "call": {"duration": 0.0004227040335536003, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.448997348546982e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention18", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention18", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013467855751514435, "outcome": "passed"}, "call": {"duration": 0.0002931831404566765, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.091369479894638e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention19", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention19", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014441367238759995, "outcome": "passed"}, "call": {"duration": 0.00029707886278629303, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.069390267133713e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention2", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention2", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013038143515586853, "outcome": "passed"}, "call": {"duration": 0.000294635072350502, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.071439176797867e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention20", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention20", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013114232569932938, "outcome": "passed"}, "call": {"duration": 0.0002921205013990402, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.176678627729416e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention21", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention21", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013047177344560623, "outcome": "passed"}, "call": {"duration": 0.00029485486447811127, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.328763604164124e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention22", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention22", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013428833335638046, "outcome": "passed"}, "call": {"duration": 0.00041871797293424606, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.647275924682617e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention23", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention23", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001409277319908142, "outcome": "passed"}, "call": {"duration": 0.0002970295026898384, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.366854697465897e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention24", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention24", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013577938079833984, "outcome": "passed"}, "call": {"duration": 0.0002919808030128479, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.22268596291542e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention25", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention25", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001332676038146019, "outcome": "passed"}, "call": {"duration": 0.0002869442105293274, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.962288171052933e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention26", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention26", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001328764483332634, "outcome": "passed"}, "call": {"duration": 0.0002885563299059868, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.128436118364334e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention27", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention27", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013347715139389038, "outcome": "passed"}, "call": {"duration": 0.00028672441840171814, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.415842264890671e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention28", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention28", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012695789337158203, "outcome": "passed"}, "call": {"duration": 0.0002990216016769409, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.00019120145589113235, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention29", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention29", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013103336095809937, "outcome": "passed"}, "call": {"duration": 0.0002914201468229294, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.211696356534958e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention3", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention3", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013294536620378494, "outcome": "passed"}, "call": {"duration": 0.00029253214597702026, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.188599556684494e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention30", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention30", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013174396008253098, "outcome": "passed"}, "call": {"duration": 0.0002883961424231529, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.995256990194321e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention31", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention31", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013452768325805664, "outcome": "passed"}, "call": {"duration": 0.00028927717357873917, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.916187703609467e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention32", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention32", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013533979654312134, "outcome": "passed"}, "call": {"duration": 0.0002913493663072586, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.200613617897034e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention33", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention33", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001337183639407158, "outcome": "passed"}, "call": {"duration": 0.0002932734787464142, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.023382931947708e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention34", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention34", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001329062506556511, "outcome": "passed"}, "call": {"duration": 0.0004792269319295883, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.429400622844696e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention35", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention35", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014697667211294174, "outcome": "passed"}, "call": {"duration": 0.00033046770840883255, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.863622158765793e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention36", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention36", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013054348528385162, "outcome": "passed"}, "call": {"duration": 0.0002848505973815918, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.152464240789413e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention37", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention37", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013407785445451736, "outcome": "passed"}, "call": {"duration": 0.0003040190786123276, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.074512541294098e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention38", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention38", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012867990881204605, "outcome": "passed"}, "call": {"duration": 0.0002943146973848343, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.873160600662231e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention39", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention39", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001326063647866249, "outcome": "passed"}, "call": {"duration": 0.0002937428653240204, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.076375186443329e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention4", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention4", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00025727972388267517, "outcome": "passed"}, "call": {"duration": 0.00029343366622924805, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.355771958827972e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention40", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention40", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013537053018808365, "outcome": "passed"}, "call": {"duration": 0.00028948672115802765, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.108505815267563e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention41", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention41", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012994278222322464, "outcome": "passed"}, "call": {"duration": 0.0002875151112675667, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.953347474336624e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention42", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention42", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012691691517829895, "outcome": "passed"}, "call": {"duration": 0.0002912096679210663, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.074512541294098e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention43", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention43", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00015716254711151123, "outcome": "passed"}, "call": {"duration": 0.00030513945966959, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.301755249500275e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention44", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention44", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014477409422397614, "outcome": "passed"}, "call": {"duration": 0.00029277242720127106, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.15646892786026e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention45", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention45", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001329071819782257, "outcome": "passed"}, "call": {"duration": 0.0004154229536652565, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.38687813282013e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention46", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention46", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013276468962430954, "outcome": "passed"}, "call": {"duration": 0.0002917395904660225, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.995869964361191e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention47", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention47", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013113394379615784, "outcome": "passed"}, "call": {"duration": 0.000291651114821434, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.132627069950104e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention48", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention48", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001346692442893982, "outcome": "passed"}, "call": {"duration": 0.0002902587875723839, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.105525583028793e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention49", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention49", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012652575969696045, "outcome": "passed"}, "call": {"duration": 0.0002906396985054016, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.133465260267258e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention5", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention5", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014515500515699387, "outcome": "passed"}, "call": {"duration": 0.00029954221099615097, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.000286132097244e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention50", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention50", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012945104390382767, "outcome": "passed"}, "call": {"duration": 0.00041992031037807465, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.480103522539139e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention51", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention51", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001337677240371704, "outcome": "passed"}, "call": {"duration": 0.0002993103116750717, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.199589163064957e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention52", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention52", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.000133417546749115, "outcome": "passed"}, "call": {"duration": 0.00028963759541511536, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.75037333369255e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention53", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention53", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013055279850959778, "outcome": "passed"}, "call": {"duration": 0.0002923524007201195, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.202569395303726e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention6", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention6", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013482943177223206, "outcome": "passed"}, "call": {"duration": 0.0002918913960456848, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.123500108718872e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention7", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention7", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013192463666200638, "outcome": "passed"}, "call": {"duration": 0.0002906089648604393, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.101520895957947e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention8", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention8", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013600103557109833, "outcome": "passed"}, "call": {"duration": 0.0004127388820052147, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.486063987016678e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_bwd_flash_attention9", "lineno": 118, "outcome": "skipped", "keywords": ["test_bwd_flash_attention9", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013330765068531036, "outcome": "passed"}, "call": {"duration": 0.0002967575564980507, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 119, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.414910942316055e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention0", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention0", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001294417306780815, "outcome": "passed"}, "call": {"duration": 0.00028802454471588135, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.985291838645935e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention1", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention1", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012808945029973984, "outcome": "passed"}, "call": {"duration": 0.0003053005784749985, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.022358477115631e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention10", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention10", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012889038771390915, "outcome": "passed"}, "call": {"duration": 0.0002878950908780098, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.369928061962128e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention100", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention100", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013402756303548813, "outcome": "passed"}, "call": {"duration": 0.0002979002892971039, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.30268657207489e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention101", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention101", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012885034084320068, "outcome": "passed"}, "call": {"duration": 0.00041618384420871735, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.205735892057419e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention102", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention102", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013521034270524979, "outcome": "passed"}, "call": {"duration": 0.0002939729019999504, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.970297545194626e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention103", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention103", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012908969074487686, "outcome": "passed"}, "call": {"duration": 0.00028933677822351456, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.21961259841919e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention104", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention104", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001286892220377922, "outcome": "passed"}, "call": {"duration": 0.00029462575912475586, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.048435509204865e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention105", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention105", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013194512575864792, "outcome": "passed"}, "call": {"duration": 0.0003032069653272629, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.988272070884705e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention106", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention106", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012871995568275452, "outcome": "passed"}, "call": {"duration": 0.0002946155145764351, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.99041411280632e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention107", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention107", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012809038162231445, "outcome": "passed"}, "call": {"duration": 0.00040663033723831177, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.326900959014893e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention108", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention108", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013184454292058945, "outcome": "passed"}, "call": {"duration": 0.00028913747519254684, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.104501128196716e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention109", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention109", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013113394379615784, "outcome": "passed"}, "call": {"duration": 0.00028883665800094604, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.161591202020645e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention11", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention11", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012814905494451523, "outcome": "passed"}, "call": {"duration": 0.000291961245238781, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.223710417747498e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention110", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention110", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012725777924060822, "outcome": "passed"}, "call": {"duration": 0.0002896171063184738, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.478054612874985e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention111", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention111", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013083312660455704, "outcome": "passed"}, "call": {"duration": 0.00030086375772953033, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.504038512706757e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention112", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention112", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001294706016778946, "outcome": "passed"}, "call": {"duration": 0.000411338172852993, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.305759936571121e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention113", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention113", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013429764658212662, "outcome": "passed"}, "call": {"duration": 0.0002922210842370987, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.20759853720665e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention114", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention114", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013259612023830414, "outcome": "passed"}, "call": {"duration": 0.0002902783453464508, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.00183042883873e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention115", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention115", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001320047304034233, "outcome": "passed"}, "call": {"duration": 0.00028915610164403915, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.272697985172272e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention116", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention116", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013311579823493958, "outcome": "passed"}, "call": {"duration": 0.0002901284024119377, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.160566747188568e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention117", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention117", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001366725191473961, "outcome": "passed"}, "call": {"duration": 0.00028927717357873917, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.960239261388779e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention118", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention118", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013718195259571075, "outcome": "passed"}, "call": {"duration": 0.000705271027982235, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.689278572797775e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention119", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention119", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013575050979852676, "outcome": "passed"}, "call": {"duration": 0.00029399432241916656, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.283687591552734e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention12", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention12", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001307232305407524, "outcome": "passed"}, "call": {"duration": 0.00028740521520376205, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.983336061239243e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention120", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention120", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001287003979086876, "outcome": "passed"}, "call": {"duration": 0.0002965470775961876, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.084384560585022e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention121", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention121", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013018213212490082, "outcome": "passed"}, "call": {"duration": 0.0002899477258324623, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.271673530340195e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention122", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention122", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012748781591653824, "outcome": "passed"}, "call": {"duration": 0.0002845805138349533, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.044430822134018e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention123", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention123", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013065338134765625, "outcome": "passed"}, "call": {"duration": 0.00028897635638713837, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.327832281589508e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention124", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention124", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00026253797113895416, "outcome": "passed"}, "call": {"duration": 0.00030277669429779053, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.078517228364944e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention125", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention125", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013157352805137634, "outcome": "passed"}, "call": {"duration": 0.0002969279885292053, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.187668234109879e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention126", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention126", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013048388063907623, "outcome": "passed"}, "call": {"duration": 0.000288977287709713, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.008388638496399e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention127", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention127", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001338571310043335, "outcome": "passed"}, "call": {"duration": 0.0002914499491453171, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.20759853720665e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention128", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention128", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012694764882326126, "outcome": "passed"}, "call": {"duration": 0.000286933034658432, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.302779704332352e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention129", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention129", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012760888785123825, "outcome": "passed"}, "call": {"duration": 0.0002885265275835991, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.955303251743317e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention13", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention13", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012689735740423203, "outcome": "passed"}, "call": {"duration": 0.0004143407568335533, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.638761937618256e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention130", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention130", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013221614062786102, "outcome": "passed"}, "call": {"duration": 0.0002915114164352417, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.127504795789719e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention131", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention131", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001315344125032425, "outcome": "passed"}, "call": {"duration": 0.0002906695008277893, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.135514169931412e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention132", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention132", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001308843493461609, "outcome": "passed"}, "call": {"duration": 0.000295465812087059, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.846059113740921e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention133", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention133", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001363903284072876, "outcome": "passed"}, "call": {"duration": 0.00029845163226127625, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.995350122451782e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention134", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention134", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012721866369247437, "outcome": "passed"}, "call": {"duration": 0.00030605122447013855, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.960146129131317e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention135", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention135", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001286705955862999, "outcome": "passed"}, "call": {"duration": 0.0004293937236070633, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.378868758678436e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention136", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention136", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013225432485342026, "outcome": "passed"}, "call": {"duration": 0.0002905791625380516, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.56019726395607e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention137", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention137", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012885965406894684, "outcome": "passed"}, "call": {"duration": 0.00029340293258428574, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.218681275844574e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention138", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention138", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012974068522453308, "outcome": "passed"}, "call": {"duration": 0.00029077939689159393, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.2467140853405e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention139", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention139", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001289108768105507, "outcome": "passed"}, "call": {"duration": 0.00028924643993377686, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.896164268255234e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention14", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention14", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012915953993797302, "outcome": "passed"}, "call": {"duration": 0.00029009860008955, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.945338100194931e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention140", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention140", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012964103370904922, "outcome": "passed"}, "call": {"duration": 0.0004074610769748688, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.247738540172577e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention141", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention141", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013097282499074936, "outcome": "passed"}, "call": {"duration": 0.00029118917882442474, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.117539644241333e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention142", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention142", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001301513984799385, "outcome": "passed"}, "call": {"duration": 0.000299050472676754, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.419008761644363e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention143", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention143", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013055279850959778, "outcome": "passed"}, "call": {"duration": 0.0002920115366578102, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.242709398269653e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention144", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention144", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001305127516388893, "outcome": "passed"}, "call": {"duration": 0.00029158126562833786, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.0975162088871e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention145", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention145", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001294706016778946, "outcome": "passed"}, "call": {"duration": 0.00029104016721248627, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.979331374168396e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention146", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention146", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013207551091909409, "outcome": "passed"}, "call": {"duration": 0.00041532237082719803, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.295794785022736e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention147", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention147", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013137329369783401, "outcome": "passed"}, "call": {"duration": 0.00029943231493234634, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.257796823978424e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention148", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention148", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012901052832603455, "outcome": "passed"}, "call": {"duration": 0.00028977636247873306, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.066410034894943e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention149", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention149", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012946035712957382, "outcome": "passed"}, "call": {"duration": 0.00030095409601926804, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.95427879691124e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention15", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention15", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012833904474973679, "outcome": "passed"}, "call": {"duration": 0.00028749462217092514, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.991345435380936e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention150", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention150", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013113394379615784, "outcome": "passed"}, "call": {"duration": 0.0002894466742873192, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.19558447599411e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention151", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention151", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012919120490550995, "outcome": "passed"}, "call": {"duration": 0.0004108566790819168, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.287692278623581e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention152", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention152", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013480894267559052, "outcome": "passed"}, "call": {"duration": 0.0002960376441478729, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.256772369146347e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention153", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention153", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013256631791591644, "outcome": "passed"}, "call": {"duration": 0.0002870336174964905, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.334817200899124e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention154", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention154", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012671668082475662, "outcome": "passed"}, "call": {"duration": 0.00028931722044944763, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.145479321479797e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention155", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention155", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012927129864692688, "outcome": "passed"}, "call": {"duration": 0.00031139887869358063, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.040426135063171e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention156", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention156", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012961123138666153, "outcome": "passed"}, "call": {"duration": 0.0002971794456243515, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.994325667619705e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention157", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention157", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012671761214733124, "outcome": "passed"}, "call": {"duration": 0.00041806697845458984, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.24373385310173e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention158", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention158", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013624224811792374, "outcome": "passed"}, "call": {"duration": 0.00030456017702817917, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.018353790044785e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention159", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention159", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001370515674352646, "outcome": "passed"}, "call": {"duration": 0.0002999715507030487, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.186643779277802e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention16", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention16", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013260450214147568, "outcome": "passed"}, "call": {"duration": 0.00030211638659238815, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.964337080717087e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention160", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention160", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013000145554542542, "outcome": "passed"}, "call": {"duration": 0.00030068308115005493, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.163640111684799e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention161", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention161", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001373235136270523, "outcome": "passed"}, "call": {"duration": 0.00029906071722507477, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.129460573196411e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention162", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention162", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012730807065963745, "outcome": "passed"}, "call": {"duration": 0.0004162350669503212, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.128436118364334e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention163", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention163", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013152416795492172, "outcome": "passed"}, "call": {"duration": 0.00029728934168815613, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.13057816028595e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention164", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention164", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012838002294301987, "outcome": "passed"}, "call": {"duration": 0.0002969978377223015, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.313769310712814e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention165", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention165", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012701749801635742, "outcome": "passed"}, "call": {"duration": 0.00029482506215572357, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.117539644241333e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention166", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention166", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012876000255346298, "outcome": "passed"}, "call": {"duration": 0.0002954760566353798, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.956234574317932e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention167", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention167", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013035256415605545, "outcome": "passed"}, "call": {"duration": 0.00029522646218538284, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.102452218532562e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention168", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention168", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00014647562056779861, "outcome": "passed"}, "call": {"duration": 0.000421302393078804, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.60415568947792e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention169", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention169", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013125408440828323, "outcome": "passed"}, "call": {"duration": 0.0002982504665851593, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.264688611030579e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention17", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention17", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012715812772512436, "outcome": "passed"}, "call": {"duration": 0.00029747840017080307, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.219705730676651e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention170", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention170", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013083405792713165, "outcome": "passed"}, "call": {"duration": 0.0002966979518532753, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.171556353569031e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention171", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention171", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013110414147377014, "outcome": "passed"}, "call": {"duration": 0.00029068905860185623, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.000286132097244e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention172", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention172", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012900028377771378, "outcome": "passed"}, "call": {"duration": 0.0002886159345507622, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.079541683197021e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention173", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention173", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013676099479198456, "outcome": "passed"}, "call": {"duration": 0.00041286926716566086, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.313769310712814e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention174", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention174", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001337677240371704, "outcome": "passed"}, "call": {"duration": 0.00029172003269195557, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.247645407915115e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention175", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention175", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013181474059820175, "outcome": "passed"}, "call": {"duration": 0.000293804332613945, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.090438157320023e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention176", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention176", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012738816440105438, "outcome": "passed"}, "call": {"duration": 0.0002891272306442261, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.868131458759308e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention177", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention177", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012924056500196457, "outcome": "passed"}, "call": {"duration": 0.0002990821376442909, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.834138184785843e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention178", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention178", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012770947068929672, "outcome": "passed"}, "call": {"duration": 0.00028570182621479034, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.877072155475616e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention179", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention179", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012816023081541061, "outcome": "passed"}, "call": {"duration": 0.00040896423161029816, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.123500108718872e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention18", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention18", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001298505812883377, "outcome": "passed"}, "call": {"duration": 0.00029388442635536194, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.175561040639877e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention180", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention180", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013138353824615479, "outcome": "passed"}, "call": {"duration": 0.00028957705944776535, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.331836968660355e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention181", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention181", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013268552720546722, "outcome": "passed"}, "call": {"duration": 0.0002891775220632553, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.013324648141861e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention182", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention182", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012920144945383072, "outcome": "passed"}, "call": {"duration": 0.0002875532954931259, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.151532918214798e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention183", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention183", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012862961739301682, "outcome": "passed"}, "call": {"duration": 0.0002953261137008667, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.229670882225037e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention184", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention184", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013142451643943787, "outcome": "passed"}, "call": {"duration": 0.0007878933101892471, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.714424282312393e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention185", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention185", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.000133587047457695, "outcome": "passed"}, "call": {"duration": 0.00028963759541511536, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.143523544073105e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention186", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention186", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012863054871559143, "outcome": "passed"}, "call": {"duration": 0.0002938639372587204, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.124524563550949e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention187", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention187", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012859981507062912, "outcome": "passed"}, "call": {"duration": 0.0002889065071940422, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.151532918214798e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention188", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention188", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012833066284656525, "outcome": "passed"}, "call": {"duration": 0.0002875840291380882, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.905198097229004e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention189", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention189", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012804940342903137, "outcome": "passed"}, "call": {"duration": 0.00029349420219659805, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.012300193309784e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention19", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention19", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013291556388139725, "outcome": "passed"}, "call": {"duration": 0.00029403436928987503, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.086526602506638e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention190", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention190", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.000265391543507576, "outcome": "passed"}, "call": {"duration": 0.00029713939875364304, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.459986954927444e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention191", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention191", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013012252748012543, "outcome": "passed"}, "call": {"duration": 0.0002900483086705208, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.962288171052933e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention192", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention192", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013050250709056854, "outcome": "passed"}, "call": {"duration": 0.00029817037284374237, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.901193410158157e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention193", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention193", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001280391588807106, "outcome": "passed"}, "call": {"duration": 0.00029402412474155426, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.214676588773727e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention194", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention194", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013064220547676086, "outcome": "passed"}, "call": {"duration": 0.00029393378645181656, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.081497460603714e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention195", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention195", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001279488205909729, "outcome": "passed"}, "call": {"duration": 0.00028862617909908295, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.091369479894638e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention196", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention196", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001344885677099228, "outcome": "passed"}, "call": {"duration": 0.0004134206101298332, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.494073361158371e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention197", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention197", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013563036918640137, "outcome": "passed"}, "call": {"duration": 0.0002931114286184311, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.160566747188568e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention198", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention198", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012934021651744843, "outcome": "passed"}, "call": {"duration": 0.0002889959141612053, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.068552076816559e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention199", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention199", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001323055475950241, "outcome": "passed"}, "call": {"duration": 0.0002901582047343254, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.84717670083046e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention2", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention2", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013168435543775558, "outcome": "passed"}, "call": {"duration": 0.0002868734300136566, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.34478235244751e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention20", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention20", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013614073395729065, "outcome": "passed"}, "call": {"duration": 0.00029200129210948944, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.125642150640488e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention200", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention200", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012863986194133759, "outcome": "passed"}, "call": {"duration": 0.00041916780173778534, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.609277963638306e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention201", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention201", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001386646181344986, "outcome": "passed"}, "call": {"duration": 0.0002953559160232544, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.397867739200592e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention202", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention202", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013033300638198853, "outcome": "passed"}, "call": {"duration": 0.0002921009436249733, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.173605263233185e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention203", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention203", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012933090329170227, "outcome": "passed"}, "call": {"duration": 0.0002966569736599922, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.039401680231094e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention204", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention204", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001302231103181839, "outcome": "passed"}, "call": {"duration": 0.0002938946709036827, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.14445486664772e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention205", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention205", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001279190182685852, "outcome": "passed"}, "call": {"duration": 0.00028942711651325226, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.901193410158157e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention206", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention206", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013278517872095108, "outcome": "passed"}, "call": {"duration": 0.0004095742478966713, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.313769310712814e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention207", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention207", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013109389692544937, "outcome": "passed"}, "call": {"duration": 0.0002966085448861122, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.431861013174057e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention208", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention208", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013283640146255493, "outcome": "passed"}, "call": {"duration": 0.0002927519381046295, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.042381912469864e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention209", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention209", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013759266585111618, "outcome": "passed"}, "call": {"duration": 0.0002894764766097069, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.094535976648331e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention21", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention21", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013381708413362503, "outcome": "passed"}, "call": {"duration": 0.00028701312839984894, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.012393325567245e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention210", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention210", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013345666229724884, "outcome": "passed"}, "call": {"duration": 0.0002887863665819168, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.037445902824402e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention211", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention211", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001308433711528778, "outcome": "passed"}, "call": {"duration": 0.0004098452627658844, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.52713531255722e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention212", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention212", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001352895051240921, "outcome": "passed"}, "call": {"duration": 0.0002966979518532753, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.272697985172272e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention213", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention213", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013238564133644104, "outcome": "passed"}, "call": {"duration": 0.0002980399876832962, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.974302232265472e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention214", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention214", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012964103370904922, "outcome": "passed"}, "call": {"duration": 0.00029301270842552185, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.161591202020645e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention215", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention215", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012939982116222382, "outcome": "passed"}, "call": {"duration": 0.00029447395354509354, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.084477692842484e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention22", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention22", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012713856995105743, "outcome": "passed"}, "call": {"duration": 0.00029278360307216644, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.459986954927444e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention23", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention23", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001297425478696823, "outcome": "passed"}, "call": {"duration": 0.00041197799146175385, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.325783371925354e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention24", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention24", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013393815606832504, "outcome": "passed"}, "call": {"duration": 0.000290638767182827, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.277727127075195e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention25", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention25", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001270277425646782, "outcome": "passed"}, "call": {"duration": 0.0002884361892938614, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.629208266735077e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention26", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention26", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.000127498060464859, "outcome": "passed"}, "call": {"duration": 0.0002992022782564163, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.122475653886795e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention27", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention27", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012859981507062912, "outcome": "passed"}, "call": {"duration": 0.00029948167502880096, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.290765643119812e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention28", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention28", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013025198131799698, "outcome": "passed"}, "call": {"duration": 0.0002875151112675667, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.080379873514175e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention29", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention29", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001301923766732216, "outcome": "passed"}, "call": {"duration": 0.0004144422709941864, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.18953087925911e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention3", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention3", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001350194215774536, "outcome": "passed"}, "call": {"duration": 0.00028936658054590225, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.16252252459526e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention30", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention30", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012997165322303772, "outcome": "passed"}, "call": {"duration": 0.00029525626450777054, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.017422467470169e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention31", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention31", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012892112135887146, "outcome": "passed"}, "call": {"duration": 0.00029146019369363785, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.423944771289825e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention32", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention32", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012786872684955597, "outcome": "passed"}, "call": {"duration": 0.0003014346584677696, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.10953027009964e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention33", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention33", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001275390386581421, "outcome": "passed"}, "call": {"duration": 0.0002987207844853401, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.161591202020645e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention34", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention34", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012863986194133759, "outcome": "passed"}, "call": {"duration": 0.0004115179181098938, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.390882819890976e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention35", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention35", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013064313679933548, "outcome": "passed"}, "call": {"duration": 0.0002872142940759659, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 8.107908070087433e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention36", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention36", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012867990881204605, "outcome": "passed"}, "call": {"duration": 0.0002875952050089836, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.076375186443329e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention37", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention37", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.000128219835460186, "outcome": "passed"}, "call": {"duration": 0.00029091909527778625, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.0253387093544e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention38", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention38", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013309624046087265, "outcome": "passed"}, "call": {"duration": 0.00029121991246938705, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.202662527561188e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention39", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention39", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013386737555265427, "outcome": "passed"}, "call": {"duration": 0.00029955245554447174, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.009319961071014e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention4", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention4", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013133417814970016, "outcome": "passed"}, "call": {"duration": 0.0004025641828775406, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.246620953083038e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention40", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention40", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001283399760723114, "outcome": "passed"}, "call": {"duration": 0.0002863425761461258, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.123500108718872e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention41", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention41", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012886058539152145, "outcome": "passed"}, "call": {"duration": 0.00028853584080934525, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.639359682798386e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention42", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention42", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.000128990039229393, "outcome": "passed"}, "call": {"duration": 0.0002914099022746086, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.247645407915115e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention43", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention43", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013206526637077332, "outcome": "passed"}, "call": {"duration": 0.0002872040495276451, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.957259029150009e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention44", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention44", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012781936675310135, "outcome": "passed"}, "call": {"duration": 0.0002919808030128479, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.985291838645935e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention45", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention45", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013356655836105347, "outcome": "passed"}, "call": {"duration": 0.0004094541072845459, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.364898920059204e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention46", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention46", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013284571468830109, "outcome": "passed"}, "call": {"duration": 0.00029096007347106934, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.067341357469559e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention47", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention47", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012934114784002304, "outcome": "passed"}, "call": {"duration": 0.0002906695008277893, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.130485028028488e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention48", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention48", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012711621820926666, "outcome": "passed"}, "call": {"duration": 0.00029435567557811737, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.257703691720963e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention49", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention49", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012748781591653824, "outcome": "passed"}, "call": {"duration": 0.00028969813138246536, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.87725841999054e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention5", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention5", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001288698986172676, "outcome": "passed"}, "call": {"duration": 0.00029554590582847595, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.318798452615738e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention50", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention50", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001275995746254921, "outcome": "passed"}, "call": {"duration": 0.0004092445597052574, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.015373557806015e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention51", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention51", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012881029397249222, "outcome": "passed"}, "call": {"duration": 0.00028945785015821457, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.171556353569031e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention52", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention52", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013127457350492477, "outcome": "passed"}, "call": {"duration": 0.00028870534151792526, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.215607911348343e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention53", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention53", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013235490769147873, "outcome": "passed"}, "call": {"duration": 0.0002855919301509857, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.066503167152405e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention54", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention54", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012885965406894684, "outcome": "passed"}, "call": {"duration": 0.00029399432241916656, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.404852658510208e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention55", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention55", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013368669897317886, "outcome": "passed"}, "call": {"duration": 0.00029136985540390015, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.187575101852417e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention56", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention56", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001340676099061966, "outcome": "passed"}, "call": {"duration": 0.0007493263110518456, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.585249841213226e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention57", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention57", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013408716768026352, "outcome": "passed"}, "call": {"duration": 0.0002916501834988594, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.479079067707062e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention58", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention58", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001318352296948433, "outcome": "passed"}, "call": {"duration": 0.0002870531752705574, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.976258009672165e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention59", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention59", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013097282499074936, "outcome": "passed"}, "call": {"duration": 0.0002904478460550308, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.214583456516266e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention6", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention6", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012921076267957687, "outcome": "passed"}, "call": {"duration": 0.0002935631200671196, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.157493382692337e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention60", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention60", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001435922458767891, "outcome": "passed"}, "call": {"duration": 0.0002898676320910454, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.064361125230789e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention61", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention61", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012698769569396973, "outcome": "passed"}, "call": {"duration": 0.0002888757735490799, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.193628698587418e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention62", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention62", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00026310794055461884, "outcome": "passed"}, "call": {"duration": 0.00029340293258428574, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.144547998905182e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention63", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention63", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013243500143289566, "outcome": "passed"}, "call": {"duration": 0.0002904478460550308, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.396936416625977e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention64", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention64", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001323651522397995, "outcome": "passed"}, "call": {"duration": 0.00029483530670404434, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.46697187423706e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention65", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention65", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013479869812726974, "outcome": "passed"}, "call": {"duration": 0.0002940939739346504, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.337890565395355e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention66", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention66", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012834928929805756, "outcome": "passed"}, "call": {"duration": 0.00029495544731616974, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.262825965881348e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention67", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention67", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013268552720546722, "outcome": "passed"}, "call": {"duration": 0.00029304157942533493, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.930157542228699e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention68", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention68", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013107433915138245, "outcome": "passed"}, "call": {"duration": 0.0004169866442680359, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.315818220376968e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention69", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention69", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013522058725357056, "outcome": "passed"}, "call": {"duration": 0.00028850510716438293, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.330812513828278e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention7", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention7", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013120286166667938, "outcome": "passed"}, "call": {"duration": 0.0002969074994325638, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.982311606407166e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention70", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention70", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001333458349108696, "outcome": "passed"}, "call": {"duration": 0.0002850908786058426, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.880145519971848e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention71", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention71", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013392791152000427, "outcome": "passed"}, "call": {"duration": 0.00028569065034389496, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.229577749967575e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention72", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention72", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012999214231967926, "outcome": "passed"}, "call": {"duration": 0.00028490088880062103, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.113534957170486e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention73", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention73", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013035163283348083, "outcome": "passed"}, "call": {"duration": 0.00041901879012584686, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.454957813024521e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention74", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention74", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013401638716459274, "outcome": "passed"}, "call": {"duration": 0.00029394403100013733, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.3237344622612e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention75", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention75", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013128388673067093, "outcome": "passed"}, "call": {"duration": 0.0002925219014286995, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.103476673364639e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention76", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention76", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012747850269079208, "outcome": "passed"}, "call": {"duration": 0.00030080415308475494, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.081497460603714e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention77", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention77", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012939143925905228, "outcome": "passed"}, "call": {"duration": 0.0002941638231277466, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.042381912469864e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention78", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention78", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012644752860069275, "outcome": "passed"}, "call": {"duration": 0.0002878652885556221, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.143616676330566e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention79", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention79", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013354793190956116, "outcome": "passed"}, "call": {"duration": 0.0004096543416380882, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.406901568174362e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention8", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention8", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001337071880698204, "outcome": "passed"}, "call": {"duration": 0.00029253214597702026, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.318798452615738e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention80", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention80", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013217516243457794, "outcome": "passed"}, "call": {"duration": 0.0002867532894015312, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.077492773532867e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention81", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention81", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001320643350481987, "outcome": "passed"}, "call": {"duration": 0.0002873847261071205, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.580220699310303e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention82", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention82", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013261567801237106, "outcome": "passed"}, "call": {"duration": 0.0002927221357822418, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.074512541294098e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention83", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention83", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001294706016778946, "outcome": "passed"}, "call": {"duration": 0.00028909649699926376, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.229670882225037e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention84", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention84", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012781843543052673, "outcome": "passed"}, "call": {"duration": 0.0004120785742998123, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.305759936571121e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention85", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention85", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.000134187750518322, "outcome": "passed"}, "call": {"duration": 0.0002923412248492241, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.586181163787842e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention86", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention86", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001263059675693512, "outcome": "passed"}, "call": {"duration": 0.0002853507176041603, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 6.886199116706848e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention87", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention87", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012771785259246826, "outcome": "passed"}, "call": {"duration": 0.00029424484819173813, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.103476673364639e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention88", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention88", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012700818479061127, "outcome": "passed"}, "call": {"duration": 0.0002867626026272774, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.131602615118027e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention89", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention89", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001428406685590744, "outcome": "passed"}, "call": {"duration": 0.0002947961911559105, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.420964539051056e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention9", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention9", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012780819088220596, "outcome": "passed"}, "call": {"duration": 0.0004128403961658478, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.352884858846664e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention90", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention90", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012945104390382767, "outcome": "passed"}, "call": {"duration": 0.0002930331975221634, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.028412073850632e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention91", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention91", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.0001353500410914421, "outcome": "passed"}, "call": {"duration": 0.0002900483086705208, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.263757288455963e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention92", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention92", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012631621211767197, "outcome": "passed"}, "call": {"duration": 0.0002959566190838814, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.015373557806015e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention93", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention93", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013068225234746933, "outcome": "passed"}, "call": {"duration": 0.0002904888242483139, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.011368870735168e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention94", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention94", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013524945825338364, "outcome": "passed"}, "call": {"duration": 0.0002908604219555855, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.009319961071014e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention95", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention95", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013115350157022476, "outcome": "passed"}, "call": {"duration": 0.0004079621285200119, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.158610969781876e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention96", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention96", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013176444917917252, "outcome": "passed"}, "call": {"duration": 0.00029208138585090637, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.26776197552681e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention97", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention97", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00012890994548797607, "outcome": "passed"}, "call": {"duration": 0.0002979487180709839, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.204711437225342e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention98", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention98", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013002194464206696, "outcome": "passed"}, "call": {"duration": 0.0002949461340904236, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 7.390975952148438e-05, "outcome": "passed"}}, {"nodeid": "tests/pallas/mgpu_attention_test.py::FlashAttentionTestCase::test_flash_attention99", "lineno": 53, "outcome": "skipped", "keywords": ["test_flash_attention99", "__wrapped__", "__x_params_repr__", "FlashAttentionTestCase", "mgpu_attention_test.py", "pallas", "tests", "jax", ""], "setup": {"duration": 0.00013097282499074936, "outcome": "passed"}, "call": {"duration": 0.00029390305280685425, "outcome": "skipped", "longrepr": "('/dockerx/rocm/rocm-jax/jax/tests/pallas/mgpu_attention_test.py', 54, 'Skipped: Only works on GPU with capability sm90a')"}, "teardown": {"duration": 0.0010085878893733025, "outcome": "passed"}}]}